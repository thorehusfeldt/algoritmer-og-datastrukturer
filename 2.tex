\chapter{Indledning}
\renewcommand{\labelprefix}{ch:intro}
\llabel{}
\vspace*{-4.5cm}
\mbox{}\hspace{\fill}\includegraphics[width=5cm]{img/stonehenge2.eps} 
\vspace{1cm}


\aufmacher{\noindent Hvis man vil blive billedhugger,\footnote{%
Billedet af stenkredsen ved Stonehenge er taget fra \cite{Pin1808}.}
skal man lære en masse grundlæggende teknikker:
Hvor finder man passende sten?
Hvordan flytter man dem, hvordan virker mejslen, hvordan bygger man et stillads, osv.
Når de grundlæggende teknikker er på plads, er man langtfra nogen berømt kunstner, men selv en sjældent talent kan ikke blive berømt uden at beherske grundlaget.
 Wer die Grundtechniken beherrscht, 
Man behøver ikke kende det hele, inden man går i gang med sin første skulptur
Men man skal være parat til at gå tilbage til grundteknikkerne for at blive bedre og bedre.
}

Dette indledende kapitel spiller en lignende rolle i bogen.
Vi præsenterer de grundlæggende begreber og metoder for at bedre kunne beskrive og analysere algoritmer i de senere kapitler.
Man behøver ikke at arbejde sig igennem dette kapitel fra A til Z inden man giver sig i kast med de følgende kapitler.
Vi anbefaler ved første læsning at studere materialet til og med afsnit~\lref{s:pseudocode} grundigt, og skimme de følgende afsnit.
Afsnit~\lref{s:o} beskriver notation og terminologi for at beskrive algoritmers kompleksitet kort og præcist.
I afsnit~\lref{s:model} præsenteres en enkel beregningsmodel, som gør det muligt at abstrahere bort fra mange af de komplikationer, der ville opstå ved at tage hensyn til egenskaberne ved moderne maskinarkitektur.
Modellen er tilstrækkelig konkret til at levere nyttige forudsigelser, men tilstrækkelig abstrakt til at tillade elegante overvejelser.
I afsnit~\lref{s:pseudocode} introduceres et notation for pseudokode, som minder om et højniveausprogrammeringssprog og tillader en bekvem beskrivelse af algoritmer end maskinmodellens kode.
Desuden giver pseudokoden mulighed for at benytte notation fra matematikken, uden at vi behøver at bekymre os om, hvordan denne ville skulle oversættes til en ægte maskine. 
Vi vil gøre hyppig brug af kommentarer i programmerne, både for at øge deres læsbarhed og for at gøre det letter at føre formelle korrekthedsbeviser.
Teknikker for den slags beviser er genstant for afsnit~\lref{s:correct}.
%
Afsnit~\lref{s:binary search} indeholder det første omfattende eksempel:
Binærsøgning i en sorteret række.
I afsnit~\lref{s:analysis} beskrives matematiske teknikker for programmers kompleksitetsanalyse med vægt på indlejrede løkker og rekursive procedurekald. 
For analysen af gennemsnitligt tidsforbrug
\index{algoritmenanalyse!gennemsnit}
har vi brug for yderligere teknikker; disse beskrives i afsnit~\lref{s:average case analysis}.
Randomiserede algoritmer, præsenteret i afsnit ~\lref{s:random}, gør brug af tilfældighed ved under udførelsen at kunne slå plat og krone.
Afsnit~\lref{s:graphnot} handler om grafer, et begreb som spiller en stor rolle i resten af bogen.
I afsnit~\lref{s:P and NP} diskuteres spørgsmålet, hvornår man skal betegne en algoritme som effektiv, og kompleksitetsklasserne $\classP$ og $\NP$ samt den vigtige klasse af $\NP$-fuldstændige problemer. 
Som alle andre kapitler slutter kapitlet med implementationsaspekter
(afsnit~\lref{s:implementation}) og historiske anmærkninger og videre resultater 
(afsnit~\lref{s:further}).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Asymptotisk notation}\llabel{s:o}

Algoritmeanalysens formål er primært at skabe tilforladelige udsagn om algoritmers opførsel, bl.a. deres kørselstid, som er både præcise, kortfattede, almentgyldige og begribelige.
Det er selvfølgeligt vanskeligt at opfylde alle disse krav samtidigt.
For fx at beskrive algoritmens tidsforbrug $T$
\index{tid|siehe{kørselstid}}
\index{regnetid|siehe{kørselstid}}
\index{beregningstid|siehe{kørselstid}}
\index{udførelsestid|siehe{kørselstid}}
\index{kørselstid}
kan man opfatte $T$ som en funktion, der afbilder mængden
 $\Inputs$
 af alle mulige probleminstanser (eller \emph{input})
\index{instans} 
til mængden $\RR_+$ af positive reelle tal.
For hver instans 
\index{probleminstans}
\index{instans}
$\Input$ til problemet er da $T(\Input)$ kørselstiden på $\Input$. 
Denne detaljeringsgrad fører dog til så overvældende meget information, at det ville være håbløst at udvikle en brugbar teori.
I stedet skal vi betragte  algoritmens opførsel i et mere mere almengyldigt perspektiv.

Vi vil opdele mængden af instanser i klasser af »lignende« instanser og så sammenfatte algoritmens opførsel på instanser fra samme klasse som et eneste tal.
Det mest gængse kriterium for klassedelingen er 
\index{instansstørrelse}
\index{størrelse!instans-}
instansens \emph{størrelse}.
Sædvanligvis er der en naturlig måde for at bestemme instansens størrelse.
Størrelsen på et heltal er antallet af cifre i dens binær\-repræsentation;
størrelsen af en mængde er dens kardinalitet, dvs. antallet af elementer.
Instansstørrelsen er altid et naturligt tal.
Sommetider bruger man mere end én parameter for at angive størrelsen på en instans; fx er det gængs at karakteriser størrelsen på en graf i termer af både antal knuder og antal kanter.
Vi vil i første omgang se bort fra de komplikationer, der optræder herved.
% TODO size notation introduced in original, but never used.
%Størrelsen af instansen $\Input$ skrives som $\Size(\Input)$,
%\index{Size@\ensuremath{\Size}|siehe{instansstørrelse}}
%og
Mængden af alle instanser af størrelse $n$ skrives som  $\Inputs_n$ for $n \in \NN$. 
For instanser af størrelse $n$ kan vi interessere os for maksimale, minimale og gennemsnitlige kørselstider, defineret på følgende måde:
\footnote{Vi vil altid sikre, at mængden 
  $\setGilt{T(\Input)}{\Input\in\Inputs_n}$ har både maksimum og minimum, og at mængden 
  $\Inputs_n$ er endelig, når vi beregner gennemsnit.}
%
\[ 
T(n) = \begin{cases}
  \max\setGilt{T(\Input)}{\Input\in\Inputs_n} &
  \text{»i værste fald«}\,, \\
  \min\setGilt{T(\Input)}{\Input\in\Inputs_n} &
\text{»i bedste fald«}\,, \\
      \displaystyle\frac{1}{|\Inputs_n|}\sum_{\Input\in \Inputs_n}T(\Input) &
  \text{»i gennemsnit«}\,.
\end{cases}
\]
\index{værste fald|sieheunter{kørselstid}}
\index{bedste fald|sieheunter{kørselstid}}
\index{gennemsnit|sieheunter{kørselstid}}
\index{kørselstid!værste fald|textbf}
\index{kørselstid!bedste fald|textbf}
\index{kørselstid!gennemsnit|textbf}%
Den mest interessante af disse størrelse er kørselstiden i værste fald, fordi den udgør den mest omfattende garanti for algoritmens opførelse.
Sammenligningen af opførelsen i bedste og værste fald fortæller os, hvor stor variation i kørselstid der kan forekomme mellem instanser i samme størrelsesklasse.
Når afvigelsen mellem bedste og værste fald er meget stor, kan sommetider en analyse af den gennemsnitlige kørselstid give nærmere indsigt i algoritmens faktiske tidsforbrug.
Vi skal se nærmere på en eksempel i afsnit~\lref{s:average case analysis}.

Vi går endnu et skridt videre i vores forsøg på at informationen mere overskuelig ved at gøre analysen grovere:
Vi koncentrerer os på kørselstidens \emph{vækstrate} ved at bruge \emph{asymptotisk analyse}.
\index{asymptotisk analyse}  
To funktioner $f(n)$ og $g(n)$ har samme 
\index{vækstrate}
\emph{vækstrate}, hvis der eksisterer positive konstanter $c$ og $d$, så uligheden $c\le f(n)/g(n)\le d$ gælder der for alle tilstrækkeligt store $n$.
Funkionen $f(n)$ \emph{vokser hurtigere} end $g(n)$, hvis der gælder for hver positive konstant $c$, at uligheden $f(n)\ge c\cdot g(n)$ for alle tilstrækkeligt store $n$.
For eksempel har funktionerne $n^2$, $n^2 + 7n$, $5n^2 - 7n$ og
$\frac{1}{10}n^2 + 10^6 n$ alle samme vækstrate.
Desuden vokser disse funktioner alle hurtigere end funktionen $n^{3/2}$, som selv vokser hurtigere end funktionen  $n \log n$.
Læg mærke til, at vækstraten fokuserer på opførslen for store $n$, hvilket også er meningen med begrebet »asymptotisk« i »asymptotisk analyse«.\footnote{Ovs. anm.:
Ordet »asymptotisk« er kendt fra matematisk analyse, hvor det betegner opførslen af en funktion, som nærmer sig en grænseværdi uden at antage den.
Funktionen $x\mapsto 1/x$ har fx $y$-aksen som asymptote for $x\rightarrow 0$.
Ordet er græsk og betyder »ikke-sammenfaldende« og betoner altså, at funktionen nærmer sig, men aldrig helt når, en bestemt linje.
I modsætning hertil vil man lede forgæves efter en god forklaring for, hvorfor man i algoritmeanalysen bruger »asymptotisk« for at betegne noget i retning af »opførsel for store $n$«.}

Hvad er grunden til, at vi kun interesserer os for vækstrate og opførslen for store $n$?
Hovedårsagen for at udvikle effektive algoritmer er netop ønsket om at kunne håndtere store instanser.
Når algoritmen $A$ har en lavere vækstrate end algoritmen $B$ for samme problem, vil $A$ typisk være $B$ overlegen for store $n$.
Desuden er vores maskinmodel 
\index{maskinmodel}
i forvejen en abstraktion af de faktiske kørselstider og nøjes med at bestemme en konkret maskines opførelse inden for en maskinafhængig konstant faktor.
\index{konstant faktor}
Derfor vil vi ikke skelne mellem algoritmer, hvis kørselstider har samme vækstrate.
Vores indskrænkning til vækstrater har desuden den glædelige sideeffekt, at algoritmers kørselstider kan karakteriseres af meget enkle funktioner.
Vi vil dog i bogens implementationsafsnit regelmæssigt se lidt nærmere på de maskinnære detaljer, som den asymptotiske analyse er blind for. 
Generelt bør læseren ved studiet og anvendelsen af algoritmer i denne bog altid spørge sig selv, om det asymptotiske perspektiv er relevant.

Vi skal nu indføre den gængse notation for funktioners \emph{asymptotiske opførsel}.
\index{asymptotisk|textbf}%
\index{omega@$\Omega(\cdot)$|textbf}% 
\index{omega@$\omega(\cdot)$|textbf}% 
\index{o@$o(\cdot)$|textbf}% 
\index{theta@$\Theta(\cdot)$|textbf}% 
\index{o@$O(\cdot)$|textbf}
Her betegner $f(n)$ og $g(n)$ funktioner, som afbilder naturlige tal til ikke-negative reelle tal. 
Vi definerer
\begin{align*}
  O(f(n)) & = \{\,g(n)\colon\exists c>0\colon\exists n_0\in\NN_+\colon\forall n\geq n_0\colon g(n)\leq c\cdot f(n)\}\,,\\
\Omega(f(n)) & = \{\,g(n)\colon\exists c>0\colon\exists n_0\in\NN_+\colon\forall n\geq n_0\colon g(n)\geq c\cdot f(n)\,\}\,,\\
  \Theta(f(n)) & = O(f(n))\cap{}\Omega(f(n))\,,\\
o(f(n)) & = \{\,g(n)\colon\forall c>0\colon\exists n_0\in\NN_+\colon\forall n\geq n_0\colon g(n)\leq c\cdot f(n)\,\}\,,\\
\omega(f(n)) & = \{\,g(n)\colon\forall c>0\colon\exists n_0\in\NN_+\colon\forall n\geq n_0\colon g(n)\geq c\cdot f(n)\,\}\,.
\end{align*}
Venstresiderne læses som »store-o af $f(n)$« og tilsvarende for  »store-omega«, »theta«, »lille-o« og »lille-omega«.
Læg mærke til, at »$f(n)$« i udtrykket  »$O(f(n))$« og »$g(n)$« i udtrykket »$\{\,g(n)\colon \ldots\}$« betegner funktionerne $f$ og $g$ -- notationen forsøger blot at tydeliggøre, at funktionen afhænger af variablen $n$.
Derimod menes i betingelsen »$\forall n\geq n_0\colon g(n) \le c\cdot f(n)$« funktions\emph{værdien} for $n$.

Lad os betragte nogle eksempler.
Mængden $O(n^2)$ indeholder de funktioner, som vokser højst kvadratisk.
Mængden $o(n^2)$ indeholder de funktioner ,som vokser langsommere end kvadratisk.
Mængden $o(1)$ indeholder de funktioner, som går mod $0$ for voksende $n$, hvor strengt taget symbolet »$1$« betegner den konstante funktion $n \mapsto 1$, som altid har funktionsværdien $1$.
Dermed tilhører funktionen $f(n)$ mængden $o(1)$, hvis $f(n) \le c\cdot 1$ for hvert positive $c$ og tilstrækkeligt stort $n$, dvs. når $f(n)$ går mod $0$ for voksende $n$.
Generelt kan man tænke på $O(f(n))$ som mængden af funktioner, som »ikke vokser hurtigere end« $f(n)$; og på $\Omega(f(n))$ som mængden af funktioner, som »vokser mindst lige så hurtigt som« $f(n)$.
For eksempel ligger den asymptotiske værstefaldstid for Karatsubas algoritme for heltalsmultiplikation i $O(n^{1,585})$, mens den asymptotiske kørselstid for skolemetoden ligger i $\Omega(n^2)$. 
Derfor kan vi sige, at Karatsubas algoritme er hurtigere end skolemetoden.
Notationen $o(f(n))$ angiver mængden af funktioner, som »vokser skarpt langsommere end« $f(n)$.
Dens modsætning, notationen $\omega(f(n))$, forekommer ganske sjældent i den grundlæggende algoritmeanalyse og er her kun medtaget for fuldstændighedens skyld.

De fleste algoritmer i denne bog har kørselstider, som kan skrives som polynomium eller som logaritmisk funktion, eller som produkt af den sådanne funktioner.
Det næste resultat ser nærmere på polynomier i det asymptotiske perspektiv; beviset giver nogle eksempler på omgangen med notationen.

\begin{lemma}\llabel{lem:polynomial}
  Lad $p(n)=\sum_{i=0}^ka_in^i$ være et polynomium med reelle koefficienter, hvor $a_k>0$.
  Da gælder $p(n)\in \Theta(n^k)$.
\end{lemma}
\begin{proof}
  Vi skal vise $p(n)\in O(n^k)$ og $p(n)\in \Omega(n^k)$.
  Vi bemærker først, at der for $n>0$ gælder
  \[ p(n)\leq\sum_{i=0}^k\abs{a_i}n^i\leq n^k\sum_{i=0}^k\abs{a_i}\,, \]
  hvilket medfører $p(n)\leq (\sum_{i=0}^k\abs{a_i})n^k$ for alle positive $n$. 
  Derfor gælder $p(n)\in O(n^k)$.

  Sæt $A=\sum_{i=0}^{k-1}\abs{a_i}$.
  For alle $n>0$ har vi nu
\[ p(n)\geq
  a_kn^k-An^{k-1}=\frac{a_k}{2}n^k+n^{k-1}\left(\tfrac{a_k}{2}n-A\right)\,, \]
  og derfor $p(n) \geq (\frac{1}{2}a_k)n^k$ for $n > 2A/a_k$.
  Ved at vælge  $c=\frac{1}{2}a_k$ og $n_0=2A/a_k$ i definitionen af
  $\Omega(n^k)$ ses nu, at $p(n)$ tilhører $\Omega(n^k)$.
\qed\end{proof}


\begin{exerc} 
  Sandt eller falskt? 
  (a)~$n^2 + 10^6 n \in O(n^2)$; 
  (b)~$n \log n
\in O(n)$; (c)~$n \log n \in \Omega(n)$; (d)~$\log n \in o(n)$. 
\end{exerc}

Asymptotisk notation er så udbredt i algoritmeanalysen, at man af bekvemmelighedsgrunde ofte anvender den præcise notation på en mere fleksibel måde.
Ikke mindst benytter man ofte betegnelser for funktionsmængder (fx $O(n^2)$) som om de selv var en enkelt funktion. 
Især plejer man at skrive $h(n)= O(f(n))$ i stedet for $h(n)\in O(f(n))$ og $O(h(n))= O(f(n))$ is stedet for $O(h(n)) \subseteq O(f(n))$, som fx:
\[   3n^2 + 7n = O(n^2) = O(n^3) \,. \]
Følger af »ligninger« med $O$-notation skal strengt taget opfattes som udsagn om tilhørsforhold og mængdeinklusioner, og de giver kun mening læst fra venstre til højre.

For en funktion  $h(n)$, funktionsmængder $F$ og $G$ og en operator $\diamond$ (fx $+$, $\cdot$ eller $/$) lad $F\diamond G$ være en forkortelse for $\{\,f(n) \diamond g(n)\colon f(n)\in F, g(n)\in G\,\}$, og $h(n)\diamond F$ være en forkortelse for $\{h(n)\} \diamond F$. 
Med denne konvention betegner $f(n)+o(f(n))$ altså mængden af funktioner $f(n) + g(n)$ med den egenskab,  at $g(n)$ vokster stærkt langsommere end als $f(n)$, dvs. at kvotienten $(f(n) + g(n))/f(n)$ går mod $1$ for $n\to\infty$.
Ækvivalent skrives $(1+o(1))f(n)$.
Vi bruger denne notation, når vi vil understrege $f(n)$s rolle som
»\emph{førende term}«,
\index{førende term|textbf}
i forhold til hvilken »\emph{termer af lavere orden}«
\index{term af lavere orden|textbf}
kan ignoreres.

\begin{lemma}[regneregler]
\llabel{lem:ocalculus} 
Der gælder: 
\begin{align*}
cf(n)&=\Theta(f(n))\text{, for hver positive konstant $c$,}\\
f(n)+g(n)&=\Omega(f(n))\,,\\
f(n)+g(n)&=O(f(n))\text{, når }g(n)=O(f(n))\,,\\
O(f(n)) \cdot O(g(n)) &= O(f(n) \cdot g(n))\,.
\end{align*}
\end{lemma} 


\begin{exerc}
Bevis lemma~\lref{lem:ocalculus}.
\end{exerc}



\begin{exerc}
Skærp lemma~\lref{lem:polynomial} ved at vise $p(n)=a_kn^k+o(n^k)$.
\end{exerc}

\begin{exerc} 
Bevis, at der gælder $n^k = o(c^n)$ for heltal $k$ og vilkårligt $c > 1$. 
Hvor står $n^{\log\log n}$ i forhold til $n^k$ og $c^n$?
\end{exerc}

\section{Maskinmodellen}
\llabel{s:model}

\begin{wrapfigure}{r}{4cm}
\begin{center}
\includegraphics[width=3cm]{img/portre.pdf}
\end{center}
\caption{\llabel{fig:john}John von Neumann, ${}^*\,$28{.}12{.}1903 i Budapest,
$\dag\,$8{.}2{.}1957 i Washington, DC.}
\end{wrapfigure}

\index{maskinmodel|textbf}%
\index{model|textbf}%     
År 1946 foreslog John von Neumann  
(fig.~\lref{fig:john})
\index{von Neumann@von Neumann, J.}%
\index{von Neumann-maskine|sieheunter{maskinmodel}}
\index{maskinmodel!von Neumann}    
en enkel, men magtfuld beregningsarkitektur~\cite{Neu45}.
Den tids begrænsede hardwaremuligheder førte ham til en elegant model, som indskrænkede sig til det væsentlige; ellers havde realiseringen ikke været mulig.
I årene efter 1945 har hardwareteknologien godtnok videreudviklet sig dramatisk, men von Neumanns programmeringsmodel
\index{programmeringsmodel|siehe{maskinmodel}},
er så elegant og kraftfuld, at den den dag i dag udgør grundlaget for en stor del af programmeringen.
Normal virker de programmer, som er udviklet i von Neumann-modellen også i nutidens meget mere komplekse hardware.

Til analyse af algoritmer benytter man en variant av von Neumann-maskinen, som hedder \emph{registermaskinen} eller ram (\emph{random access machine}),
\index{ram|sieheunter{maskinmodel}}%
\index{registermaskine|sieheunter{maskinmodel}}%
\index{maskinmodel!ram|textbf} 
\index{maskinmodel!registermaskine|textbf}%
en maskine med vilkårlig lageradgang).
Denne model blev foreslået i 1963 af Shepherdson
\index{Shepherdson, J.} og Sturgis~\cite{Shepherdson-Sturgis}. 
\index{Sturgis, H.}
Det drejer det sig om en \emph{sekventiel} regner
\index{maskinmodel!sekventiel} 
med uniformt lager, dvs. at der findes bare én centralenhed (CPU),
\index{cpu}
og hver lageradgang koster samme tid.
Lageret består af uendeligt mange lagerceller $S[0]$, $S[1]$, $S[2]$, $\ldots$, hvoraf kun et endeligt antal er i brug til enhver tid.
Derudover råder registermaskinen over et lille, konstant antal \emph{registre} $R_1,\ldots, R_k$.
\index{register|textbf}

Lagercellerne og registrene
\index{lagercelle}
\index{celle}
har plads til »små« heltal, som også kaldes (\emph{maskin})-\emph{ord}.
\index{maskinmodel!ordmodel}
\index{ord|siehe{maskinord}}
\index{maskinord}
%TODO: maskineord? maskinemodel
I vores overvejelser om heltalsaritmetik i kap.~\ref{ch:amuse:} gik vi ud fra, at »lille« betyder »etcifret«.
Det er dog mere fornuftigt og bekvemt at antage, at definitionen af »lille« afhænger af inputstørrelsen.
Vores standardantagelse skal være, at et tal kan gemmes i en lagercelle, hvis dets størrelse er begrænset af et polynomium i inputstørrelsen.
Binærrepræsentationen af disse tal behøver et antal bit, der er logaritmisk i inputstørrelsen.
Denne antagelse er fornuftig, fordi vi altid kan opdele indholdet af en lagercelle i logaritmisk mange celler, som kun kan gemme en enkelt bit, uden at tid- eller pladsbehovet øges med mere end en logaritmisk faktor.
Idet registre bruges til at adressere lagerceller, vil vi forlange, at hver adresse, der forekommer i vores beregninger, finder plads i et register.
Hertil rækker det med en polynomiel begrænsning af størrelsen.

På den anden side er antagelsen om en logaritmisk grænse på bitlængden af de gemte tal også nødvendig:
Hvis man tillod, at en lagercelle kunne gemme tal af vilkårlig størrelse, ville det i mange tilfælde lede til algoritmer med absurd lille tidsforbrug.
Fx ville man men $n$ på hinanden følgende kvadreringer af tallet $2$ (som fylder $2$ bit) skabe et tal med $2^n$ bit
Man begynder med $2=2^1$, kvadrerer en gang for at få $2^2=4$, kvadrer igen for at få $16=2^{2\cdot 2}$, osv. 
Efter $n$ kvadreringer opnår man tallet $2^{2^n}$.

Vores model tillader en bestemt, ubegrænset form  for parallelisme:
enkle operationer på logaritmisk mange bit kan gennemføres i konstant tid.

En registermaskinen kan udføre et (maskin)program.
\index{maskinprogram|textbf} 
Sådan et \emph{program}
\index{program|textbf}
udgøres af en liste af maskinkommandoer,
\index{maskinkommando|siehe{kommando}}
\index{kommando|textbf}
nummereret fra $1$ til et tal $l$.
Indgangene i listen hedder \emph{linjer}
\index{programlinje|textbf} 
i programmet.
Programmet står i programlageret.
Vores registermaskine kan udføre følgende maskinkommandoer:
\begin{itemize}
  \item $R_i := S[R_j]$ \emph{læser}
    \index{læsekommando|textbf} 
    indholdet af den lagercelle, hvis indeks står i register $R_j$, ind i register $R_j$.
  \item $S[R_j]:= R_i$ \emph{skriver}
    \index{skrivekommando|textbf}
    indholdet af register $R_i$ ind i den lagercelle, hvis indeks står i register $R_j$.
  \item $R_i := R_j\odot R_h$ udfører en binær operation $\odot$ på indholdet af $R_j$ og $R_h$ og gemmer resultatet i register $R_i$.
    Der er en række muligheder for, hvad  »$\odot$« kan være.
    De \emph{aritmetiske}
    \index{aritmetik}
    operationer er som sædvanligt $+$, $-$ og $*$; disse fortolker registerindholdet som heltal.
    Operationerne $\div$ og $\mod$,
    \index{div@{\bf div}|textbf}
    \index{mod@{\bf mod}|textbf}
    \index{rest (ved heltalsdivision)|textbf}
    ligeledes på heltal, giver kvotienten hhv. resten ved heltalsdivision.
    \emph{Sammenligningsoperationerne}
   \index{sammenligning|textbf}%
   \index{mindre end ($<$)|textbf}%
   \index{lig med ($=$)|textbf}%
   \index{stxrre@større end ($>$)|textbf}%
   \index{logiske operationer|textbf}%
   \index{sandhedsvxrdi@sandhedsværdi|textbf}%
   \index{eksklusivt eller ($\oplus$)|textbf}%
   \index{ikke|textbf}%
   \index{og|textbf}%
   \index{eller|textbf}%
   \index{sand@\Id{sand}|textbf}%
   \index{falsk@\Id{falsk}|textbf}%
   \index{flydende komma|textbf}% 
    $\leq$, $<$, $>$ og $\geq$ giver deres sandhedsværdier som resultat, dvs. \emph{sand} ($=1$) og \emph{falsk}  ($=0$).
    Derudover findes de bitvise operationer $|$ (logisk \emph{eller}), \texttt{\&} (logisk \emph{og}) og $\oplus$ (eksklusiv \emph{eller}). disse fortolker registerindholdet som bitstreng.
    Operationerne \texttt{»} (højreskift) og \texttt{«} (venstreskift) tolker sit første argument som bitstreng og sit andet som ikkenegativ forskydningsværdi.
    De logiske operationer $\wedge$ og $\vee$ bearbejder på \emph{sandhedsværdierne} $1$ og $0$.
    Vi kan også antage, at der findes operationer, som tolker registerindholdet som et tal med flydende komma, dvs. som endelig tilnærmelse af et reelt tal.
  \item $R_i:=\odot R_j$ udfører en \emph{unær} operation
    \index{unær operation}
    på register $R_j$ og gemmer resultatet i register $R_i$.
    Hertil findes operationerne $-$ (for heltal), $\neg$ (logisk negation af sandhedsværdier) og \texttt{\~} (bitvis negation af bitstrenge).
  \item $R_i:= C$ tildeler register $R_i$ den \emph{konstante}
    \index{konstant|textbf} 
    værdi $C$.
  \item  \texttt{JZ} $k, R_i$ fortsætter beregningen på programlinje $k$, hvis register $R_i$ indeholder $0$ (\emph{forgrening}
    \index{forgrening}
    eller \emph{betinget hop}),
    \index{betinget hop}
    ellers fortsættes der på næste programlinje.\footnote{%
      Hopmålet $k$ kan, hvis det er nødvendigt, også være angivet som indhold $S[R_j]$ af en lagercelle.}
    \item \texttt{J} $k$ fortsætter beregningen på programlinje $k$ (\emph{ubetinget hop}).
      \index{ubetinget hop}
\end{itemize}

Et program af denne slags udføres skridt for skridt på givet input.
\index{beregningsmodel}
Input står i begyndelsen i lagercellerne $S[1]$, $\ldots$, $S[R_1]$, udførelsen begynder på programlinje $1$.
Med undtagelse af forgreningerne \text{JZ} og \text{J} følges udførelsen af en programlinje altid af udførelsen af den næste programlinje.
Udførelsen af programmet ender, hvis den skal udføre en linje, hvis nummer ligger uden for området $\{1,\ldots,l\}$.

Vi bestemmer tidsforbruget for at udføre et program på givet input på den enklest tænkelige måde:
\emph{Det tager præcis en tidsenhed at udføre en maskinkommando}.
\index{tidsenhed}
Et programs totale \emph{kørselstid} 
\index{kørselstid|textbf}
er summen af alle udførte kommandoer.
\index{kommando}

Det vigtigt at klargøre, at registermaskinen er en abstraktion; man må ikke forveksle modellen med en virkelig beregner.
Især har ægte elektroniske beregner
\index{beregner!virkelig}
et endeligt lager og et fast antal bit per register og lagercelle (fx 32 eller 64).
\index{register}
\index{maskinord}
I modsætning hertil vokser (»skalerer«) ordlængde og lagerstørrelse i en registermaskine med inputstørrelsen.
Man kan betrage dette som en abstraktion af den historiske udvikling: 
mikroprocessorer har i løbet af tiden haft maskinord af længde 4, 8, 16, 32 og 64 bit
Med ord af længde 64 kan man adressere et lager af størrelse $2^{64}$.
På grund af prisen for dagens lagermedier begrænses størrelsen af lageret i dag derfor af omkostningerne og ikke af længden af adresserne.
Interessant nok var dette også sandt ved indførelsen af 32-bit ord!

Vores model for beregningskompleksitet
\index{beregningskompleksitet}
\index{kompleksitet}
er groft forenklet på den måde, at moderne processorer prøver at gennemføre mange operationer samtidigt.
\index{parallelbehandling}
I hvilket omfang dette fører til en tidsbesparelse, afhænger af faktorer som dataafhængighed
\index{dataafhængighed}
mellem påhinandenfølgende operationer.
Derfor er det ikke muligt at knytte en fast omkostning til hver operation.
Denne effekt gør sig ikke mindst gældende ved lageradgang.
\index{lageradgang}
I værste fald kan lageradgangen bruge mange hundrede gange så lang tid som det bedste fald!
Dette hænger sammen med at moderne processorer prøver at holde hyppigt benyttet data i \emph{skyggelageret},
\index{skyggelager}
hvilket er en forholdsvis lille, hurtig lagerenhed som ligger tæt forbundet til processoren.
Hvilken tidsforbedring opnås af skyggelageret afhænger stærkt af maskinarkitekturen, programmet og det konkrete input.

Vi ville kunne forsøge at udvikle en mere nøjagtig omkostningsmodel,
\index{maskinemodel!nøjagtig}
\index{maskinemodel!kompleks}
\index{maskinemodel!enkel}
men det skulle være at skyde over målet.
Resultatet er en megen kompleks model, som er vanskelig at arbejde med.
Selv en vellykket analyse leder til monstrøse formler, som afhænger af mange parametre og som desuden skulle ændres ved hver ny generation af processorer.
Selvom informationen i sådan en formel ville være meget præcis, ville den være ganske ubrugelig allerede i kraft af sin kompleksitet.
Derfor går vi til den andet yderpunkt ved at eliminere samtlige modelparametre og blot antage, at hver maskinkommando tager én tidsenhed.
Det leder til, at konstante faktorer ikke spiller nogen rolle i vores model -- endnu en grund til at for det meste holde sig til asymptotisk
\index{asymptotisk}
analyse af algoritmer.
Som udligning finder der i hvert kapitel et afsnit om implementationsaspekter, i hvilket vi diskuterer implementationsvarianter og afvejninger
\index{afvejning}
som afhængigheder mellem behovet af forskellige resurser som tid og lagerplads.

\subsection{Baggrundslager}
\index{maskinmodel!baggrundslager}
\index{baggrundslager|sieheauch{maskinmodel}}%
\index{maskinmodel!fjernlager}
\index{fjernlager|sieheauch{maskinmodel}} 
Den mest dramatiske forskel mellem registermaskinen og en virkelig regnemaskine er lagerstrukturen:
regnemaskinens uniforme lager er en utilstrækkelig beskrivelse af den virkelige maskines komplekse lagerhierarki.
I afsnit ~\ref{ch:sort:s:external}, \ref{ch:pq:s:external} og~\ref{ch:search:s:implementation} skal vi betrage algoritmer, som er skræddersyede til store datamængder, som skal holdes i det langsomme baggrundslager som fx på et eksternt hårddrev.
For at undersøge den slags algoritmer benytter vi \emph{baggrundslagermodellen}.

Yderlagermodellen ligner registermaskinen, men adskiller sig i lagerstrukturen.
Det (hurtige) indre lager
\index{hurtigt lager}
\index{indre lager}
\index{M@$M$ (størrelsen af det hurtige lager)}
(sommetider kaldt »hovedlageret«) består af kun $M$ ord; det (langsomme) ydre lager (sommetider kaldt »baggrundslageret«) har ubegrænset størrelse.
Der findes en \emph{blokflytnings\-operation},
\index{blokflytning}
\index{B@$B$ (blokstørrelse)}
\index{blok|siehe{lagerblok}}
\index{lagerblok|textbf},
som flytter $B$ på hinanden følgende lagerceller mellem det langsomme og det hurtige lager.
\index{hurtigt lager}
\index{langsomt lager}
Hvis yderlageret fx er et hårddrev,
\index{hxrddrev@hårddrev}
vil $M$ betegne størrelsen af hovedlageret og $B$ betegne blokstørrelsen for dataoverførsel mellem hovedlager og hårddrevet, som repræsenterer et godt kompromis mellem den store ventetid (latens)
\index{latens}
%TODO latenstid? ventetid?
og den store båndbredde
\index{båndbredde}
ved overførslen fra et lagermedium til det andet.
I skrivende stund er $M= 2$~Gbyte og $B=2$~MByte realistiske værdier.
En blokflytning varer omtrent 10~ms, hvilket svarer til $2\cdot 10^7$ klokcykler
\index{klokcykler}
% TODO klokcykel
i en 2~GHz-processor.
Med andre fastlæggelser for parametrene $M$ og $B$ kan vi modellere den mindre forskel mellem maskinens skyggelager og hovedlageret.

\subsection{Parallelbehandling}
\index{parallelbehandling|textbf}
\index{maskinmodel!parallel|textbf}

I moderne regnemaskiner findes mange slags parallelbehandling.
Mange processorer råder over \emph{SIMD}-registre
\index{SIMD|textbf}
\index{register}
med en bredde mellem 128 og 256 bit, som muliggør parallel udførelse af en kommando på en hel række dataobjekter (SIMD står for \emph{single instruction, multiple data}: enkel operation -- multiple data).

\emph{Simultan flertråding}
\index{flertråding}
\index{tråd}
gør det muligt for processer at bedre udnytte deres resurser ved at flere tråde (delprocesser) udføres samtidigt på en processorkerne.
\index{processorkerne}
\index{kerne}
Sågar mobile enheder
\index{mobil enhed}
indeholder ofte flerkerneprocessorer,
\index{flerkerneprocessor|textbf}
som kan udføre programmer uafhængigt af hinanden, og de fleste værtsmaskiner har flere af den slags flerkerneprocessorer, som har adgang til et \emph{fælles lager}.
\index{lager!fælles|textbf}

Koprocessorer, 
\index{koprocessor}
især dem, der bruges i computergrafik,
\index{computergrafik}
\index{grafikprocessor}
indeholder endnu flere parallelt arbejdende komponenter på en og samme chip.
Højpræstationsregnere
\index{højpræstationsregner}
\index{vært}
består af flere værtssystemer, som er sammenkoblet i et eget hurtigt netværk.
\index{netværk}
Endeligt findes muligheden for at ganske løst sammenkoble al slags computere i et netværk (internettet, radionetværk, osv.), som herved danner et \emph{fordelt system}, 
\index{fordelt system}
i hvilken millioner af knuder arbejder sammen.
Det er klart, at ingen simpel model rækker til at beskrive parallelle programmer, som kører på så mange forskellige niveauer af parallelitet
I denne bog vil vi derfor indskrænke os til af og til (og uden formel argumentation) at forklare, hvorfor en bestemt sekventiel algoritme er mere eller mindre velegnet til at blive omarbejdet til parallel udførelse.
For eksempel ville man kunne bruge SIMD-instruktioner til algoritmerne for aritmetik for lange heltal
\index{aritmetik} i kapitel~\ref{ch:amuse:}.
\section{Pseudokode}
\llabel{s:pseudocode}
\index{pseudokode|textbf}


Registermaskinen er en abstrakion og forenkling af maskinprogrammer, som skal udføres på en mikroprocessor, der muliggør en præcis definition af begrebet »udførelsestid«.
Modellen er samtidigt netop på grund af sin enkelhed uegnet til formuleringen af komplekse algoritmer, fordi de tilsvarende registermaskineprogrammer bliver alt for lange og stort set ulæselige for mennesker.
I stedet vil vi formulere vores programmer i \emph{pseudokode}, som danner en passende abstraktion og forenkling af imperative programmeringssprog som C, C++, Java, C\# og Pascal,
\index{programmeringssprog}
\index{imperativ programmering}
\index{C}
\index{C++@\CC}
\index{Java}
\index{Pascal}
\index{Csharp@C\#}
kombineret med rundhåndet brug af matematisk notation.
Vi skal nu beskrive vores konventioner for pseudokodenotation i denne bog, og præsentere en tidsmålingsmodel for pseudokodeprogrammer.
Denne er meget enkel: 
\emph{Grundlæggende pseudokodeinstruktioner behøver konstant tid; procedure- og funktionskald behøver konstant tid plus tiden for at udføre deres krop.}
Vi skal retfærdiggøre dette ved at skitsere, hvordan man kan oversætte pseudokode til ækvivalent registermaskinkode, som netop har denne tidsopførsel.
Vi skal nøjes med at forklare denne oversættelse så langt, at tidsmålingsmodellen bliver forståelig.
Vi bland andet enhver overvejelse om optimeringsteknikker for oversættere,
\index{oversætter}
idet konstante faktorer ikke spiller nogen rolle i vores teori.
Læseren er dog velkommen til at bladre forbi de næste afsnit og acceptere tidsmålingsmodellen for pseudokode som aksiomatisk.
Den her benyttede syntax ligner programmeringssproget Pascal~\cite{JenWir91}, fordi denne notation virker typografisk mere behagelig i bogform end den velkendte syntax fra C og dette sprogs videreudviklinger som C++ og Java.

\subsection{Variabler og elementære datatyper}

En \emph{variabelerklæring}
\index{deklaration|textbf}
\index{variabel|textbf}
»\DeclareInit{$v$}{$T$}{$x$}« 
introducerer en variabel $v$ af typen $T$
\index{type|textbf}
\index{datatype|sieheunter{type}}
og initialiserer
\index{initialiserer|textbf}
den med værdien $x$.
For eksempel skaber »\DeclareInit{\Id{svar}}{$\NN$}{$42$}«
en variabel $\Id{svar}$, som kan antage ikke-negative heltalsværdier, og giver den værdien $42$.
Sommetider undlader vi at nævne typen af en variabel, hvis den fremgår af sammenhængen.
Som typer forekommer elementartyper som fx heltal,
\index{heltal|textbf} 
booleske værdier,
\index{boolsk}
pegere (referencer)
\index{referencetype!sieheunter{peger}}
\index{peger}
og sammensatte typer.
\index{type!sammensat}
Vi fordefinerer nogle sammensatte typer, nemlig rækker og visse anvendelsesspecifikke klasser
\index{klasse}
(se nedenfor).
Når typen af en variabel er irrelevant for fremstillingen, benytter vi den uspecificerede type $\Id{Element}$
\index{element|textbf}
som pladsholder for en vilkårlig type.
Sommetider udvides de numeriske typer
\index{type!numerisk}
med værdierne $+\infty$ og $-\infty$,
\index{uendeligt ($\infty$)|textbf}
når det er bekvemt.
Ligeledes udvider vi sommetider typer med den udefinerede værdi (betegnet med symbolet $\bot$),
\index{udefineret værdi ($\bot$)|textbf}
som skal kunne skelnes fra »egentlige« objekter af typen $T$.
Især ved pegertyper er en udefineret værdi nyttig.
En værdi af pegertypen »\PointerTo $T$« kaldes et \emph{greb}
\index{greb}
om et objekt af typen $T$.
I registermaskinen er sådan et greb bare indeks (kaldt »adressen«)
\index{adresse}
af første celle af det lagersegment, som indeholder et objekt af typen $T$.

Erklæringen »\Declare{$a$}{\Array$[i..j]$ \Of $T$}« introducerer \emph{rækken}
\index{række|textbf}
$a$, som består af $j-i+1$ \emph{indgange} af typen $T$, hvilke er gemt i lagercellerne $a[i]$, $a[i+1]$, $\ldots$, $a[j]$.
Rækker er registermaskinen realiserede som sammenhængende lagersegmenter.
For at finde indgangen ved $a[k]$ er det nok at kende startadressen af $a$ og størrelsen af objekter af typen $T$.
Hvis fx registeret $R_a$ indeholder startadressen for rækken~$a$, registeret $R_i$ indeholder indeks $42$, og hvis hver indgang fylder et lager\-ord, så læser kommandosekvensen »$R_1:= R_a+R_i; R_2:= S[R_1]$« inholdet af $a[42]$ ind i register $R_2$.
Størrelsen af rækken ligger fast, når erklæringen er udført; den slags rækker hedder \emph{statiske}.
\index{statisk række|textbf}
I afsnit~\ref{ch:sequence:s:array} viser vi, hvordan man implementerer \emph{dynamiske rækker}, som kan vokse og krympe i løbet af programudførelsen.

Erklæringen »$c \colon \Class \Id{alder} \colon \NN, \Id{indkomst} \colon \NN$«
stiller en variabel $c$ til rådighed, hvis værdier er heltalspar.
Komponenterne kaldes sommetider felter og angives som $c.\Id{alder}$ og $c.\Id{indkomst}$.
Udtrykket »$\Address c$«
\index{greb}
\index{afreferering}
giver et greb om $c$ (dvs.\ adressen på $c$).
Når $p$ er en variabel af en passende pegertype, så kan vi gemme grebet i $p$ med kommandoen »$p := \Address c$«; tilsvarende giver udtrykket »$\Star p$« objektet $c$ tilbage.
De to felter
\index{felt (af en variabel)}
tilhørende $c$ kan også betegnes som $p\Points \Id{alder}$ og $p\Points\Id{indkomst}$.
Den alternative skrivemåde $(\Star p).\Id{alder}$ og $(\Star p).\Id{indkomst}$ giver mening, men bliver sjældent brugt.

Pladsen for rækker og objekter stilles til rådighed med kommandoen $\tAllocate$ og frigøres igen med kommandoen $\tDispose$.
\index{alloker@{\bf alloker}|textbf}
\index{afalloker@{\bf afalloker}|textbf} 
For eksempel stiller kommandoen »$p:=\Allocate \Array[1..n] \Of T$«
\index{af@{\bf af} (i typeerklæring)}
en række med $n$ objekter af typen $T$ til rådighed, dvs.\ at der reserveres et sammenhængende lagersegment med plads til akkurat $n$ objekter af typen $T$, og variablen $p$s værdi sættes til rækkens greb (dvs.\ lagersegmentets startadresse).
Kommandoen $\Dispose p$
\index{oversættelse (af pseudokode)}
frigiver lagerområdet, så det kan anvendes til andet brug.
Med operationerne $\tAllocate$ og $\tDispose$ kan vi opdele rækken $S$ af registermaksinens lagerceller i disjunkte segmenter, som kan tilgås enkeltvis.
Begge funktioner kan implementeres på en sådan måde, at de kun kræver konstant tid, 
\index{lagerforvaltning|textbf}
fx på følgende ekstremt nemme måde:
Adressen til den første ledige lagercelle i $S$ holdes i specialvariablen $\Id{fri}$.
Et kald til $\tAllocate$ reserverer et lagerafsnit, som begynder ved $\Id{fri}$, og øger derefter $\Id{fri}$ med omfanget af det reserverede afsnit.
Et kald til $\tDispose$ har ingen effekt.
Denne implementation er tidsbesparende men ikke pladsbesparende:
Godtnok kræver både $\tAllocate$ og $\tDispose$ kun konstant tid, men det totale lagerforbrug er summen af længderne af samtlige nogenside reserverede segmenter i stedet for maksimum af den til et bestemt tidspunkt brugte (dvs. reserverede, men endnu ikke frigivne) plads.
Det er et åbent problem at realisere en vilkårlig følge af $\tAllocate$- og $\tDispose$-operationer både lagereffektivt og med konstant tidsforbrug per operation.
For de algoritmer, som præsenteres i denne bog, kan $\tAllocate$ og $\tDispose$ dog realiseres både tids- og pladseffektivt på samme tid.

Vi vil gøre brug af nogle sammensatte datatyper fra matematiken.
Især vil vi benytte tupler,
\index{tupel|textbf}
endelige følger
\index{følge|textbf}
og mængder.
\index{mængde|textbf}
Ordnede \emph{par},
\index{par|textbf}
\emph{tripler}
\index{tripel|textbf}
og andre \emph{tupler} skriver vi i parenteser som fx $(3,1)$, $(3,1,4)$ og $(3,1,4,1,5)$.
Idet antallet af komponenter i en tupel er fastlagt af typen, kan operationer på tupler brydes ned i operationer på de enkelte komponenter på den oplagte måde.
En \emph{følge},
\index{fxlge@følge|textbf}
\index{af@{\bf af} (i typeerklæring)}
skrevet i spidse parenteser, gemmer objekter af samme type i en bestemt orden; i modsætning til tupler er antallet af indgange ikke fastlagt af typen.
For eksempel erklærer tildelingen »$\DeclareInit{s}{\Id{Følge} \Of \ZZ}{\seq{3,1,4,1}}$« en følge $s$ af heltal og initialiserer den med følgen $\langle 3,1,4,1\rangle$ af tallene $3$, $1$, $4$ og $1$ i den rækkefølge.
Den tomme følge skrives som $\langle\,\rangle$.
\index{tom følge $\seq{\,}$|textbf} 
Følgen er en naturlig abstraktion af mange andre datatyper som fx arkivet
\index{arkiv}
(eng. \emph{file}), strengen
\index{streng}
(eller »tegnfølgen«), listen,
\index{liste}
stakken
\index{stak}
og køen.
\index{kø}
I kap.~\ref{ch:sequence:} skal vi se grundigt på mange forskellige måder at repræsentere følger.
I de senere kapitler vil vi så bruge følger som matematisk abstraktion uden at tage videre hensyn til implementationsdetaljer.

Mængder
\index{mxngde@mængde|textbf}
spiller en vigtig rolle i matematiske overvejelser, og vi vil ofte bruge dem i vores pseudokode.
Der vil altså forekomme erklæringer som fx »$\DeclareInit{M}{\Set \Of \NN}{\set{3,1,4}}$«, som skal forstås på samme måde som erklæringer af rækker eller mængder.
Implementationen af datatypen »mængde« sker ofte ved hjælp af følger.

\subsection{Kommandoer}
\index{kommando}

Den enkleste anvisning er \emph{tildelningen}
\index{tildeling}
»$x:= E$«, hvor $x$ er en variabel og $E$ er et udtryk.
En tildeling kan transformeres til en følge af operationer i registermaskinen af konstant længde.
\index{oversættelse!tildeling}
For eksempel bliver tildelingen $a:= a+b\cdot c$ oversat til »$R_1:= R_b*R_c$; $R_a:= R_a+R_1$«, hvor $R_a$, $R_b$ og $R_c$ angiver de registre, som indholder $a$, $b$ og $c$.
Fra programmeringssproget C låner vi forkortelserne $\Increment$
\index{zzg@øg ($\Increment$)|textbf}
og $\Decrement$
\index{sxnk@mindsk ($\Decrement$)|textbf}
for øgning og mindskning af en variabel med $1$.
Vi vil også bruge \emph{simultan tildeling} af flere variable.
Hvis fx $a$ og $b$ er variabler af samme type, så bytter
\index{ombyt@\Id{ombyt}}
kommandoen »$(a,b):= (b,a)$« indholdet af $a$ og $b$.

Den betingede kommando
\index{betinget kommando|textbf}
\index{hvis@{\bf hvis}|textbf}
$\If C\Then I \Else J$, hvor $C$ er et boolsk udtryk og $I$ og $J$ er anvisninger, oversættes til kommandofølgen
\[ \Id{eval}(C);\ \mathtt{JZ}\ \Id{sElse},\ R_C ;\ \Id{oversæt}(I); 
\ \mathtt{J}\ \Id{sEnd};\ \Id{oversæt}(J)  \]
De enkelte dele skal læses som følger:
$\Id{eval}(C)$ er en følge af kommandoer, som evaluerer udtrykket $C$ og gemmer resultatet i registret $R_C$;
$\Id{oversæt}(I)$ er en følge af anvisninger, som implementerer kommandoen $I$;
$\Id{oversæt}(J)$ implementerer tilsvarende kommandoen $J$;
$\Id{sElse}$ er nummeret på den første kommando i $\Id{oversæt}(J)$;
endelig er $\Id{sEnd}$ nummeret på den første kommando som følger efter $\Id{oversæt}(J)$.
Instruktionerne beregner først værdien af $C$.
Hvis denne er $\Id{falsk}$ (dvs.\ lig med $0$), så hopper registermaskinen til den første kommando i oversættelsen af $J$.
\index{oversættelse}
Hvis resultatet derimod er $\Id{sandt}$ (dvs.\ lig med $1$), så udfører programmet oversættelsen af $I$ og hopper derefter videre til den første kommando, som følger efter oversættelsen af $J$.
Vi skriver »$\If C \Then I$« som forkortelse for »$\If C \Then I\Else \,;$«, dvs.\ en betinget udførelse med tom ellers-gren.

Man kan gruppere enkelte kommandoer ved at sætte dem efter hinanden som $I_1;$ $\ldots;$ $I_r$, og kan opfatte denne gruppe som én (mere kompleks) kommando.
Ved oversættelsen til et maskinprogram bliver oversættelserne af de enkelte dele sat efter hinanden.

Vores pseudokodenotation for programmer er tiltænkt en menneskelig læser og følger derfor ikke en lige så streng syntaks som programmeringssprog
\index{programmeringssprog}
normalt skal gøre.
Blandt andet vil vi bruge indrykninger
\index{indrykning} for at gruppere anvisninger, hvilket sparer os for en masse parenteser sammenlignet med fx C eller Pascal.
Vi vil kun bruge parenteser til gruppering, hvis programmet ellers var flertydigt.
På samme måde bruger vi linjeskift i stedet for semikolon
\index{semikolon}
for at dele to anvisnigner.

Løkken
\index{løkke}
»\Repeat $I$ \Until $C$« oversættes til $\Id{oversæt}(I);\ \Id{eval}(C);\ \mathtt{JZ}\ \Id{sI},\ R_C$, hvor $\Id{sI}$ er nummeret på den første kommando i $\Id{oversæt} (I)$.
Vi skal bruge mange andre former for løkker, som man kan opfatte som forkortelser for forskellige $\Repeat$-løkker.
I følgende tabel angives til venstre nogle eksempler på den slags forkortelser, og til højre den fuldstændige formulering.

\medskip
\begin{tabular}{ll}
$\Kw{mens } C \Kw{ udfør } I$ &  $\Kw{hvis } C \Kw{ så gentag } I \Kw{ indtil } \neg C$\\
  $\Kw{for } i:= a \Kw{ til } b \Kw{ udfør } I$ &  $i:=a; \,\Kw{mens } i\leq b \Kw{ udfør } I;\, i\texttt{++} $\\
  $\Kw{for } i:= a \Kw{ to } \infty \Kw{ mens } C \Kw{ udfør } I\quad$ &  $i:=a;\, \Kw{mens } C\Kw{ udfør } I;\, i\texttt{++} $\\
$\Kw{for hvert } e\in s \Kw{ udfør } I$ &  $\Kw{for} i:=1 \Kw{ til } |s|\Kw{ udfør } e:=s[i];\, I$\\
\end{tabular}
\medskip

Ved oversættelsen
\index{oversættelse!løkke}
af løkker til registermaskinen er mange maskinnære forbedringer mulige, som vi dog ikke tager hensyn til her.
For vores formål er det kun af betydning, at tiden
\index{udførelsestid!løkke}
for udførelsen af en løkke er summen af de enkelte gennemløbstider samt udførelsestiden for at evaluere løkkens iterationsbetingelse.

\subsection{Procedurer og funktioner}
Et underprogram
\index{underprogram|textbf}
\index{procedure|textbf}
ved navn \Id{foo} erklæres som
»$\Procedure$ \Id{foo}$(D)$ $I$«.
Her er $I$ prodedurens krop, og $D$ en liste af variabelerklæringer, som angiver de formelle parametre 
\index{parameter|textbf}
\index{parameter!formel}
af proceduren \Id{foo}.
Et kald til $\Id{foo}$ har formen $\Id{foo}(P)$, 
hvor $P$ er en liste af \emph{argumenter} eller \emph{egentlige} parametre
\footnote{Ovs. anm.:
Man kan sommetider støde på begrebet »aktuel paremeter«,  som er en misforståelse af det engelske udtryk »\emph{actual parameter}«.}
\index{argument}
som har samme længde som listen af variabelerklæringer.
Argumenterne er enten værdier eller referencer.
Medmindre andet er sagt, går vi ud fra, at elementære objekter som fx heltal og boolske værdier overføres som værdier, mens komplekse objekter som fx rækker overføres som reference.
Denne konvention svarer til moderne sprog som C og sikrer, at parameteroverførelsen tager konstant tid.
Parameteroverførelsens semantik er følgende:
Når $\Id{x}$ er en værdiparameter af typen $T$, så skal det tilsvarende kaldsargument være et udtryk $E$, som leverer en værdi af typen $T$.
Parameteroverførelsen bliver derved ækvivalent til erklæringen af en lokal variabel ved navn $\Id{x}$, som initialiseres med værdien af $E$.
Når derimod $\Id{x}$ er en referenceparameter af typen $T$, så skal det tilsvarende kaldsargument være en variabel af samme type;
\index{parameter!formel}
\index{parameter!egentlig} 
i løbet af udførelsen af procedurekroppen er $\Id{x}$  blot et alterantivt navn til denne variabel.

Ganske som ved variabelerklæringer undlader vi sommetider at skrive typen i parametererklæringen, når vi finder den uvæsentlig eller åbenlyst ud fra sammenhængen.
Sommetider vil vi bruge matematisk notation til en implicit erklæring af en parameter.  
For eksempel erklærer »\Procedure \Id{bar}$(\seq{a_1,\ldots,a_n})$ $I$«
en procedure, hvis argument er en følge af elementer af ikke nærmere specificeret type.

I procedurekroppen kan den særlige anvisning \Return{}  forekomme, som afslutter udførelsen af procedurekroppen og fortsætter udførelsen ved den første anvisning efter kaldsstedet.
Det samme sker, når udførelsen når enden af procedurekroppen.

De fleste procedurekald kan oversættes
\index{oversættelse!procedure} 
ved blot at erstatte kaldet med procedurekroppen og tage hensyn til parameteroverførelsen.
Denne fremgangsmåde kaldes »linjeindlejring«.
\index{linjeindlejring|textbf}
Overførelsen af argumentet $E$ til den formelle parameter \Declare{$x$}{$T$} realiseres som en følge af kommandoer, som evaluerer udtrykket $E$ og tildeler resultatet til den lokale variabel $x$.
Overførelsen til en formel referenceparameter \Declare{$x$}{$T$} sker ved, at den lokale variabel $x$ får typen $\PointerTo T$ og alle forekomster af $x$ i procedurekroppen erstattes af  $(\Star x)$. 
Ved indgangen til procedurekroppen udføres desuden tildelingen $x := \Address y$, hvor $y$ er kaldsargumentet.
Brugen af linjeindlejring giver oversætteren rige muligheder for alskens optimeringer, så denne fremgangsmåde fører til den mest effektive kode, når proceduren er lille eller kun kaldes fra et enkelt sted.

\newcommand{\Rr}{R_{\it res}} %{R_{\Id{result}}}

\begin{figure}[t]

\begin{tabbing}
  \tabstops
\Funct{fakultet}{$n$}{$\NN$}\quad\+\\
  \If $n=0$ \Then \Return $1$ \Else \Return $n\cdot \Id{fakultet}(n-1)$
\end{tabbing}

  \begin{tikzpicture}
    \matrix [matrix of nodes,
	     column 1/.style = {nodes =  {font = \small\ttfamily}},
	     column 2/.style = {anchor = base west},
	     column 3/.style = {anchor = base west},
     ] (m) {
    34313 & 
    $R_{\it arg} := \Id{RS}[R_r - 1]$ &
    \comment{indlæs $n$ til register $R_{\it arg}$}\\
  34314
  &\texttt{HN\ 34323}, $R_{{\it arg}}$ 
    &\comment{hop til \Then-grenen, hvis $n=0$}\\
  34315
    &$\Id{RS}[R_r] := \texttt{34319}$ & 
  \comment{returadresse for rekursionen}\\
  34316&
  $\Id{RS}[R_r + 1] := R_{{\it arg}} - 1$  &
  \comment{parameteren er $n - 1$}\\
  34317
    &$R_r := R_r + 2$ &\comment{øg stakpegeren}\\
  34318
    &$\texttt{H 34313}$ &\comment{rekursivt kald \Id{fakultet}$(n-1)$}\\
    34319
  &$\Rr := \Id{RS}[R_r - 1]    * \Rr$ &\comment{gem $n \cdot \Id{fakultet}(n-1)$}\\
    34320
    &$R_r := R_r - 2$          &\comment{frigør aktiveringsposten}\\
    34321
&$\texttt{H}$\ \ $\Id{RS}[R_r]$ &\comment{hop retur}\\
    34322
    & \texttt{H 34326}  &\\
   34323
    &$\Rr := 1$ &\comment{gem $1$ i resultatregisteret}\\
    34324
    &$R_r := R_r - 2$          &\comment{frigør aktiveringsposten}\\
    34325
&$\mathtt{H}$\ \ $\Id{RS}[R_r]$ &\comment{hop retur}\\
    34326
&\\
    };
    \draw [decorate, decoration=brace, callout]
    (m-9-1.south west)-- node [callout, anchor= east] 
    {\Else} (m-3-1.north west);
    \draw [decorate, decoration=brace, callout]
    (m-13-1.south west)-- node [callout, anchor= east] 
    {\Then} (m-11-1.north west);
    \node [left of = m-1-1, callout] {\Id{fakultet} $\rightarrow$};
  \end{tikzpicture}

  \caption{\llabel{alg:factorial}
    Rekursiv funktion $\Id{fakultet}$ til beregning af fakultetsfunktionen og den tilsvarende maskinkode.
  Maskinkoden returnerer resultatet $n!$ i register $\Rr$.
  Koden svarer nogenlunde til dette kapitels konventioner for \Id{oversæt}.
  For læselighedens skyld er maskinens repertoire af kommandoer udvidet med et ubetinget indirekte hop til adressen $\Id{RS}[R_r]$ i linje 34322 og 34325.
  Desuden sammenfatter linje~34319 udregningnen højresiden, som egentlig skulle kræve register\-operationer som fx  $R_0:= R_r$; $R_1:= 1$; $R_0:= R_0-R_1$; $R_0:=\Id{RS}[R_0]$; $\Rr:= R_0*\Rr$.
  På den anden side ville en moderne oversætter fjerne linje 34322, som jo alrig nås, og muligvis slå de to identiske programstumper for \Return{} i linje 34320--34321 og 34324--34325 sammen.}%
\end{figure}
%TODO wrongly compiled in original 
\begin{figure}\sidecaption
  \begin{tikzpicture} 
    \matrix [matrix of nodes, minimum width = 1.5cm] (m)
    {
      \ \\
       3\\
      \texttt{34319}\\
      4\\
      \texttt{34319}\\
      5\\
      \texttt{25436}\\
    };
    \foreach \i in {1,...,7} 
      \draw (m-\i-1.south east) -- (m-\i-1.south west);
    \draw (m-1-1.north east) -- (m-7-1.south east);
    \draw (m-1-1.north west) -- (m-7-1.south west);
    \node [minimum width = 1pt, left of = m-1-1]  {$R_r\rightarrow$};
  \end{tikzpicture}
\caption{\llabel{fig:recursionstack} Rekursionsstakken for kaldet $\Id{factorial}(5)$, når rekursionen er nået til kaldet $\Id{factorial}(3)$ 
  Nederst ligger returadressen til den kaldende kode.}
\end{figure}

\emph{Funktioner} minder meget om procedurer, bortset fra at de deres $\Return$-anvisning også skal returnere en værdi. 
\index{rekursion|textbf},
\index{returner@{\Return}}
I figur~\lref{alg:factorial} ses erklæringen af en funktion $\Id{fakultet}$, som på argument $n$ returnerer resultatet $n!$, samt dens oversættelse til registermaskinen.
Linjeindlejringen virker ikke for \emph{rekursive} funktioner og procedurer som fx $\Id{factorial}$, som kalder sig selv enten direkte eller indirekte  -- kode\-erstatningen ville aldrig blive færdig.
For at realisere rekursive underprogrammer i registermaskinkode, har man brug for en
\index{stak}
\index{rekursionsstak|textbf}
\emph{rekursionsstak}.
Udtrykkelige kald og rekursionsstakken bruges også for omfattende underprogrammer, som kaldes fra flere steder i programmet, fordi linjeindlejringen ville føre til for meget kode.
Rekursionsstakken, betegnet med $\Id{RS}$, placeres i en til formålet reserveret del af lageret.
Den indeholder en følge af 
\index{aktivierungspost}
\emph{aktiveringsposter}, en for hvert aktivt underprogramskald.
Et særligt register $R_r$ peger altid på den første ledige plads på rekursionsstakken.
Aktiveringsposten for et underpogram med $k$ parametre og $\ell$ lokale variable har størrelse $1 + k + \ell$.
Første position indeholder \emph{returadressen}, dvs.\ nummeret på den programlinje, ved hvilken programmets udførelse skal fortsætte efter kaldets afslutning;
de næste $k$ positioner er reserverede til parametre, og de resterende $\ell$ positioner til de lokale variable.
Kaldet af et underpogram realiseres på følgende måde som maskinkode:
Først skriver den kaldende procedure \Id{kalder} returadressen og argumenterne på stakken, øger værdien af $R_r$ med $1+k$ og hopper til den første programlinje underprogrammet \Id{kaldt}.
Det kaldte underprogram begynder med at reservere plads til sine $\ell$ lokale variabler ved at øge $R_r$ med $\ell$.
Derefter udføres kroppen af $\Id{kaldt}$.
I løbet af dennne udførelse findes den $i$te formelle parameter (for $0 \le i < k$) i celle $\Id{RS}[R_r - \ell - k + i]$ og den $j$te lokale variable (for $0 \le j < \ell$) i celle $\Id{RS}[R_r - \ell  + j]$.
Anvisningen \Return  i \Id{kaldt} oversættes til en følge af maskinkommandoer, som mindsker $R_r$ med $1 + k + \ell$ (\Id{kaldt} kender naturligvis tallene $k$ og $\ell$) og hopper så retur til den adresse, som er gemt i  $\Id{RS}[R_r]$.
Udførelsen fortsætter nu i proceduren \Id{kalder}.
Læg mærke til, at rekursion med denne metode ikke længere er et problem, fordi hvert kald (eller »inkarnation«) af underprogrammet har sit eget område på rekursionsstakken for sine parametre og lokale variable.
Figur~\lref{fig:recursionstack} viser rekursionsstakkens indhold
\index{rekursionsstak}
for et kald af $\Id{factorial}(5)$ i det øjeblik, hvor rekursionen er nået til kaldet af $\Id{factorial}(3)$.
Værdien $\mathtt{25436}$ nederst i stakken står for nummeret på den programlinje, som følger efter kaldet $\Id{factorial}(5)$. 
\index{oversættelse!procedurekald}
\index{oversættelse!funktionskald}
\index{oversættelse!rekursion}



\begin{exerc}[Eratostenes’ si]
  \index{Eratostenes’ si}
  \llabel{ex:sieve:Eratosthenes}
  Oversæt nedenstående pseudokode, som finder og udskriver alle primtal
  \index{primtal}
  op til tallet $n$, til maskinkode.
  (Vi er ligeglade med at oversætte udskriftskommandoen på sidste linje.)
  Vis først, at algoritmen er korrekt.

  \begin{quote}
  \begin{tabbing}\tabstops
  \DeclareInit{$a$}{$\Array[2..n]$ \Of $\set{0,1}$}{$\seq{1,\ldots, 1}$}\\
  \comment{referenceparameter; til sidst gælder $a[i]=1$ \ $\Leftrightarrow$ \ $i$ er primsk}\\
  \ForFromTo{i}{2}{\floor{\sqrt{n}}}\+\\
    \If $a[i]$ \Then \ForFromToStep{j}{2i}{n}{i} $a[j]:= 0$\\
  \comment{Hvis $a[i]=1$ så er $i$ primsk, men multipler af $i$ er ej}\-\\
  \ForFromTo{i}{2}{n} \If $a[i]$ 
      \Then {\rm udskriv}$(i, \text{» er primsk«})$
  \end{tabbing}
  \end{quote}
\end{exerc}

\subsection{Objektorientering}

Vi vil holde os til en enkel form for objektorientering med formålet at adskille datastrukturers specifikation (også kaldt \emph{grænseflade}) fra implementationen.
Vi vil forklare den relevante notation med et eksempel.
Definitionen

\begin{quote}
\begin{tabbing}\tabstops
$\Kw{Klasse }\Id{Komplekst}(x,y\colon\Id{tal}) \Kw{ af } \Id{tal}$\\
\>$\Id{re} = x\colon \Id{tal}$\\
\>$\Id{im} = y\colon \Id{tal}$\\
\>$\Kw{Funktion }\Id{abs}\colon \RR \Kw{ returner } \sqrt{\Id{re}^2+ \Id{im}^2}$\\
\>$\Kw{Funktion }\Id{addér}(c'\colon\Id{ Komplekst})\colon
\Id{Komplekst}$\\
\>\>$\Kw{ returner }\Id{ Komplekst}(\Id{re} + c'.\Id{re}, \Id{im} + c'.\Id{im})$
\end{tabbing}
\end{quote}

\noindent
stiller en (delvis) implementation af en taltype for komplekse tal til rådighed, som kan bruge vilkårlige taltyper som $\ZZ$, $\QQ$ eller $\RR$ til real- og imaginærdelen.
(»$\Id{Komplekst}$« er også et eksempel på, at vi ofte vil skrive vores klassenavne med stort begyndelsesbogstav.)
Real- og imaginærdel holdes i \emph{instansvariablerne} $\Id{re}$ og $\Id{im}$.
Deklarationen »$c\colon\Id{Komplekst}(2,3)\Kw{ af }\RR$« skaber et komplekst tal $c$, som initialiseres til $2+3i$, hvor $i$ angiver den imaginære enhed.
Udtrykket $c.\Id{im}$ tilgår imaginærdelen af $c$, og metoden $c.\Id{abs}$ bestemmer absolutværdien af $c$, som er et reelt tal.

Angivelsen af en type efter $\Kw{af}$ i variabeldeklarationen gør det muligt at parameterisere klasser på lignende måde som \emph{template}-mekanismen i C++ og de generiske typer i Java.
Læg mærke til, at vores tidligere deklarationer »$\Id{Mængde }\Kw{af }\Id{Element}$« og »$\Id{Følge }\Kw{af }\Id{Element}$« med denne notation definerer helt almindelge parameteriserede klasser.
Objekter i en klasse intialiseres ved at tildele instansvariablerne de værdier, som er angivet i variabeldeklarationen.

\section{Konstruktion af korrekte algoritmer og programmer}
\llabel{s:correct}

\section{Eksempel: Binærsøgning}
\llabel{s:binary search}

\index{binxrsxning@binærsøgning|sieheunter{søgning}}%
\index{algoritmekonstruktion!del-og-hersk}%
\index{algoritmekonstruktion!forberedelse}%
\index{algoritmekonstruktion!sortering!anvendelse}%
\index{sortering!anvendelse}%
\index{sxning@søgning!binærsøgning|textbf}%
\index{sorteret følge}%

Binærsøgning er en nyttig teknik til at søge efter en element i en sorteret række.
Vi kommer til at bruge binærsøgning meget i de senere kapitler.

Det enkleste scenario ser ud på følgende måde.
Givet er en sorteret række $a[1..n]$ af parvist forskellige indgange, dvs.\ at $a[1] < a[2] < \cdots < a[n]$, samt et element $x$.
Vi skal afgøre, om $x$ forekommer i rækken og finde det indeks $k \in \{1,\ldots, n+1\}$, som opfylder $a[k-1] < x \le a[k]$.
Her skal man opfatte $a[0]$ og $a[n+1]$ som fiktive indgange med værdierne $-\infty$ hhv.\ $+\infty$. 
Vi bruger disse fiktive indgange i invarianter og beviser for at forenkle notationen, men de forekommer ikke i programmet.

Binærsøgning er et eksempel på det algoritmisk princip, som hedder »del og hersk«.
Vi vælger et index $m\in\{1,\ldots,n\}$ og sammenligner $x$ med $a[m]$.
Hvis $x = a[m]$, så er vi færdige og returnerer $k=m$. 
Hvis derimod $x < a[m]$, så indskrænker vi søgningen til delrækken før $a[m]$.
Hvis $x > a[m]$, så indskrænker vi søgning til delrækken efter $a[m]$. 
Vi skal nu præcisere, hvad vi mener med »at indskrænke søgningen til en delrække«.
Vi vedligeholder to indeks $v$ og $h$ (for »venstre« og »højre«) og invarianten
\index{invariant!løkkeinvariant}
{\renewcommand{\theequation}{\textrm{I}}
\begin{equation}
	\qquad 0 \le v < h \le n+1  \quad \text{og}\quad a[v] < x < a[h] \tag*{$\textrm{(I)}$}
	\,.
\end{equation}}
Invarianten gælder klart i begyndelsen, hvor $v = 0$ og $h = n + 1$.
Hvis $v$ og  $h$ er på hinanden følgende indeks, dvs.\ $v + 1 = h$, så kan vi konkludere, at $x$ ikke forekommer i rækken.
Figur~\lref{alg:binary search} viser det fuldstændige program.

\begin{buchalgorithmpos}{b}{alg:binary search}{Binærsøgning efter $x$ i en sorteret række $a[1..n]$.
  Algoritmen returnerer et indeks $k$, som opfylder $a[k-1] < x \le a[k]$.  
  }%
  $\Function \Id{binærsøgning}( x\colon\Element, a\colon\Array[1..n] \Of \Element)\colon 1..n+1$\+\\
$(v,h) :=(0,n+1)$ \\  
\While $v+1<h$ \Do \+ \\
    \Assert $I \wedge v+1< h$\RRem{løkkeinvarianten og iterationsbetingelsen gælder}\\
    $m := \floor{(v + h)/2}$ \RRem{$v < m < h$}\\
    $s := \Id{sammenlign}(x,a[m])$ \RRem{$s=-1$ hvis $x < a[m]$, $s=0$ hvis $x = a[m]$,
$s=+1$ hvis $x > a[m]$}\\
    \If $s = 0$  \Then \Return $m$\RRem{$a[m] = x$} \\ 
    \If $s < 0$ \Then $h := m$ \RRem{$a[v] < x < a[m] = a[h]$} \\
    \If $s > 0$ \Then $v := m$ \RRem{$a[v] = a[m]  < x < a[h]$}\\
    \Assert $I$\-\\
    \Assert $I \wedge v+1=h$\-\\
    \Return $h$ \RRem{$a[h-1] < x < a[h]$} 
\end{buchalgorithmpos}
%
% TODO should this not return l+1 in the failure branch?

\index{sammenligning!trevejs-}%
Lad os vise programmets korrekthed.
Vi viser først, at løkkeinvarianten $I$ er gyldig, hver gang iterationsbetingelsen »$l+1<r$« bliver prøvet.
Dette gøres ved induktion i antallet af iterationer.
Vi har allerede etableret, at invarianten gælder i begyndelsen, altså når $v=0$ og $h=n+1$.
Dette er basis for induktionen.
For induktionsskridtet skal vi vise, at et gennemløb af løkkens krop opretholder invarianten.
Lad os altså antag, at invarianten gælder, og at iterationsbetingelsen $v+1<h$ er opfyldt.
Idet $v$ og $h$ er heltal, har vi $v+2\leq h$.
Derfor har vi både
\[
  m=\lceil \frac{v + h}{2}\rceil
  = \lceil \frac{v + 2 + h- 2}{2}\rceil =
  \leq \lceil \frac{2h - 2}{2}\rceil = h - 1 < h
\]
og på samme måde $m > v$, dvs. $v<m< h$.
Især er $m$ et gyldigt indeks og det giver mening at evaluere $a[m]$.
Hvis $x= a[m]$, standser vi.
Ellers sætter vi enten $h=m$ eller $v=m$, og i begge tilfælde gælder $h<v$.
Hvis $x<a[m]$ så sætter vi $h=m$, således at $x<a[h]$, mens $a[l]<x$ gælder fra invarianten.
Tilfældet $x>a[m]$ behandles på samme måde.
Invarianten er nu genetableret og er derfor gyldig hver gang, løkkens iterationsbetingelse bliver evalueret.

For at færdiggøre beviset betragter vi endeligt situationen $h+1\geq v$, hvor løkkens krop ikke bliver besøgt.
Fra invarianten fås $h< v$, og idet $h$ og $v$ er heltallige, har vi $h+1  =v$.
Ifølge invarianten har vi nu $a[h-1] < x <a[h]$.
Hermed er vi færdige: enten retunerer programmet et indeks $m$ med $a[m]=x$, eller et indeks $h$ med $a[h-1]< x<a[h]$.

Vi mangler at argumentere for, at løkken terminerer.
\index{terminering}
Vi lægger først mærke til, at  alle løkkegennemløb bortset fra det sidste enten forøger $v$ eller formindsker $h$, dvs. at $h-v$ er skarpt aftagende positive heltal.
Derfor terminerer løkken. 
Vi vil dog vise noget stærkere, nemlig at løkken terminerer allerede efter et logaritmisk antal gennemløb.
Hertil betragter vi værdien $h-v-1$.
Det er netop antallet af indeks $i$, som opfylder $v < i < h$ og derfor et naturligt mål for størrelsen af det aktuelle delproblem.
Vi vil nu vise, at hvert løkkegennemløb på nær det sidste gør størrelsen af delproblemet halvt så stort eller endnu mindre.
I hvert af disse gennnemløb minsker $h - v - 1$ til en værdi, der er højst
\begin{align*}
  &\max\left\{h - \left\lfloor\frac{ h + v}{2}\right\rfloor - 1, \left\lfloor\frac{h + v}{2}\right\rfloor - v - 1\right\}\\
  &\qquad \le \max\{{h
  - ((h + v)/2 - \tfrac{1}{2}) - 1, (h + v)/2 -  v- 1} \\
  &\qquad = \max\set{(h - v - 1)/2, (h - v)/2 - 1} = (r - v - 1)/2 \,,
\end{align*}
hvilket udgør mindst en halvering.
\index{rekursionsligning}
Vi begynder med $h - v - 1 = n + 1 - 0 - 1 = n$.
Efter $k\ge0$ gennnemløb gælder derfor uligheden $r - v - 1 \le n/2^k$.
Lad gennemløb nummer $k+1$ være det sidste gennemløb, i hvilket $x$ bliver sammenlignet med indgangen $a[m]$.
(Hvis søgningen lykkes, viser det sig, at $x=a[m]$, og søgningen afsluttes. 
Hvis søgningen mislykkes, fører sammenligningen i runde $k+2$ til, at $h = v + 1$, og søgningen afsluttes uden yderligere sammenligninger.)
Da må uligheden $h - v - 1 \ge 1$ have været opfyldt i runde $k$.
Det følger, at $1 \le n/2^k$, og dermed $2^k \le n$, dvs. $k\le \log n$. 
Idet $k$ er et heltal, kan uligheden skærpes til $k\le \lfloor\log n\rfloor$.

\begin{thm} 
  Binærsøgning finder et element i en sorteret række af længde $n$ i højst $1 + \lfloor\log n\rfloor$ sammenlignigner mellem indgange. 
  Udførelsestiden er $O(\log n)$.
\end{thm}

\begin{exerc}
  Vis, at denne grænse er skarp, dvs. at der for hvert $n$ findes instanser af størrelse $n$, som leder til $1 + \floor{\log n}$ sammenligninger i algoritmen.
\end{exerc}

\begin{exerc} 
  Formuler binærsøgning i termer af tovejssammenligninger,
  \index{sammenligning!tovejs-}
  dvs. at sammenligningen kun afgør, om $x \le a[m]$ eller $x > a[m]$.
\end{exerc}

Vi skal nu betragte to vigtige udvidelser af binærsøgning.
For det første behøver værdierne $a[i]$ ikke at være gemt i en række. 
Vi skal bare være i stand til at beregne værdien $a[i]$ for givet $i$. 
Hvis vi fx får givet en strengt monotont voksende funktion $f$ og argumenter $i$ og $j$ med
 $f(i) < x \le f(j)$, så kan vi benytte binærsøgning for at bestemme det index $k\in\{i+1,\ldots, j\}$, som opfylder $f(k-1) < x \le f(k)$.
I denne sammenhæng kaldes binærsøgning ofte »bisektion«.
\index{bisektion|textbf}
%TODO bisektion? bisektionsmetode? noget andet?

For det andet kan vi også benytte binærsøgning i situationer, hvor rækken er uendelig lang.
Antag, at vi har et sorteret række $a[1 .. \infty]$ og vil finde det index $k\in\{1,2,3,\ldots\}$, som opfylder $a[k] < x \le a[k+1]$.
Hvis $x$ er større end alle indgange i rækken, må proceduren gerne tage uendelig lang tid.
Fremgangsmåden er at sammenligne $x$ med $a[2^0]$, $a[2^1]$, $a[2^2]$, $a[2^3]$, $\ldots$, indtil vi finder det første $i$, som opfylder $x \le a[2^i]$.
Denne fremgangsmåde kaldes \emph{eksponentiel søgning}
\index{eksponentiel søgning}
\index{søgning!eksponentiel søgning}
Hvis $i=0$, så returneres $k=1$, ellers afsluttes søgnningen med sædvandlig binærsøgning i den endelige række $a[(2^{i-1}+1) .. 2^i]$.


\begin{thm} %\MDcommentout{Position der $\le$ und $<$ verändert!} 
  Kombinationen af eksponentiel og binærsøgning finder $x$ i en ubegrænset sorteret række med højst $(2 \log k) + 3$ sammenligninger, hvor $k$ er givet ved betingelsen $a[k-1] < x \le a[k]$.
\end{thm}

\begin{proof}
  Vi behøver $i+1$ sammenligninger, for at finde det mindste $i$ med $x \le a[2^i]$, og (givet at $i>0$) derefter højst $\log(2^i - 2^{i-1}) + 1=i$ sammenligninger for binærsøgningen. 
  Sammenlagter er dette $2i + 1$ sammenligninger. 
  Idet $k \ge 2^{i-1}$, får vi $i \le 1 + \log k$, hvoraf påstanden følger.
\end{proof}

%\MDcommentout{Position der $\le$ und $<$ verändert!} 
Binærsøgning er en certificerende algoritme.
\index{algoritmekonstruktion!certifikat}
Den returnerer et indeks $k$ som opfylder $a[k-1] < x \le a[k]$. 
Hvis $x = a[k]$, så bekræfter indeks $k$, at $x$ forekommer i rækken.
Hvis $a[k-1] < x < a[k]$, og rækken er sorteret, bekræfter indeks $k$, at $x$ ikke forekommer i rækken.
Læg mærke til, at hvis rækken ikke opfylder sorteringsforudsætningen, så har vi ingen information i dette tilfælde. 
Det er umuligt at kontrollere sorteringsforudsætningen i logaritmisk tid.

\section{Grundlæggende algoritmeanalyse}
\llabel{s:analysis}


\index{algoritmeanalyse|textbf}
\index{programanalyse|siehe{algoritmeanalyse}}
\index{analyse|siehe{algoritmeanalyse}}%

Vi kan sammenfatte de hidtil betragtede principper for algoritmeanalyse på følgende måde.
Vi abstraherer bort fra  komplikationerne i en virkelig datamat ved at betragte registermaskinen.
I denne model måles kørselsstid ved at tælle antallet af udførte kommandoer.
Vi forenkler analysen yderligere ved at inddele problemer efter instans\-størrelse og fokusere på værstefald.
Den asympotiske notation gør det muligt at ignorere konstante faktorer og lavere\-ordens termer.
Dette grovkornige perspektiv tillader os også at betragte øvre grænser
\index{zzvre@øvre grænse|siehe{kørselstid, værstefald}}
for kørselstiden i stedet for eksakte vurderinger, så længe det asymptotiske resultater forbliver uændret.
Samlet leder disse forenklinger til, at vi kan analysere kørselstiden af pseudokode direkte, dvs.\ uden at behøve at oversætte programmet til maskinsprog.

Vi skal nu præsentere et sæt enkle regler for analyse af pseudokode.
\index{algoritmeanalyse}
\index{kørselstid} 
Lad $T_n(I)$ være den maksimale kørselstid for en programstump $I$ på instanser af størrelse~$n$.
\index{instansstørrelse}
\index{stzrrelse@størrelse!instans-} 
For kørselstiden af større programmer gælder da følgende:
\begin{itemize}
\item For på hinanden følgende kommandoer: $T_n(I; J)\le T_n(I)+T_n(J)$.
\item For betingede kommandoer: $T_n(\text{\If $C$ \Then $I$ \Else $J$})=
  T_n(C)+\max\set{T_n(I),T_n(J)}$.
\item For løkker: $T_n(\text{\Repeat $I$ \Until $C$})=\sum_{i=1}^{k(n)} T_n'(I,C,i)$, hvor $k(n)$ angiver det maksimale antal løkkegennemløb på instanser af størrelse $n$, og 
$T_n'(I,C,i)$ er kørselstiden for det $i$te gennemløb af løkken, inklusive betingelsen $C$.
\end{itemize}
Underprogrammer betragtes i afsnit~\lref{s:recurrences}.
Bland reglerne foroven er det kun løkkereglen, der kan give anledning til vanskeligheder, idet man skal vurdere summer.

\subsection{Summer}

\index{algoritmeanalyse!sum|textbf}%
Vi viser nu nogle grundlæggende teknikker til evaluering af summer.
Summer dukker op i forbindelse med analyse af løkker samt ved analysen af gennemsnitlige og forventede kørselstider.

For eksempel består algoritmen \Id{indsættelsessortering},
\index{sortering!indsættelses-}
\index{indsættelsessortering|siehe{sortering, indsættelses-}}
som forklares i afsnit~\ref{ch:sort:s:simple}, af to indlejrede løkker.
\index{løkke}
Den ydre løkke tæller variablen $i$ fra $2$ op til  $n$.
Den indre løkke gennemløbes højst $(i-1)$ gange. 
Derfor er det samlede antal gennemløb gennem den indre løkke højst
\[
\sum_{i=2}^n (i-1)
 =\sum_{i=1}^{n-1} i 
 =\frac{n(n-1)}{2} = O(n^2)\,,\]
hvor den anden ligning følger af (\ref{app:notation:eq:sumi}).
Idet et enkelt gennemløb af den indre løkke kræver tid $O(1)$, bliver kørselstid i værste fald $\Theta(n^2)$.
Alle indlejrede løkker, hvis antal iterationer er let at bestemme, kan analyseres på en tilsvarende måde:
man arbejder »indfra og ud«, og prøver at finde et estimat for kørselstiden af den aktuelle »indre løkke« som et lukket udtryk.
Med simple omformninger som
$\sum_i ca_i=c\sum_ia_i$,
$\sum_i(a_i + b_i)=\sum_i a_i + \sum_i b_i$ eller
$\sum_{i=2}^n a_i=-a_1+\sum_{i=1}^n a_i$
kan man ofte få summerne på en pæn form, som man kan finde i et katalog over summeformler.
Et lille udvalg af den slags formler findes i appendix~\ref{app:notation:}.
Idet vi normal kun interesserer os for asympototisk opførsel, kan vi se bort fra præcise formler for disse summer og i stedet nøjes med estimater.
For eksempel kan vi i stedet for at udregne summen foroven eksakt, begrænse den opad- og nedadtil meget nemmere (for $n\ge 2$):
\begin{align*}
\sum_{i=2}^n (i-1) &\leq \sum_{i=1}^n n = n^2 = O(n^2)\,,\\
\sum_{i=2}^n (i-1) &\geq \sum_{i=\ceil{n/2}+1}^n n/2 = \left\lfloor\frac{n}{2}\right\rfloor \cdot\frac{n}{2}=
\Omega(n^2) \,. \end{align*}

%----------------------------------------------------------------------
\subsection{Rekursionsligninger}\llabel{s:recurrences}

\index{rekursionsligning|textbf}
\index{algoritmeanalyse!rekursion}
\index{rekursion|sieheauchunter{algoritmekonstruktion; algoritmeanalyse}}%
I vores regler for algoritmeanalyse har vi hidtil ikke bekymret os om kald til underprogrammer.
Ikke-rekursive underprogrammer er lette at håndtere, fordi vi kan analysere underprogrammet for sig og sætte den resulterende grænse ind i udtryk for kørselstiden for det kaldende program.
Men når vi bruger denne fremgangsmåde på rekursive underprogrammer, fører den ikke til en lukket formel, men til en rekursionsligning.

I den rekursive udgave af skolemetoden for multiplikation 
optræder ved analysen af antallet af elementaroperationer for eskempel ligningerne
 $T(1)=1$ og $T(n)=4T(\ceil{n/2})+4n$.
De tilsvarnde ligninger for Karatsubas algoritme er
$T(n)=3n^2$ for $n \le 3$ og 
$T(n)=3T(\ceil{n/2} + 1)+8n$ for $n > 3$.
Generelt definerer en \emph{rekursionsligning} som en funktion i termer af funktionens værdier på mindre argumenter.
Definition fuldstændiggøres ved udtrykkelige definitioner af små parameterværdier (»basis«). 
Løsningen af rekursionsligninger, dvs.\ at finde en ikke-rekursiv, lukket formel for en rekursivt definieret funktion, er en interessant og omfattende del af matematikken.
I vores kontekst skal vi hovedsageligt koncentere os om den slags rekursionsligninger, som optræder i forbindelse med analysen af del-og-hersk-algoritmer.
\index{algoritmekonstruktion!del-og-hersk}
Vi begynder med et enkelt specialtilfælde for at gøre hovedidéen forståelig.
Givet en instans af størrelse $n=b^k$ for et naturligt tal $k$.
For $k \ge 1$ benytter vi lineært meget arbejde for at opdele instansen i $d$ mindre instanser af størrelse $n/b$ og for at kombinere resultaterne af de rekursive kald på de enkelte delinstanster.
For $k=0$ sker ingen rekursive, og vi bruger $a$ meget arbejde for at løse opgaven for instansen direkte.
\index{del-og-hersk-ligninger|sieheunter{algoritmeanalyse}}\index{algoritmeanalyse!del-og-hersk-ligninger|textbf} 

\begin{thm}[Klassifikation af del-og-hersk-ligninger, enkel form]\llabel{thm:master}
For positive konstanter $a$, $b$, $c$ og $d$, og $n=b^k$ for et naturligt tal $k$, lad $r$ være en rekursionsligning givet ved
\[ r(n)=
\begin{cases}
a & \text{ for }n=1\,,\\
d \cdot r(n/b) + cn & \text{ for } n > 1 \,.
\end{cases} \]
Da gælder
\[ r(n)=
\begin{cases}
\Theta(n) & \text{ for }d<b\,,\\
\Theta(n\log n) & \text{ for }d=b\,,\\
\Theta(n^{\log_bd}) & \text{ for }d>b \,.
\end{cases} \]
\end{thm}

\begin{figure}
  \begin{tikzpicture}[scale = .36]
    \node at (-16,-1) [anchor = west]{$d=2, b=4$};
    \foreach \y in {0,...,3} {
      \pgfmathtruncatemacro\n{2^\y};
      \pgfmathtruncatemacro\m{\n-1};
      \foreach \i in {0,...,\m} {
      \pgfmathsetmacro\w{32/4^\y};
      \pgfmathsetmacro\hw{\w/2};
      \pgfmathsetmacro\x{-(\w*\m/2) +\i*\w};
      \draw [fill = white, rounded corners = 1pt]  (\x,-\y) ++(-\hw,-.2) rectangle ++(\w,.4);
      \begin{scope}[on background layer]
      \ifthenelse{\y=3}{}{
	%\draw [blue] (\x,-\y) -- (-\w*\n/4, -\y-1);
	\draw [callout] (\x,-\y) -- (-\w*\n/4 + \i*\w/2 + \hw/4, -\y-1);
	\draw [callout] (\x,-\y) -- (-\w*\n/4 + \i*\w/2 + \hw/4 + \hw/2 , -\y-1);
    }
      \end{scope}
    }
  }
    \begin{scope}[yshift = -6cm]
      \node at (-16,1)[anchor = west] {$d= b=2$};
      \foreach \y in {0,...,4} {
	\pgfmathtruncatemacro\m{2^\y-1};
	\foreach \i in {0,...,\m} {
	\pgfmathtruncatemacro\w{32/2^\y};
	\pgfmathtruncatemacro\hw{\w/2};
	\pgfmathtruncatemacro\x{-(\w*\m/2) +\i*\w};
	\draw [fill = white, rounded corners = 1pt]  (\x,-\y) ++(-\hw,-.2) rectangle ++(\w,.4);
	\begin{scope}[on background layer]
	  \ifthenelse{\y=4}{}{
	\draw [callout] (\x,-\y) -- ++(-\hw/2, -1);
	\draw [callout] (\x,-\y) -- ++( \hw/2, -1);
	  }
	\end{scope}
      }
    }
    \end{scope}
    \begin{scope}[yshift = -12cm]
      \node at (-16,-1)[anchor = west] {$d=3, b=2$};
      \node at (0,0) {+};
      \foreach \y in {0,...,3} {
        \pgfmathtruncatemacro\m{3^\y-1};
        \pgfmathtruncatemacro\n{3^\y};
        \foreach \i in {0,...,\m} {
          \pgfmathsetmacro\w{(32/27)*2^(3-\y)};
	  \pgfmathsetmacro\hw{\w/2};
        \pgfmathsetmacro\x{-(\w*\m/2)  +\i*\w};
        \draw [fill = white, rounded corners = 1pt]  (\x,-\y) ++(-\hw,-.2) rectangle ++(\w,.4);
        \begin{scope}[on background layer]
          \ifthenelse{\y=3}{}{
	    \draw [callout] (\x,-\y) -- (-3*\w*\n/4 + 3*\i*\w/2 + \hw/2,-\y -1);
	    \draw [callout] (\x,-\y) -- (-3*\w*\n/4 + 3*\i*\w/2 +\w/2+\hw/2,-\y -1);
	    \draw [callout] (\x,-\y) -- (-3*\w*\n/4 + 3*\i*\w/2 + \w+\hw/2 ,-\y -1);
          }
        \end{scope}
      }
    }
    \end{scope}
  \end{tikzpicture}
\caption{\llabel{fig:master}
Illustration af de tre fald i klassifikationen af del-og-hersk-ligninger.
Instanser fremstilles som rektangler, hvis længde repræsenterer instansens størrelse.
Delinstanserne, som opstår ud fra en instans, er vist et niveau længere nede.
  Figurens øvre del viser tilfældet $d = 2$ og $b = 4$, i hvilket en instans leder til to delinstanser med en fjerdedel af størrelsen -- den samlede størrelse af delinstanserne udgør altså halvdelen af den oprindelige instans.
  Figurens midterste del viser tilfældet 
 $d = b = 2$, og den nederste del  $d =3$ og $b = 2$.}
\end{figure}
%
Figur \lref{fig:master} illustrerer den centrale indsigt, som ligger til grund for sætning~\lref{thm:master}.
Vi betragter tidsforbruget på hvert niveau af rekursionen.
Vi begynder med en instans af størrelse $n$.
På nivau $i$ af rekursionen har vi $d^i$ delinstanser af størrelse $n/b^i$ hver.
Det samlede størrelse af alle instanser på niveau $i$ er derfor
\[ d^i \frac{n}{b^i} = n \left( \frac{d}{b} \right)^i \,. \]
Tidsforbruget for en instans (bortset fra de rekursive kald, den selv måtte lede til) er $c$ gange instansstørrelse.
Derfor er det samlede tidsforbrug på et rekursionsniveau proportionalt med den samlede størrelse af alle instanser på dette niveau.
Afhængigt af, om $d/b$ er mindre end $1$, lig med $1$, eller større end $1$, opstår der forskellige opførsler.

For $d<b$ er tidsforbruget \emph{geometrisk aftagende}
\index{sum!geometrisk} 
med rekursionsniveauet, og det \emph{første} niveau i rekursionen er allerede ansvarlig for en konstant til af det samlede tidsforbrug.

For $d=b$ bruges på \emph{hvert} rekursionsniveau \emph{præcis den samme} tid.
Idet der findes logaritmisk mange niveauer, er det samlede tidsforbrug $\Theta(n\log n)$.

For $d>b$ er tidsforbruget \emph{geometrisk voksende} i rekursionsniveauet, og en konstant del af den samlede tid bruges på rekursionens \emph{sidste} niveau. 
Vi skal nu gennemgå disse overvejelser i detaljer.

\begin{proof} 
  Vi begynder med en enkelt instans af størrelse $n = b^k$. 
  Lad os kalde den for niveau $0$ i rekursionen.%
  \footnote{I beviset holder vi os til terminologien af rekursive programmer med henblik på at støtte intuitionen for, hvad der foregår.
  De matematiske overvejelser kan dog bruges for alle ligninger af sætning~\lref{thm:master}, uanset om de stammer fra et rekursivt program eller ej.}
  På niveau~1 findes $d$ instanser af størrelse $n/b = b^{k - 1}$ hver.
  På niveau~2 findes $d^2$ instanser af størrelse $n/b^2 = b^{k - 2}$ hver.
  Generelt findes der på det $i$te niveau $d^i$ mange instanser af størrelse $n/b^i = b^{k - i}$ hver.
  I bunden af rekursionen, på niveau $k$, findes $d^k$ instanser af størrelse $n/b^k = b^{k - k} = 1$ hver.
  Hver sådan instans forårsager omkostning $a$, så den samlede omkostning på rekursionens sidste niveau er $ad^k$.

  Hernæst beregner vi den samlede omkostning for del-og-hersk-skridtene på nivau $i$ for $i\in\{0,\ldots, k - 1\}$. 
  På niveau $i$ forekommer $d^i$ rekursive kald, hver til én instans af størrelse $b^{k-i}$. 
  Hvert sådant kald forårsager omkostning $ c \cdot b^{k - i}$;
  den samlede omkostning på niveau $i$ er derfor $d^i\cdot c\cdot b^{k-i}$. 
  Ved at summere bidraget fra alle niveauer fra $0$ bis $k-1$ fås
  \[ \sum_{i=0}^{k-1}d^i\cdot c\cdot b^{k-i}=
  c\cdot
  b^k \cdot \sum_{i=0}^{k-1}\left(\frac{d}{b}\right)^i=cn \cdot \sum_{i=0}^{k-1}\left(\frac{d}{b}\right)^i\,.
  \]
  Vi deler op i tilfælde afhængigt af størrelseforholdet mellem $d$ og $b$:
  \begin{description}
    \item[$d=b$:]
      Vi får omkostning $ad^k=ab^k=an=\Theta(n)$ for rekursionens sidste niveau $k$ og
      samlet omkostning $cnk=cn\log_{b}n=\Theta(n\log n)$ for del-og-hersk-skridtene.

\item[$d<b$:] Vi får omkostning $ad^k<ab^k=an=O(n)$ for rekursionens sidste niveau.
  Til del-og-hersk-skridtene benytter vi ligning (\ref{app:notation:eq:geometric}) for summen af en geometrisk række,
  \index{række|siehe{sum}}\index{geometrisk række|sieheunter{sum}}  
  nemlig $\sum_{i=0}^{k-1} q^i = (1 - q^k)/(1 - q)$ for
  $q > 0$ og $q \not= 1$, og opnår grænserne
  \begin{align*}
    cn \cdot \sum_{i=0}^{k-1}\left(\frac{d}{b}\right)^i & = cn \cdot
    \frac{1-(d/b)^k}{1-d/b} < cn \cdot \frac{1}{1-d/b} =O(n)\\
    \intertext{og}      %\mbox{\MDcommentout{Schon der erste Summand liefert $cn(d/b)^0=cn=\Om(n)$}}
    cn \cdot \sum_{i=0}^{k-1}\left(\frac{d}{b}\right)^i &= cn \cdot
    \frac{1-(d/b)^k}{1-d/b}>cn =\Omega(n)\,.
  \end{align*}

\item[$d>b$:] Der gælder
  \[ d^k
  =(b^{\log_b d})^k
  =b^{k\log_b d}
  =(b^k)^{\log_b d}
  =n^{\log_b d}\,. \]
  Derfor har det sidste niveau af rekursionen omkostning $an^{\log_b d}=\Theta(n^{\log_b d})$.
      For del-og-hersk-skridtene benytter vi igen formlen (\ref{app:notation:eq:geometric}) for den geometriske række og får 
  \begin{equation}
    cb^k\frac{(d/b)^k-1}{d/b-1} =
    c\frac{d^k-b^k}{d/b-1} =
    cd^k\frac{1-(b/d)^k}{d/b-1} =
    \Theta(d^k) =
    \Theta(n^{\log_b d})\,. 
  \end{equation}
  \end{description}
\end{proof}

Vi kommer til at bruge del-og-hersk-klassifikationen i sætning~\lref{htm:master} mange gange i denne bog.\footnote{Ovs. anm.: 
Klassifikationen lader til at fylde så meget i mange grundlæggende fremstillinger af algoritmeanalysens matematiske værktøjer, at den i mange kredse er kendt under det noget højttravende navn »mestersætningen«.} 
Desværre dækkes fx rekursionsligningen $T(n) \le 3n^2$ for $n \le 3$ og  $T(n) \le 3T(\ceil{n/2} + 1) + 8n$ for $n>3$, som optræder i analysen af Karatsubas algoritme, ikke af sætningen i sin enkle form, fordi den ikke tager hensyn til hverken uligheder eller afrundinger i de indgående udtryk.
Det kan også forekomme, at den additive term i rekursionsligningen ikke er lineær.
Vi skal nu vise, hvordan man kan udvide sætningen til at gælde for mere generelle utryk i form af en 
\emph{rekursionsrelation}.
\index{rekursionsrelation}
%
\begin{equation}
  \llabel{eq:1000}
  r(n)\leq
  \begin{cases}
    a\,, & \text{hvis }n\leq n_0\,;\\
    cn^s + d \cdot r(\ceil{n/b} + e_n)\,, & \text{hvis } n > n_0\,.
  \end{cases}
\end{equation} 
Her er $a>0$, $b>1$, $c>0$, $d>0$ og $s\ge 0$ konstante reelle tal, og  værdierne $e_n$ for $n>n_0$ er reelle tal med $-\ceil{n/b} < e_n \le e$ for noget heltal $e\ge0$.
%
\addtocounter{theorem}{-1}
\begin{thm}[Klassifikation af del-og-hersk-relationer]\llabel{master:full}
  Lad $r$ være en rekursionsrelation som opfylder (\lref{eq:1000}).
  Da gælder
\[ r(n)=
\begin{cases}
O(n^s) & \text{for $d < b^s$, dvs. $\log_b d < s$}\,;\\
O(n^s\log n) & \text{for $d = b^s$, dvs. $\log_b d = s$}\,;\\
  O(n^{\log_bd}) & \text{for $d > b^s$, dvs. $\log_b d > s$}\,.
\end{cases} \]
\end{thm}

\begin{proof}
  \emph{Udeladt.}
%  TODO
%  \hspace*{-0.5em}{}\footnote{Dieser Beweis kann beim ersten Lesen überblättert werden.}
%Über die Parameter dürfen wir
%ohne Beschränkung der Allgemeinheit die folgenden technische Annahmen machen:
%\begin{description}[(iii)]
%	\item[(i)] $\ceil{n/b} + e < n$ für $n > n_0$,
%  \item[(ii)] $n_0 \ge 2(e+1)/(1-1/b)$,
%  \item[(iii)]  $a\le c(n_0+1)^s$. 
%\end{description}
%Wenn nötig, lassen sich (i) und (ii) erreichen, indem man $n_0$ erhöht.
%Eventuell muss dann auch $a$ erhöht werden, um
%sicherzustellen, dass für $n\le n_0$ die Ungleichung $r(n)\le a$ gilt.
%Nachher kann man auch (iii) garantieren, indem man $c$ erhöht.
%
%Wir "`glätten"' die Rekurrenz (\lref{eq:1000}),  
%indem wir $\hat r(n)\Is\max(\{0\}\cup\{r(i) \mid i\le n\})$ definieren.
%Offensichtlich gilt dann $0 \le r(n) \le \hat r(n)$ für alle $n$.
%Daher genügt es, die angegebenen Schranken für $\hat r(n)$ zu beweisen.
%Wir behaupten, dass $\hat r(n)$ die folgende einfachere Rekurrenz erfüllt:
%%
%%\begin{equation}
%%\llabel{eq:2000}
%\[
% \hat r(n)\leq
%\begin{cases}
%a & \text{, wenn }n\leq n_0\text{,}\\
%cn^s + d \cdot \hat r(\ceil{n/b} + e) & \text{, wenn } n > n_0\text{.}
%\end{cases}
%\] 
%%\end{equation}
%%
%Um dies einzusehen, betrachte ein $n$.
%Wenn $n\le n_0$, gilt offenbar $\hat r(n)\leq a$.
%Sei also $n>n_0$. Dann gibt es ein $i\le n$ mit $\hat r(n) = r(i)$. 
%Wenn $i\le n_0$, ergibt sich aus Annahme~(iii) und $\hat r(\ceil{n/b} + e)\ge0$
%die Ungleichung $\hat r(n) = r(i) \le a \le c(n_0+1)^s \le cn^s + d \cdot \hat r(\ceil{n/b} + e)$.
%Wenn $i>n_0$, haben wir $\hat r(n) = r(i) \le  ci^s + d \cdot  r(\ceil{i/b} + e_i) \le cn^s + d \cdot \hat r(\ceil{n/b} + e)$,
%weil $\ceil{i/b} + e_i \le \ceil{n/b} + e$ gilt.
%
%Nun beweisen wir die in Satz~\lref{master:full} behaupteten Schranken
%für jede Funktion $r(n)\ge0$,
%die die Rekurrenz 
%%
%\begin{equation}\llabel{eq:3000}
%r(n)\leq
%\begin{cases}
%a & \text{, wenn }n\leq n_0\text{,}\\
%cn + d \cdot r(\ceil{n/b} + e) & \text{, wenn } n > n_0\text{,}
%\end{cases}
%\end{equation}
%%
%erfüllt, wobei für die Konstanten die Annahmen (i)--(iii) gelten.
%Daraus folgt der Satz. 
%
%Sei $n > n_0$ beliebig.
%Zunächst betrachten wir die Argumente für $r(\cdot)$,
%die entstehen, wenn die Rekurrenz (\lref{eq:3000}) wiederholt angewendet wird. 
%Sei $N_0=n$ und
%$N_i =\ceil{N_{i-1}/b} + e$, für $i=1,2,\ldots$.
%Nach Annahme (i) gilt $N_i < N_{i-1}$, solange $N_{i-1} > n_0$.
%Sei $k$ die kleinste Zahl mit $N_k\le n_0$.
%
%\medskip
%
%\noindent\emph{Behauptung} 1:
%%\begin{equation}\llabel{eq:5000}
%\[
%N_i \le 2n/b^i,\text{ für \ } 0\le i < k.
%%\end{equation}
%\]
%\emph{Beweis} von Beh. 1: Sei $\beta = b^{-1} < 1$. Durch Induktion über $i$ zeigen wir Folgendes:
%\begin{equation}\llabel{eq:4000}
%N_i \le \beta^i n +(e+1)\sum_{0\le j<i}\!\!\beta^j,\text{ für \ } 0\le i < k. 
%\end{equation}
%Der Fall $i=0$ ist trivial. Sei also $0 < i < k$. Dann haben wir:
%\begin{eqnarray*}
%N_i &=& \ceil{\beta N_{i-1}} + e \\
%&\stackrel{\text{I.V.}}{\le}& \ceil{\beta\cdot\left(\beta^{i-1}n +(e+1)\sum_{0\le j<i-1}\!\!\beta^j\right)} + e \\
%&\le& \beta^i n +(e+1)\sum_{0\le j<i-1}\!\!\beta^{j+1} + 1 +e\\
%&=&\beta^i n +(e+1)\sum_{0\le j<i}\beta^j\eqndot
%\end{eqnarray*}
%Weil $\sum_{0\le j<i}\beta^j = (1-\beta^i)/(1-\beta) < 1/(1-\beta)$ (siehe~(\ref{app:notation:eq:geometric})), 
%folgt aus (\lref{eq:4000}) die Ungleichung $n_0 < N_i \le \beta^i n + (e+1)/(1-\beta)$. 
%Nach Annahme (ii) haben wir $(e+1)/(1-\beta) \le n_0/2$.
%Es folgt $(e+1)/(1-\beta) < \beta^i n$,
%also $N_i \le 2 \beta^i n = 2n/b^i$.
%
%\medskip
%
%\noindent\emph{Behauptung} 2: $\log_b(n/n_0) \le k < \log_b(2n/n_0) + 1$ und $b^k =\Th{n}$.\\
%\emph{Beweis} von Beh. 2: Weil $2 n/b^{k-1}\ge N_{k-1} > n_0$ gilt,
%haben wir $b^k < 2bn/n_0$, also $b^k=\Oh{n}$, und $k < \log_b(2n/n_0) + 1$.
%Auf der anderen Seite sieht man mit 
%Induktion ganz leicht, dass $N_i\ge n/b^i$ gilt, für $0\le i \le k$.
%Daher ist $n_0 \ge N_k \ge n/b^k$; daraus folgt $b^k \ge n/n_0$, also $b^k=\Om{n}$, und $k\ge \log_b(n/n_0)$.
%\medskip
%
%Wiederholtes Anwenden der Rekurrenz (\lref{eq:3000}) liefert Folgendes:
%\begin{eqnarray}
%r(n) & \le &  d r(N_1) + c N_0^s \nonumber\\
%&\le & d^2 r(N_2) + cdN_1^s +cN_0^s\nonumber\\
%&\vdots& \vdots \nonumber\\
%&\le & d^k r(N_k) + cd^{k-1}N_{k-1}^s +\ldots + cdN_1^s +cN_0^s\nonumber\\
%&\stackrel{\text{(Beh.\,1)}}{\le} & d^k a + c\sum_{0\le i < k} d^i(2n/b^i)^s\nonumber\\
%& =  & d^k a + 2^s c \cdot n^s \sum_{0\le i < k} (d/b^s)^i.\llabel{eq:6000}
%\end{eqnarray}
%%
%
%\vspace{0.8ex}
%\noindent\mbox{\bf Fall} $d<b^s$. --
%Dann ist die Summe $\sum_{0\le i < k} (d/b^s)^i$ in~(\lref{eq:6000})
%nach (\ref{app:notation:eq:geometric}) durch eine Konstante beschränkt,
%woraus sich $r(n) \le d^k a +  \Oh{n^s}$ ergibt. Weil $d<b^s$, gilt 
%$d^k < (b^k)^s$, und nach Beh.~2 gilt $(b^k)^s=\Oh{n^s}$. Damit erhalten wir $r(n)= \Oh{n^s}$.
%
%\vspace{0.8ex}
%\noindent
%\noindent\mbox{\bf Fall} $d=b^s$. -- Die $k$ Terme in der Summe in (\lref{eq:6000})
%sind alle gleich $1$; daher gilt $r(n) \le d^k a + 2^s c n^s k$. 
%Mit $d^k = (b^{k})^s =\Oh{n^s}$ (nach Beh.~2) erhalten wir
%$r(n) = \Oh{n^s} +  \Oh{n^s k}  = \Oh{n^s}  + \Oh{n^s\log_b n} = \Oh{n^s\log n}$.
%
%\vspace{0.8ex}
%\noindent\mbox{\bf Fall} $d>b^s$. -- Dann steigen die 
%Terme in der Summe in (\lref{eq:6000}) an. 
%Weil $b^k=\Om{n}$ (nach Beh.~2), also $(b^s)^k=\Om{n^s}$, liefert (\ref{app:notation:eq:geometric}) in diesem Fall
%\[n^s \sum_{0\le i < k} (d/b^s)^i < n^s\cdot\frac{(d/b^s)^k}{d/b^s-1} =\Oh{d^k}.\]
%Mit (\lref{eq:6000}) folgt $r(n)=\Oh{d^k}$.
%Nach Beh.~2 haben wir $d^k = b^{k\,{\log_b d}} = \Oh{n^{\log_b d}}$, und wir erhalten $r(n) = \Oh{n^{\log_b d}}$.
%%
%\qed
\end{proof}

Der findes mange yderligere generaliseringer af sætning~\ref{thm:master}:
Man kan afbryde rekursionen tidligt, variere delinstansernes størrelse mere, lade antallet af delinstanser afhænge af instansstørrelsen osv.
For mere information henvises læseren til bøgerne~\cite{GKP94,Sedgewick-Flajolet}\index{Sedgewick, R.}\index{Flajolet, P.}\index{Knuth, D.}\index{Graham, R.
 L.}\index{Patashnik, O.}.

\begin{exerc}
  \llabel{ex:mergesortrecurrence}
  Betragt rekursionsligningen
\[ C(n) = \begin{cases}    1 & \text{for $n = 1$},\\
           C(\floor{n/2})+C(\ceil{n/2})+ cn & \text{for $n > 1$.}
\end{cases} \]
Vis, at $C(n) = O(n\log n)$.
\end{exerc}

\begin{exerc}
  Angiv en del-og-hersk-algoritme, hvis kørselstid er givet af rekursionsligningen
  $T(1)=a$ og \[
    T(n)=\ceil{\sqrt{n}\,\,}\cdot T\left(\left\lceil\frac{n}{\ceil{\sqrt{n}\,\,}}\right\rceil\right)  + cn\quad \text{for }n>1\,.\]
  Vis, at $T(n)=O(n\log\log n)$.
\end{exerc}

\begin{exerc}
  Tidsforbruget for opslag i en datastruktur beskrives ofte af rekursionsligningen
$T(1)=a$, $T(n)=T(n/2) + c$.
Vis, at $T(n)=O(\log n)$.
\end{exerc}

\subsection{»Globale« betragtninger} 

\index{algoritmenanalyse!global|textbf}
De teknikker for algoritmeanalyse, vi har introduceret foroven, er »syntaktiske« i følgende forstand:
For at analysere et større program, betragter vi i første omgang de enkelte dele og kombinerer derefter deres analyseresultater til en samlet analyse af hele programmet.
Ved kombinationsskridtet benyttes summer og rekursionsligninger.

Vi skal dog også bruge en helt anden fremgångsmåde, som man kunne kalde »semantisk«.
Hertil associerer vi dele af programmets kørsel til dele af en kombinatorisk struktur og argumenter siden i termen af denne struktur.
For eksempel kan vi observere, at en bestemt programstump udføres højst én gang per kant i en graf, og at derfor den samlede omkostning svarer til antallet af kanter i grafen.
Eller vi kan observere, at udføreslsen af en vis del af programmet fordobler størrelsen af en vis struktur;
hvis man derudover ved, at strukturens størrelse begynder med 1 og ender som højst $n$ i slutningen af programmet, kan den pågældende del af programmet højst udføres $\log n$ gange.

\section{Gennemsnitsanalyse}
\llabel{s:average case analysis}


\index{algoritmeanalyse!gennnemsnit|textbf}
\index{kørselstid!gennnemsnit}%
I denne afsnit præsenterer vi gennemsnitsanalysen af algoritmer.
Vi gør dette ved hjælp af tre eksempler af voksende kompleksitet. 
Vi går ud fra, at læseren er fortrolig med sandsynlighedsregningens grundlæggende begreber, fx udfaldsrum, middelværdi, indikatorvariabel og middelværdiens linearitet.
Disse begreber er sammenfattede i afsnit~\ref{app:notation:s:prob}.
\index{udfaldsrum}% 
\index{stokastisk variabel}% 
\index{linearitet af middelværdi}% 
\index{middelværdi}% 
\index{indikatorvariabel}% 

\subsection{Tælle én op}

Vi begynder med et enkelt eksempel.
Givet er en række $a[0..(n-1)]$ at nuller og ettere.
Opgaven er at forøge (modulo $2^n$) det binære tal beskrevet af cifrene $a[n-1]$,\ldots, $a[0]$.
\begin{quote}
\begin{code}
$i := 0$\\
\While $(i < n$ og $a[i] = 1)$ \Do $a[i] := 0\mathtt{;}$ $i\Increment$\\
\If $i < n$ \Then $a[i] := 1$
\end{code}
\end{quote}
Hvor ofte bliver løkkens krop udført?
I værste fald (nemlig når $a = \langle 1,\ldots, 1\rangle$) klart $n$ gange, og i bedste fald (nemlig når $a[0] = 0$) klart $0$ gange.
Men hvad er gennemsnitstiden?
For at spørgsmålet giver mening, skal vi først definere en stokastisk model, dvs.\ det udfaldsrum, som ligger til grund for analysen.
\index{udfaldsrum}
Lad os fastlægge os på følgende stokastiske model:
Hvert ciffer er 0 eller 1 med sandsynlighed $\frac12$, og cifrene er uafhængige.
Ækvivalent kunne vi sige, at hver bitfølge med $n$ bit har samme sandsynlighed, nemlig $(\frac12)^n$.
Antal afviklinger af løkkekroppen er $k$, hvis og kun hvis $k\in\{0,\ldots, n-1\}$ og $a[0]=\cdots=a[k-1]=1$ samt $a[k] = 0$, eller hvis $k=n$ og $a = \langle 1,\ldots, 1\rangle$. 
Den første hændelse sker med sandsynlighed $2^{-(k+1)}$, den anden med sandsynlighed $2^{-n}$.
Middelværdien for antallet af løkkegennemløb er altså
\[ \sum_{k=0}^{n-1} k 2^{-(k + 1)} + n 2^{-n} \le \sum_{k=0}^{n-1} (k+1) 2^{-(k + 1)} < \sum_{k \ge 1} k 2^{-k} = 2
\,,\]
hvor den sidste ligning kan findes i appendix som (\ref{app:notation:eq:ipowi}).

\subsection{Finde maksimum}\llabel{s:left:to:right}

Vores næste eksempel er mere krævende.
Vi betragter følgende programstump, som finder den største ingang i rækken $a[1..n]$:
\begin{quote}
\begin{code}
$m := a[1]$\emph{;}\qquad 
\For $i := 2$ \To $n$ \Do \If $a[i] > m$ \Then $m := a[i]$
\end{code}
\end{quote}

Hvor ofte udføres tildelingen $m := a[i]$?
I værste fald (fx når $a=\seq{1,\ldots, n}$) sker tildelingen ved hvert løkkegennemløb, dvs.\ $(n-1)$ gange.
I bedste fald (fx når $a= \seq{1,\ldots,1}$) slet ikke. 
For at bestemme gennemsnittet skal vi igen begynde med at bestemme udfaldsrummet.
Lad os antage, at rækken består af $n$ forskellige indgange, og at hver rækkefølge er lige sandsynlig.
Med andre ord består udfaldsrummet af de $n!$ forskellige permutationer af rækkens indgange. 
Hver permutation er lige sandsynlig, nemlig $1/n!$.
Det er for analysen ligegyldig, hvilke konkrete værdier, der står i rækken (bare de er forskellige); vi kan altså antage, at $a$ indeholder tallene fra $1$ til $n$ i en eller anden rækkefølge.
Vi interesserer os nu for det forventede antal af »foreløbige maksima«.
\index{foreløbigt maksimum|textbf}
Her kaldes en indgang $a[i]$ for \emph{foreløbigt maksimum}, hvis den er større end alle tidligere indgange.
For eksempel har følgen $\seq{1,2,4,3}$ tre foreløbige maksima, og følgen $\seq{3,1,2,4}$ har to foreløbige maksima.
Antallet af gange, tildelingen $m := a[i]$ bliver udført, er 1 mindre end antallet af foreløbige maksima.
For en permutation%
\index{permutation}
$\pi$ af tallene fra $1$ til $n$ lad $M_n(\pi)$ være antallet af foreløbige maksima i $\pi$.
Det står tilbage at bestemme middelværdien $E[M_n]$.
Vi skal vise to måder at gøre det på.
For små $n$ er det nemt at udregne middelværdien direkte.
For $n=1$ findes der kun én permutation, nemlig $\seq{1}$, 
med ét foreløbigt maksimum, så $M_1=1$ og derfor $E[M_1=1]$.
For $n = 2$ findes der to permutationer, nemlig $\seq{1,2}$ og $\seq{2,1}$.
De har henholdsvis 2 og 1 foreløbige maksima, hvorfor $E[M_2] = \frac32$.
For større $n$ skal vi bruge følgende analyse.

Skriv $M_n$ som sum af indikatorvariablerne $I_1$  til $I_n$, dvs. $M_n=I_1+\cdots+I_n$, hvor $I_k=1$ for en permutation $\pi$, hvis den $k$te indgang i $\pi$ er et foreløbigt maksimum.
For eksempel er $I_3(\seq{3,1,2,4}) = 0$ og $I_4(\seq{3,1,2,4}) = 1$. 
Da gælder
\begin{align*}
E[M_n] &= E[I_1 +  \cdots + I_n] \\
&= E[I_1] +  \cdots
+ E[I_n] \\
&= \Pr(I_1 = 1) +  \cdots + \Pr(I_n = 1) \,,
\end{align*}
hvor vi har brugt middelværdiens linearitet for den anden ligning (\ref{app:notation:eq:linearity}), og tredje ligning gælder fordi $I_k$'erne er indikatorvariable.
Vi skal stadig beregne sandsynligheden for hændelsen $I_k = 1$. 
Den $k$te indgang i en tilfældig permutation
\index{Permutation!zufällige}
er et foreløbigt maksimum hvis og kun hvis den er størst blandt de $k$ første indgange.
I en tilfældig permutation står den største af de $k$ første indgange på hver af pladserne med samme sandsynlighed.
Derfor har vi $\Pr(I_k = 1) = 1/k$,  og der gælder
\[ E[M_n] = \sum_{k=1}^n \Pr(I_k = 1) = \sum_{k=1}^n \frac{1}{k} \,. \]
Hermed kan man fx udregne $E[M_4] = 1 + \frac12 + \frac13 + \frac14 = \frac{1}{12}(12 + 6 + 4 + 3) = \frac{25}{12}$.
Summen  $\sum_{k=1}^n 1/k$ vil dukke up igen senere i bogen.
Den kaldes det »$n$te harmoniske tal« 
\index{sum!harmonisk}
\index{harmonisk sum|sieheunter{sum}}
\index{Hn@$H_n$|sieheunter{sum, harmonisk}} 
og betegnes $H_n$. 
Man kan begrænse den som  $\ln n \le H_n \le 1 + \ln n$, dvs. $H_n \approx \ln n$, se~(\ref{app:notation:eq:harmonic}).
Vi konkluderer, at antallet af foreløbige maksima i gennemsnit er meget mindre end i værste fald.

\begin{exerc}
  \llabel{ex:harmonic}
  Bevis $\displaystyle\sum_{k=1}^n \frac{1}{k}\leq \ln n + 1$. 
  \emph{Vink}:
  Begynd med at vise $\displaystyle\sum_{k=2}^n \frac{1}{k}\leq \int_{1}^n \frac{1}{x}\ dx$.
\end{exerc}

Nu skal vis beskrive en anden metode for at bestemme det forventede antal af foreløbige maksima.
Vi vil forkorte $A_n=E[M_n]$ for $n\ge1$ og sætter $A_0=0$. 
Enhver permutations første indgang er et foreløbigt maksimum, og hvert tal har sandsynlighed $1/n$ for at stå på første plads.
Når første tal er $i$, kan kun tallene fra $i+1$ til $n$ være foreløbige maksima længere til højre.
Disse $n-i$ står i resten af permutationen i tilfældig rækkefølge; derfor er det forventede antal yderligere foreløbige maksima givet ved $A_{n-i}$. 
Vi har altså rekursionsligningen
\[ A_n = 1 + \frac{1}{n}\cdot\sum_{i=1}^{n} A_{n - i}  \qquad\text{dvs.}\qquad
nA_n = n + \sum_{i=1}^{n-1} A_i 
\,.\]
Denne ligning kan forenkles med et standardtrick.
Samme ligning for $n-1$ i stedet for $n$ giver $(n-1)A_{n-1} = n-1 + \sum_{i=1}^{n-2} A_i$.
Vi trækker denne ligning fra ligningen for $n$ og opnår
\[ nA_n - (n-1)A_{n-1} = 1 + A_{n-1} \qquad\text{dvs.}\qquad A_n = 1/n + A_{n-1}
\,,\]
som giver $A_n = H_n$.  

\subsection{Lineær søgning}

Vi er nået til det tredje og og mest krævende eksempel.
Det drejer sig om følgende søgningsproblem.
\index{søgning!lineær}
\index{søgning!dynamisk|textbf}
Vi begynder med at placere indgange med nummer 1 til $n$ på en eller anden måde, indgang $i$ står fx på position $\ell_i$.
Efter placeringen gennemføres nogle søgninger.
For at lede efter en indgang med nøgle $x$, gennemgås følgen fra venstre til højre, til vi støder på nøgle $x$.
På denne måde koster det $\ell_i$ skridt at få adgang til den  $i$te indgang.   

Vi antager nu, at sandsynligheden for at søge efter en bestemt indgang er den samme for hver søgning.
Vi vedtager, at indgang nummer $i$ søges med sandsynlighed $p_i$.
Der gælder $p_i \ge 0$ for alle $i\in\{1,\ldots,  n\}$, og $\sum_i p_i = 1$.
I denne situation er de \emph{forventede} eller \emph{gennemsnitlige} omkostninger ved en søgning lig med $\sum_i p_i \ell_i$, fordi vi leder efter den $i$te indgang med sandsynlighed $p_i$, og denne søgning koster $\ell_i$ skridt. 

Hvad er nu den »bedste« måde at arrangere de $n$ indgange på, dvs.\ den ordning, som minimerer gennemsnitssøgetiden?
Intuitivt er det klart, at indgangene nok bør sorteres efter faldende søgningssandsynlighed.
Lad os vise dette.

\begin{lemma} 
  En ordning er optimal med hensyn til den forventede søgetid, hvis der gælder $\ell_i<\ell_j$ for alle $i,j\in\{1,\ldots,m\}$ med $p_i>p_j$.
  For søgningssandsynligheder med  $p_1 \ge p_2 \ge \cdots \ge p_n$ fører ordningen $\ell_1= 1$, $\ldots$, $\ell_n =n$ til den optimale forventede søgetid $\sum_i p_i i$. 
\end{lemma} 
%
\begin{proof} 
  Betragt modsætningsvist en optimal ordning, hvor der findes $i$ og $j$ med $p_i > p_j$ og $\ell_i > \ell_j$, dvs.\ at indgang $i$ er mere sandsynlig end indgang $j$ men alligvel ordnet længere bagved.
  Hvis vi bytter runndt på de to ingange, er ændringen i gennemsnitlig søgetid givet ved
  \[ - (p_i \ell_i + p_j \ell_j) + (p_i \ell_j + p_j \ell_i) = (p_j - p_i) (\ell_i - \ell_j) < 0
  \,. \]
  Med andre ord er den nye ordning bedre end den gamle, i modstrid med antagelsen.

  Betragt først tilfældet $p_1 > p_2 > \cdots > p_n$. 
  Idet der kun findes endelig mange ordninger (nemlig $n!$), eksisterer der en optimal ordning.
  Hvis $i<j$ og indgang $i$ er ordnet efter indgang $j$, så kan ordningen ifølge afsnittet foroven ikke være optimal.
  Derfor placerer den optimale ordning indgang $i$ på plads $\ell_i = i$, og den resulterende forventede søgetid er derfor $\sum_i p_i i$. 

  Hvis $p_1 \ge p_2 \ge \cdots \ge p_n$, så er ordningen $\ell_i = i$ for alle $i$ stadig optimal.
  Hvis nogle af sandsynlighederne er ens, findes mere end en optimal ordning.
  Men ordningen af indgange med samme sandsynlighed er irrelevant.
\end{proof}  

%TODO extremely unsatisfying proof

Kan vi også gøre noget snedigt, hvis vi ikke kender sandsynlighederne $p_i$? 
Svaret er ja, og den enkle strategi er den såkalde »flyt-forrest«-heuristik.
\index{flyt-forrest}
\index{heuristik!flyt forrest}
Den virker på følgende måde.
Antag, at en søgning leder efter indgang $i$ og finder den på plads $\ell_i$.
Hvis $\ell_i = 1$, så er vi godt tilfredse og gør ikke mere.
Ellers flytter vi ingang $i$ til første plads og forskyder indgangene på pladsern $1$, $\ldots$,  $\ell_i-1$ en position bagud.
På denne måde håber vi, at hyppigt søgte indgange står tidligt i ordningen, mens sjældent søgte indgange befinder sig længere bagud.
Lad os analysere den forventede opørsel af flyt-forrest-heuristiken.

\begin{thm} 
  Ser man bort fra den første søgning efter hvert element, så er den gennemsnitlige omkostning ved søgning med flyt-forrest-heuristikken højst dobbelt så stor som den gennemsnitlige omkostning ved en optimal, fast ordning.
\end{thm}

\begin{proof}
\newcommand{\MTF}{{\mathit{ff}}}%
Vi går ud fra, at de $n$ indgange i listen oprindeligt står i en bestemt, men vilkårlig rækkefølge.
Vores stokastiske model er en følge af søgningsrunder, hvor hver runde \emph{uafhængigt af tidligere runder} søger efter en indgang, og denne indgang er valgt sådan, at indgang $i$ vælges med sandsynlighed $p_i$.
Den første søgning efter indgang $i$ afhænger hovedsagligt af den oprindelige rækkefølge og har derfor ikke særlig meget med heuristikken at gøre.
Derfor ignorerer vi den første søgning efter indgang $i$ og vedtager helt enkelt, at den har omkostning 1.
\footnote{Hermed ignorer vi omkostninger på højst $n(n-1)$.
Man kan vise, at den forventede afvigelse ved denne forenkling af argumentet for den $t$te søgning er 
$n\cdot \sum_i p_i(1-p_i)^{t-1}$.
Lad $t > n$. 
Idet funktionen $p\mapsto p(1-p)^{t-1}$ har sit maksimum i $p=1/t$, bliver summen maksimeret ved at sætte $p_1=\cdots=p_{n-1}=1/t$ og $p_n=1-(n-1)/t$.
Afgivelsen er altså  $n(n-1)(1/t)(1-1/t)^{t-1} < n^2/et$.}%
Betragt søgerunde~$t$.
Lad $C_{\MTF}$ betegne omkostningen i runde $t$.
For hvert  $i$ lad $\ell_i$ angive indgang $i$s plads i listen i begyndelsen af runde $t$.
Betragt de stokastiske variable $\ell_1$, $\ldots$ , $\ell_n$; de afhænger kun af, hvad der er sket i de første $t-1$ runder.
Hvis $t$ søger efter indgang $i$, så er omkostningen  $1+Z_i$, hvor
\[ Z_i = 
  \begin{cases}  
  \ell_i - 1 & \text{hvis $i$ blev søgt før runde $t$}\,,\\
            0 & \text{ellers.}
  \end{cases}
\]
De stokastiske variable $Z_1$, $\ldots$, $Z_n$ afhænger kun af, hvad der er sket i de første $t-1$ runder.
Vi har altså $C_{\MTF} = \sum_i p_i (1+E[Z_i]) = 1 + \sum_i p_i E[Z_i]$.
Vi skal nu bestemme $E[Z_i]$ for givet $i$.
Definér hertil for alle $j\neq i$ indikatorvariablen  
\[ I_{ij} = \begin{cases}  1 & \text{$j$ står før $i$ i starten af runde  $t$,}\\[-0.5ex]
  & \text{og minst en af de to indgange blev søgt før runde $t$}\\
           0 & \text{ellers.}\end{cases} \]
Da gælder $Z_i \le \sum_{j\colon j\neq i} I_{ij}$. 
(Hvis runde $t$ er den første runde, som søger efter $i$, så er $Z_i=0$. 
Hvis runde $t$ ikke er den første runde, som søger efter $i$, så er $I_{ij}=1$ for hvert $j$, som står før $i$ i listen, dvs. $Z_i=\sum_{j\colon j\not= i} I_{ij}$.)
%TODO What?
Derfor gælder også $E[Z_i] \le \sum_{j\colon j\not= i} E[I_{ij}]$.
Vi skal altså bestemme middelværdien $E[I_{ij}]$ for hvert $j\not= i$. 

Hvis der ikke forekom nogen søgning efter hverken $i$ eller $j$ for runde $t$, har vi $I_{ij}=0$.
Ellers betragter vi den sidste runde $t_{ij} < t$, som søgte efter enten $i$ eller $j$.
Den betingede sandsynlighed for, at denne søgning var efter $j$ i stedet for efter $i$, dvs.\ at $I_{ij} = 1$, er $p_j/(p_i+p_j)$.
Samlet får vi: 
$E[I_{ij}]=\Pr{I_{ij}=1} \le p_j/(p_i+p_j)$, dvs. $E[Z_i] \le \sum_{j \colon j\not= i} p_j/(p_i+p_j)$.

Ved at summere over alle $i$ får vi nu:
\[ C_{\MTF} = 1 + \sum_i p_i E[Z_i]  \le
1 + \sum_{i,j \colon i \not= j} \frac{p_i p_j}{p_i + p_j}\,. \]
Bemærk nu, at termen $p_i p_j/(p_i + p_j) = p_j p_i/(p_j + p_i)$ optræder to gange i summen for hvert par $(i,j)$ med $j < i$. 
For at forenkle notationen antager vi, at $p_1 \ge p_2 \ge \cdots \ge p_n$.
\index{online-algoritme} 
Med $\sum_i p_i=1$ fås
\begin{align*}
C_{\MTF} &\le 1 + 2 \sum_{i,j \colon j < i} \frac{p_i p_j}{p_i + p_j} 
         = \sum_i p_i \left(1 + 2 \sum_{j \colon  j < i} \frac{p_j}{p_i + p_j}\right) \\
         &\le \sum_i p_i \left(1 + 2 \sum_{j \colon  j < i} 1\right) 
         < \sum_i p_i 2i = 2 \sum_i p_i i = 2 \Id{Opt} \,. 
\end{align*}
       
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Randomiserede algoritmer}
\llabel{s:random}
\index{randomiseret algoritme|sieheunter{algoritmekonstruktion; algoritmenanalyse}}%
\index{algoritmekonstruktion!randomiseret|textbf}
\index{algoritmeanalyse!randomiseret|textbf}

Lykke deltager i et underholdningsprogram på tv, som drejer sig om 100 æsker, nummereret fra 1 til 100, som hun har lov til at åbne i vilkårlig rækkefølge.
Æske~$i$ indeholder $m_i$~kr.
Lykke kender ikke $m_i$ inden hun har åbnet æske~$i$.
Forskellige æsker indeholder forskellige beløb.
Reglerne er:
\begin{itemize}
\item I begyndelsen af spillet får Lykke 10 jetoner fra studieværten.
\item Når hun åbner en æske, som indeholder et større beløb end alle tidligere åbnede æsker, betaler hun en jeton.
\footnote{Den først åbnede æskes beløb er per definition større end alle tidligerede åbnede æsker. 
    Det koster altså Lykke under alle omstændigheder 1~jeton at åbne den første æske.}
\item Når Lykke skal betale en jeton, men ikke har flere, ender spillet og Lykke har tabt.
\item Hvis Lykke kan åbne samtlige æsker, får hun lov til at beholde alle pengene.
\end{itemize}
Æskerne er dekorerede med pudsige billeder, og studieværten forsøger sommetider at påvirke Lykke ved at foreslå, hvilken æske hun burde åbne hernæst.
Lykkes tante, som aldrig går glip af et eneste afsnit at tv-programmet, kan fortælle hende, at spillet kun sjældent har en vinder.
Lykke spørger sig selv, om det overhovedet kan betale sig at deltage i programmet.
Findes der en strategi, der giver en god gevinstchance?
Og er studieværtens forslag værd at følge?

Lad os begynde med at analysere den åbenlyse algoritme, nemlig at Lykke altid følger studieværtens forslag.
I værste fald vil han pege på æsker med voksende beløb.
Så skal hun aflevere en jeton ved hver æske og har tabt, når spillet er nået til den 11. æske.
Både kandidater og tv-publikum ville ærge sig over den fæle studievært, seertallene ville falde, og studieværten blive fyret.
Værstefaldsanalysen hjælper os ikke meget videre.
I bedste fald foreslår studieværten straks den bedste æske, dvs.\ den med det største beløb.
Lykke ville være godt tilfreds, men programmet ville være for kort til en reklamepause og den venlige studievært ville også blive fyret.
Bedstefaldsanalysen hjælper os altså heller ikke videre.

Efter en smule overvejelse indser vi, at spillet egentlig bare er en omformulering af problemet at finde foreløbige maksima fra foregående afsnit.
Lykke bliver af med en jeton, hver gang hun støder på et nyt maksimum.
Fra sidste afsnit ved vi, at det forventede antal foreløbige maksima i en tilfældig permutation er
\index{Permutation!zufällige}
er det  $n$te harmoniske tal $H_n$.%
\index{sum!harmonisk}
\index{foreløbigt maksimum} 
For $n = 100$ gælder $H_n < 6$. 
Hvis studieværten altså foreslår æskerne i (uniformt) tilfældig orden, kommer Lykke af med lidt under 6 jetoner i gennemsnit. 
Men hvorfor skulle studienværten følge netop denne strategi?
Der er jo fra hans perspektiv ingen grund til at nærmest garantere deltagerens sejr.

Løsningen for Lykke er at tage skæbnen i egen hånd ved at \emph{åbne æskerne i tilfældig rækkefølge.}
Hun vælger en æske tilfældigt og åbner den, vælger næste æske tilfældigt bland de uåbnede, åbner denne, og så videre.
Hvordan vælger hun en tilfældig æske?
Hvis der er $k$ uåbnede æsker tilbage, vælger hun et tilfældigt heltal
\index{tilfældigt tal|textbf}
i området fra $1$ til $k$, fx ved at dreje en snurretop, hvis rand er en regelmæssig $k$-kant, eller ved at bruge et lykkehjul med $k$ muligheder.
På denne måde skaber Lykke en tilfældig permutation af æskerne, og vi kan genbruge analysen fra foregående afsnit.
I gennemsnit skal hun betale færre end 6~jetoner, og de 10~jetoner er næsten altid nok.
Vi har netop beskrevet en \emph{randomiseret algoritme}.
Det er værd at understrege, at selvom den matematiske analyse er identisk med den situation, hvor studieværten anbefaler æskerne i tilfældig rækkefølge, så er konklusionen helt forskellig.
I gennemsnitsscenariet er vi afhængige af, at studienværten vitterlig benytter en tilfældig rækkefølge.
I så fald gælder analysen, men hvis han følger en anden strategi, gælder analysen ikke.
Hvad der virkelig er foregået i programmet, kan man i bedste fald sige noget om efter rigtig mange udsendelser og kun i bakspejlet. 
Situationen i scenariet »Lykke bruger en randomiseret algoritme« er helt anderledes.
Hun udfører nemlig selv de tilfældige hændelser og skaber derved en tilfældig permutation.
Analysen gælder, uanset hvad studieværten foretager sig.

\subsection{Formel model}

Formelt udstyrer vi registermaskinenn med yderligere en instruktion:
$R_i := \Id{tilfældigtTal}(R_j)$
tildeler register $R_i$ en tilfældig værdi i mængden $\{0,\ldots,k-1\}$, hvor $k$ er indholdet af $R_j$. 
Alle $k$ værdier har samme sandsynlighed, og udfaldet er uafhængigt at samtlige tildigere tilfældige valg.	
 
I pseudokode skriver vi $v := \Id{tilfældigtTal}(C)$, hvor $v$ er en heltalsvariabel og $C$ en heltalsværdi.
Vi vedtager, at det tager konstant tid at vælge et tilfældigt tal.
Algoritmer uden den slags tilfældige tildelinger kaldes
\emph{deterministiske}.
\index{algoritmekonstruktion!deterministisk|textbf}
\index{deterministisk algoritme|sieheunter{algoritmekonstruktion}}. 

Kørselstiden for en randomiseret algoritme afhænger af dens tilfældige valg.
Dermed er kørselstiden for instans $i$ ikke længere et tal, men en stokastisk variabel, som afhænger af algoritmens tilfældige valg.
Vi kan undgå at lade kørselstiden afhænge af de tilfældige valg ved at udstyre maskinen med et »stopur«.
I begyndelsen af programkørslen sætter vi stopuret til værdien $T(n)$, hvor $n$ angiver størrelsen af instansen.
Under kørslen tæller vi uret ned skridt for skridt, og standser efter $T(n)$ skridt.
Skulle programmet standses »i utide« på denne måde, kommer det ikke til at producere sit sædvanlige svar og kørslen bedømmes som mislykket. 

En randomiseret algoritmes svar kan ligeledes afhænge af de tilfældige valg.
Man kan naturligvis sætte spørgsmålstegn ved værdien af en algoritme, hvis svar på instans $i$ afhænger af tilfældigheder -- det kan fx være »ja« den ene dag og »nej« den anden.
Hvis de to svar er lige sandsynlige, er algoritmens svar ganske rigtig værdiløs.
Men hvis det rigtige svar er meget mere sandsynligt end det forkerte, så er svaret af nytte.
Vi forklarer det med et eksempel.

Ane og Bo er forbundet over en langsom telefonledning. 
Ane kender et $n$-bitstal $x$, Bo kender et $n$-bitstal $y$.
De ønsker at finde ud af, om de to tal er ens.
Fordi deres kommunikationskanal er langsam, vil de kommunikere mindst mulig information.
På den anden side spiller det ingen rolle, hvor store beregninger de hver især skal gennemføre.

Den oplagte løsning er, at Ane sender sit tal til Bo, som undersøger, om de to tal er ens, og sender resultatet tilbage.
Dette kræver, at $n$ bit information kommunikeres over ledningen.
Alternativt kan Ane formidle sit tal ciffer for ciffer, Bob kan så sammenligne cifrene et ad gangen og annoncere resultatet, så snart han kender det, dvs.\ så snart to cifre på samme plads er forskellige.
I værste fald skal alle $n$ cifre kommunikeres, nemlig når begge tal er det samme.
Vi skal nu se, at randomisering fører til en dramatisk forbedring.
Vi vil blot kommunikere $O(\log n)$ bit inden resulatet om lighed eller ulighed kan annonceres, og svaret er korrekt med høj sandsynlighed.

Ane og Bo bruger følgende protokol.
Hver av dem skaber en ordnet liste $p_1,\ldots,p_L$ af primtal, nemlig de $L$ mindste primtal større end $2^k$.
Vi skal se forneden, hvordan værdierne for $k$ og $L$ skal bestemmes (i afhægighed af $n$).
På denne måde sikres, at Ane og Bo skaber samme lister.
Nu vælger Ane et tilfældigt indeks $i\in\{1,\ldots, L\}$ og sender $i$ samt $x_A \bmod p_i$ til Bo.
Han beregner $x_B \bmod p_i$. 
Hvis $x_A \bmod p_i \not= x_B \bmod p_i$, så melder han, at tallene er forskellige.
Ellers, at de er ens.
Hvis $x_A = x_B$, så er det klart, at Bos svar bliver »ens«.
Hvis derimod $x_A \not= x_B$, men alligevel  $x_A  \bmod p_i = x_B \bmod p_i$, så vil Bo fejlagtigt erklære det to tal for ens. 
Hvor stor er sandsynligheden for denne fejl?
  
Fejlen opstår, når $x_A \not= x_B$ og $x_A \equiv x_B \pmod{p_i}$.
Det sidste betingelse er ensbetydende med, at $p_i$ deler differensen $D = x_A - x_B$.
Absolutværdien af denne differens er højst $2^n$.
Idet hvert $p_i$ er større end  $2^k$, indeholder Anes liste højst $n/k$ primtal, som deler $D$.
(Lad $d$ være antallet af primtal i listen, som deler $D$. 
Da gælder $2^n \ge |D|  \ge (2^k)^d = 2^{kd}$, altså $d \le n/k$.)
Derfor er fejlsandsynligheden højst $(n/k)/L$.
Denne sandsynlighed kan gøres vilkårlig lille ved at lade $L$ være tilstrækkelig stor.
Hvis vi fx vil have fejlsandsynlighed højst $\num{0,000001} = 10^{-6}$, sætter vi $L=10^6 (n/k)$.
%TODO mindre end?

Men hvordan vælges $k$?
For tilstrækkeligt store $k$ er der bland tallene i $\{2^k,\ldots,2^{k+1}-1\}$ omtrent $2^k/\ln(2^k)\approx \num{1,4427}\cdot 2^k/k$ mange primtal.%
\footnote{For $x\ge1$ lad $\pi(x)$ betegne antal primtal, som er mindre end eller lig med $x$.
For eksempel er $\pi(10) = 4$, fordi der findes fire primtal (2, 3, 5 og 7), som er mindre end eller lig med 10.
Ifølge talteoriens Primtalssætning gælder $x/(\ln x + 2) < \pi(x) < x/(\ln x - 4)$ for all $x\ge 55$.
}
Hvis altså $2^k/k  \ge  10^6 n/k$, består listen udelukkende af tal med $k+1$~bit.
Betingelsen $2^k \ge  10^6 n$ er ensbetydende med $k \ge \log n + 6\log 10$.
Dette valg af $k$ medfører, at Ane sender  $\log L + k + 1 = \log n + 12 \log 10 + 1$~bit.
\emph{Dette udgør en eksponentiel forbedring i forhold til den naive protokol!}

Hvordan kan vi reducere fejlsandsynligheden yderligere, fx til højst $10^{-12}$?
Vi kan selfølgeligt bare sætte $L = 10^{12} n/k$ i argumentet foroven.
En alternativ løsning er at køre protokollen to gange, og annoncere, at de to tal er ens, hvis begge udførsler erklærer tallene for ens.
Denne totrinsprotokol tager fejl, kun hvis begge trin tog fejl; derfor er fejlsandsynligheden
højst $10^{-6} \cdot 10^{-6} = 10^{-12}$.  

%TODO confusion in original about "at most" and "less than"

\begin{exerc}
  Sammenlign kørselstiden for de to metoder, som opnår fejlsandsynlighed $10^{-12}$.
\end{exerc}

\begin{exerc} 
  Det virker fjollet, at Ane og Bo skal skabe enormt lange lister af primtal.
  Diskuter i stedet følgende variation af protokollet:
  Ane vælger et tilfældigt tal $p$ med $k+1$ bit (og foranstillet $1$) og afgør, om $p$ er primsk.
  Hvis dette ikke er tilfældet, gentager hun forsøget.
  Hvis $p$ er et primtal, sender hun $p$ og $p$ und $x_A\bmod p$ til Bo.
  Bo udregner $x_B\bmod p$ og sammenligner. 
\end{exerc}

\begin{exerc}\llabel{e:decreasing-error-probability} 
  Antag, at du har en randomiseret algoritme, som giver det forkerte svar med sandsynlighed højst $\frac14$.
  Gentag algoritmen $k$ gange og returner flertalsafgørelsen, dvs. det svar, som optræder mere end $k/2$ gange, hvis det findes. 
  Beregn fejlsandsynligheden som funktion af $k$.
  Find et præcist udtryk for  $k=2$ og $k=3$, og en øvre grænse for større $k$.
  Hvordan skal man vælge $k$ for at opnå en fejlsandsynlighed, som er mindre en et givet $\epsilon$?
\end{exerc}

\subsection{Las-Vegas- og Monte-Carlo-algoritmer}

Der findes i grunden to varianter af randomiserede algoritmer, 
nemlig Las-Vegas-algoritmer%
\index{algoritmekonstruktion!randomiseret!Las Vegas|textbf}
\index{Las-Vegas-algoritme|sieheunter{algoritmekonstruktion, randomiseret}} 
og Monte-Carlo-algoritmer.
\index{algoritmekonstruktion!randomiseret!Monte Carlo|textbf}
\index{Monte-Carlo-algoritme|sieheunter{algoritmekonstruktion, randomiseret}}
En \emph{Las-Vegas-algoritme} giver altid det rigtige svar, men dens kørselstid på input~$i$ er en stokastisk variabel. 
Vores løsning til tv-programmet med Lykke er en Las-Vegas-algoritme, hvis vi fortsætter med at lade den lede efter den rigtige æske;
men antallet af foreløbige maksima, dvs. antallet af betalte jetoner, er en stokastisk variabel.
En Monte-Carlo-algoritme har altid den samme kørselstid, men der findes en positiv sandsynlighed for, at den giver et forkert svar.
Sandsynligheden for et forkert svar er højst $\frac14$.
Algoritmen, som sammenligner to tal over en telefonforbindelse, er en Monte-Carlo-algoritme. 
Opgave~\lref{e:decreasing-error-probability} viser, at fejlsandsynligheden for en Monte-Carlo-algoritme kan gøres vilkårlig lille ved gentalse.  

\begin{exerc}
  Antag, at du har en Las-Vegas-algoritme med forventet kørselstid $t(n)$. 
  Lad den køre i $4t(n)$ skridt.
  Skulle den give et svar inden for denne tid, returneres dette svar, ellers returneres en vilkårlig værdi.
  Vis, at den resulterende algoritme er en Monte-Carlo-algoritme.
\end{exerc}

\begin{exerc}
  Antag, at du har en Monte-Carlo-algoritme med kørselstid $m(n)$, som giver det rigtige svar med sandsynlighed mindst $p$, og en deterministisk algoritme, som i tid $v(n)$ kan afgøre, om Monte-Carlo-algoritmens svar er korrekt.
  Beskriv, hvordan de to algoritmer kan kombineres til et Las-Vegas-algoritme med forventet kørselstid $(m(n)+v(n))/(1-p)$.
\end{exerc}

Afslutningsvis vender vi tilbage til Lykkes tv-program.
Hun har 10 jetoner og forventer at skulle bruge færre end 6 af dem.
Hvor sikker kan hun være på at vinde?
Vi skal altså vurdere sandsynligheden for, at $M_n$ er mindst 11, idet Lykke taber hvis og kun hvis antallet af foreløbige maksima i følgen af åbnede æskebeløb er 11 eller mere.
Hertil bruges \emph{Markovs ulighed}:
\index{Markov@Markov, A.}
\index{Markovs ulighed|sieheunter{ulighed}}
\index{ulighed!Markovs}
Lad $X$ være en ikke-negativ stokastisk variabel og $c\ge 1$ en vilkårlig konstant.
Da gælder $\Pr(X \ge c \cdot E[X])\le 1/c$, se~(\ref{app:notation:eq:markov}).
Vi bruger dette med $X = M_n$ og $c = \frac{11}{6}$ og får
\[ \Pr(M_n \ge 11) \le \Pr(M_n \ge \tfrac{11}{6} E[M_n]) \le \tfrac{6}{11}
\,,\]
så Lykkes vinder med sandsynlighed større end $5/11$.




\section{Grafer}
\llabel{s:graphnot}


\index{graf|textbf}%
\index{knude|textbf}%
\index{hjørne|siehe{knude}}%
\index{punkt|siehe{knude}}%
\index{kant|textbf}%
\index{bue|siehe{kant}}%
Grafer er et overordentligt nyttigt koncept i algoritmikken.
Vi bruger dem altid, når vi skal modelle objekter og deres indbyrdes relationer; i grafterminologien hedder objekterne \emph{knuder}, og relationerne mellem par af knuder hedder \emph{kanter}.
Temmelig åbenlyse anvendelser af grafer er vejkort og kommunikationsnetværk,
\index{vejkort}
\index{kommunikationsnetværk}
men der findes også mere abstrakte anvendelser.
For eksempel kan knuderne repræsentere de delopgaver, som skal udføres, når man bygger et hus, fx »byg murene« eller »sæt vinduerne i«, og kanterne kan beskrive præcedens- eller forrangsrelationer, fx »murene skal bygges, inden vinduerne kan sættes i«.
\index{præcedensrelation}
\index{relation!præcedens-}
Vi kommer også til at lære mange datastukturer at kende, i hvilke vi på en naturlig måde kan opfatte hvert objekte som en knude og hver peger som en rettet kant fra det objekt, den går ud fra, til det objekt, den peger på.

\begin{figure}
\tikzset{
	vertex/.style = {circle, fill, inner sep = 1.5pt},
	weight/.style = {font=\small, midway, auto, inner sep = 1pt, circle},
	>=stealth'}
\noindent
\begin{tabular}{cccc}

\begin{tikzpicture}[baseline]
\node [vertex] (u) [label =  below : $u$] {}; 
\draw [->] (u) edge [loop above] (u);
\node [red] at (0,1) {\emph{løkke}};
\end{tikzpicture}
&
&
\begin{tikzpicture}[scale = .7, baseline]
\node [vertex] (w) at (210:1cm) [label = 210:$w$] {}; 
\node [vertex] (v) at (-30:1cm) [label = -30:$v$] {}; 
\node [vertex] (u) at (90:1cm) [label =   90:$u$] {}; 
\node [vertex] (t) at (2,-.5) [label =   below:$t$] {}; 
\node [vertex] (s) at (2,1) [label =   above:$s$] {}; 
\node [vertex] (x) at (3,.5) [label =   below:$x$] {}; 
\node at (2,2.5) {$U$};
\draw  (u)--(v)--(w)--(u);
\draw  (s)--(t);
\end{tikzpicture}
&
\begin{tikzpicture}[scale = .7, baseline]
\node [vertex] (u) at ( 90:1cm) {}; 
\node [vertex] (v) at (162:1cm) {}; 
\node [vertex] (w) at (234:1cm) {}; 
\node [vertex] (x) at (306:1cm) {}; 
\node [vertex] (y) at ( 18:1cm) {}; 
\node at (0,2) {$K_5$};
\draw (u)--(v)--(w)--(x)--(y)--(u);
\draw (u)--(w)--(y)--(v)--(x)--(u);
\end{tikzpicture}
\\
\begin{tikzpicture}[baseline]
\node  at (0,1) {$H$};
\node [vertex] (v)              [label = below left:  $v$] {}; 
\node [vertex] (w) at (.75,.75) [label = above right: $w$] {}; 
\draw [->] (v) -- (w) node [weight] {1};
\end{tikzpicture}
&
\multirow{2}*[4cm]{\begin{tikzpicture}[baseline=(v),scale = 1, auto]
\node [vertex] (v) at (0,0) [label = below left:$v$] {}; 
\node [vertex] (u) at (3,0) [label = below right:$u$] {}; 
\node [vertex] (t) at (3,3) [label = above right:$t$] {}; 
\node [vertex] (s) at (0,3) [label = above left: $s$] {}; 
\node [vertex] (w) at (.75,.75) [label = above right: $w$] {}; 
\node [vertex] (x) at (2.25,.75) [label = above left: $x$] {}; 
\node [vertex] (y) at (2.25,2.25) [label = below left: $y$] {}; 
\node [vertex] (z) at (.75,2.25) [label = below right: $z$] {}; 
\node  at (1.5, 4) {$G$};
\node [red] at (1.5,-.5) {\emph{rettet}};
\draw [->] (u) edge node [weight] {1} (v);
\draw [->] (t) edge node [weight] {1} (u);
\draw [->] (s) edge node [weight] {1} (t);
\draw [->] (s) edge node [weight, auto = right] {2} (v);
\draw [->] (v) -- (w) node [weight] {1};
\draw [->] (w) edge node [weight, auto = right] {1} (x);
\draw [->] (x) edge node [weight, auto = right] {1} (y);
\draw [->] (y) edge node [weight, auto = right] {1} (z);
\draw [->] (z) edge node [weight, auto = right] {2} (w);
\draw [->] (z) edge node [weight] {1} (s);
\draw [->] (x) edge node [weight] {2} (u);
\draw [->] (y) edge node [weight] {2} (t);
\end{tikzpicture}
}
&
\begin{tikzpicture}[scale =.7, baseline = (w)]
\node [vertex] (w) at (210:1cm) {}; 
\node [vertex] (v) at (-30:1cm) {}; 
\node [vertex] (u) at (90:1cm)  {}; 
\draw  (u)--(v)--(w)--(u);
\node [red] at (0,-1.2) {\emph{urettet}};
\end{tikzpicture}
\quad
\begin{tikzpicture}[scale = .7,baseline = (w)]
\node [vertex] (w) at (210:1cm) {}; 
\node [vertex] (v) at (-30:1cm) {}; 
\node [vertex] (u) at (90:1cm)  {}; 
\draw [->] (u) to [bend left= 10] (v);
\draw [->] (v) to [bend left= 10] (u);
\draw [->] (w) to [bend left= 10] (u);
\draw [->] (u) to [bend left= 10] (w);
\draw [->] (v) to [bend left= 10] (w);
\draw [->] (w) to [bend left= 10] (v);
\node [red] at (0,-1.2) {\emph{dobbeltrettet}};
\end{tikzpicture}
&
\begin{tikzpicture}[scale = .5, baseline]
\node [vertex] (u) at (-1,0) {}; 
\node [vertex] (v) at (1,0) {}; 
\node [vertex] (w) at (-1,1) {}; 
\node [vertex] (x) at (1,1) {}; 
\node [vertex] (y) at (-1,2) {}; 
\node [vertex] (z) at (1,2) {}; 
\node at (0,3) {$K_{3,3}$};
\draw (u)--(v);
\draw (u)--(x);
\draw (u)--(z);
\draw (w)--(v);
\draw (w)--(x);
\draw (w)--(z);
\draw (y)--(v);
\draw (y)--(x);
\draw (y)--(z);
\end{tikzpicture}
\end{tabular}
\caption{Nogle grafer.}
\end{figure}

Når mennesker gør sig overvejelser om grafer, finder de det normalt bekvemt at arbejde med billeder, som fremstiller knuder som små cirkler og kanter som lige eller bøjede linjer eller pile.
For at bearbejde grafer algoritmisk har vi dog brug for en mere matematisk orienteret notation:
en \emph{rettet graf} $G=(V,E)$ er et par bestående af en mængde $V$ af \emph{knuder} (også kaldt \emph{hjørner} eller \emph{punkter}) og en mængde $E\subseteq V\times V$ af \emph{kanter} (også kaldt \emph{buer}).
\index{graf!rettet|textbf}
Sommetider støder man på ordet »\emph{digraf}« for »rettet graf« (fra eng. \emph{directed graph}).\footnote{%
  Ovs. anm.: Man kan desuden støde på »orienteret graf« brugt i betydningen »rettet graf«.
  Desværre er »orienteret graf« dog et veletableret begreb i grafteori og betyder noget andet: 
  En rettet graf er \emph{orienteret}, hvis den ikke indeholder et par af knuder, som er forbundet af to kanter i hver sin retning.
  Orienterede grafer er altså rettede, men rettede grafer er ikke nødvendigvis orienterede, idet de kan indeholde modsatrettede kanter.
  }
Fig.~2.7 viser den rettede graf $G=(\{s,t,u,v,w,x,y,z\},\{(s,t), (t,u), (u,v), (v,w), \allowbreak (w,x), \allowbreak (x,y),\allowbreak  (y,z), (z,s),  (s,v), (z,w), (y,t), (x, u)\})$.
I hele bogen gælder $n=|V|$ og $m=|E|$, medmindre $n$ og $m$ er tildelt en anden betydning.
En kant $e=(u,v)\in E$ repræsenterer en forbindelse fra $u$ til $v$.
Knuderne $u$ og $v$ kaldes for $e$s endeknuder eller endepunkter, $u$ kaldes $e$s \emph{startknude} og $v$ for $e$s \emph{målknude}.
\index{startknude|textbf}
\index{endepunkt|textbf}
\index{kant!endepunkt|textbf}
\index{målknude|textbf}
Vi siger at $e$ er \emph{incident med} eller \emph{hosliggende til} $u$ og $v$, at $u$ og $v$ \emph{ligger på} $e$ og at $u$ og $v$ er \emph{naboer}.
\index{incidens|textbf}
\index{kant!hosliggende|textbf}
\index{nabo|textbf}
Envidere er $e$ \emph{udgående fra} $v$ og \emph{indkommende til} $u$.
Sommetider siger vi at $e$ er \emph{udkant} til $v$ hhv. \emph{indkant} til $u$.
\index{udgående kant|textbf}
\index{udkant|textbf}
\index{indkommende kant|textbf}
\index{indkant|textbf}
\index{kant!indkommende|textbf}
\index{kant!udgående|textbf}
Ofte kalder vi også $u$ for en (umiddelbar) \emph{forgænger} til $v$ og $v$ for en (umiddelbar) \emph{efterfølger} for $u$.
\index{forgænger|textbf}
\index{efterfølger|textbf}
Vi udelukker for det meste specialtilfældet hvor $(v,v)$ er en \emph{løkke}, medmindre andet er sagt.

En knudes \emph{udgrad} $v$ er antallet af kanter, som udgår fra $v$, dens \emph{indgrad} er antallet af kanter, der indgår til $v$. 
\index{udgrad|textbf}
\index{indgrad|textbf}
Formelt har vi 
$\Id{udgrad}(v)=|\{\,u\in V\colon (v,u)\in E\,\}|$ og
$\Id{indgrad}(v)=|\{\,u\in V\colon (u,v)\in E\,\}|$.
Fx har knuden $w$ i grafen $G$ i fig.~2.7 indgrad $2$ og udgrad $1$.

En 
\emph{dobbeltrettet graf}
\index{dobbeltrettet graf|textbf}
\index{graf!dobbeltrettet|textbf}
er en rettet graf, som for hver rettede kant $(u,v)$ indeholder modkanten $(v,u)$.
En 
\emph{urettet graf}
\index{urettet graf|textbf}
\index{graf!urettet|textbf}
kan man opfatte som en strømlinet udgave af en dobbeltrettet graf, hvor vi skriver kantparret $(u,v)$, $(v,u)$ som parmængden $\{u,v\}$.
Figur~\lref{fig:graphexample} viser en urettet graf med tre kunder og dens dobbeltrettede modpart.
De fleste grafteoretiske begreber defineres for urettede grafer præcis som for rettede grafer; derfor koncentrerer vi os i dette afsnit om rettede grafer og nævner urettede grafer kun, når der findes en afvigelse.
Fx har en urettet graf kun halvt så mange kanter som sin dobbeltrettede modpart.
Når $\{u,v\}$ er en kant, kalder vi $u$ for $v$s \emph{nabo}; i den dobbeltrettede udgave er $u$ både forgænger og efterfølger til $v$.
\index{nabo}
En knudes udgrad og indgrad i en urettet graf er altid ens, så den kaldes bare knudens \emph{grad} (eller \emph{valens}).
\index{grad}
\index{valens|siehe{grad}}
Urette grafer er vigtige, fordi retninger ofte ikke spiller nogen rolle, og mange problemer i urettede grafer kan løses enklere end i almene rettede grafer; visse problemer giver overhovedet kun mening i urettede grafer.

En graf $G'= (V', E')$ er en 
\emph{delgraf}
\index{delgraf|textbf}
af grafen $G=(V,E)$, hvis $V'\subseteq V$ og $E'\subseteq E$.
Givet $G=(V,E)$ og en knudedelmængde $V'\subseteq V$ defineres den 
\emph{inducerede delgraf}
\index{induceret delgraf|textbf}
\index{delgraf|induceret|textbf}
$G'$ 
som $G' = (V', E\cap (V'\times V'))$.
I fig.~2.7 inducerer knudemængden $\{u,v\}$ i $G$ delgrafen $H = (\{v,w\}, \{(v,w)\})$.
Grafen $G'$ kaldes \emph{knudeinduceret}.
%TODO: def missing in original
En kantdelmængde $E'\subseteq E$ inducerer delgrafen $(V,E')$.

Ofte knyttes yderligere information til knuder og kanter.
Især har vi ofte behov for 
\emph{kantvægte}
\index{kantvægt|textbf}
\index{vægt!kant-|textbf}
eller \emph{kantomkostninger} $c\colon E\rightarrow \RR$, som tildeler en talværdi til hver kant.
Fx har kanten $(z,w)$ i grafen $G$ i fig.~2.7 vægten $c(\{z,w\}) = 2$.
%TODO error in original
Læg mærke til, at en kant $\{u,v\}$ i en urettet graf kun kan have én vægt, mens der i en dobbeltrettet graf kan gælde $c((u,v)) \neq c((v,u))$.

De sidste par afsnit indeholdt mange tørre definitioner på i stram form.
Hvis man efterlyser definitionerne »i aktion«, kan finde algoritmer på grafer i kapitel~\ref{ch:grepresent:}.
Men selv her skal materialet straks blive lidt mere interessant.

Et vigtigt højere begreb i grafteorien er \emph{vejen}.
\index{vej|textbf}
\index{sti|siehe{vej}}
En \emph{vej} (eller \emph{sti}) $p=\langle v_0,\ldots, v_k\rangle$ i en rettet graf $G=(V,E)$ er en følge af knuder, således at på hinanden følgende knuder er forbundet med kanter i $E$, dvs. at der gælder 
$(v_0,v_1)\in E$, 
$(v_1,v_2)\in E$, $\cdots$,
$(v_{k-1},v_k)\in E$.
Tallet $k\geq 0$ kaldes for 
\emph{længden}
\index{længde!vej-|textbf}
af $p$; vi siger at vejen \emph{går} (eller \emph{leder}) fra $v_0$ til $v_k$.
Sommetider angiver vi vejen som en følge af dens kanter.
Fx er $\langle u,v,w\rangle= \langle (u,v), (v,w)\rangle$ i fig.~2.7 en vej af længde $2$.
I en urettet graf er $p=\langle v_0,\ldots, v_k\rangle$ en vej, hvis $p$ er en vej i den tilsvarende dobbeltrettede graf.
I grafen $U$ i fig.~2.7 er $\langle u,w,v,u, w,v\rangle$ en vej af længde $5$.
En vej er simpel%
\index{vej!simpel|textbf}
\footnote{I mange fremstillinger bruges »vej« for »simpel vej« og »vandring« for »vej«.},
hvis dens knuder bortset måske fra $v_0$ og $v_k$ er parvist forskellige.
I grafen $G$ i fig.~2.7 er $\langle z,w,x,u,v,w,x,y\rangle$ en vej, som ikke er simpel.
Det ses nemt, at hvis der findes en vej fra $u$ til $v$, så findes der også en simpel vej fra $u$ til $v$.

En \emph{kreds}
\index{kreds|textbf}
\index{kreds!hamiltonsk}
\index{Hamilton, W.\,R.}
\index{cykel|siehe{kreds}}
(eller \emph{cykel}) er en vej af længde mindst $1$, hvis første og sidste knude er den samme.
I urettede grafer kræves yderligere, at den første og sidste kant er forskellige, hvorved en kreds har længde mindst $3$.
(I fig.~2.7 indeholder $G$ kredsen $\langle u,v,w,x,y,z,w,x,u\rangle$ og $U$ kredsen $\langle u, w, v, u, w, v, u\rangle$.)
En \emph{simpel} kreds er en kreds, hvis interne knuder er forskellige.
\index{kreds!simpel|textbf}
Det ses let, at hvis $G$ har en kreds, så har den også en simpel kreds.
En simpel kreds, som besøger alle knuder i grafen, kaldes en hamiltonkreds.
(I fig.~2.7 har $G$ hamiltonkredsen $\langle s,t,u,v,w,x,y,z,s\rangle$ og $U$ hamiltonkredsen $\langle w,u,v,w\rangle$.)

Begreberne »vej« og »kreds« tillader os at definere endnu mere komplekse begreber.
En rettet graf er 
\emph{stærkt sammenhængende},
\index{graf!stærkt sammenhængende|textbf}
hvis der fra hver knude $u$ og til hver knude $v$ findes en vej.
Grafen $G$ i fig.~2.7 er stærkt sammenhængende.
En \emph{stærk sammenhængskomponent} 
\index{stærk sammenhængskomponent|textbf}
\index{graf!stærk sammenhængskomponent|textbf}
er en maksimal, knudeinduceret, stærkt sammenhængende delgraf af $G$.
Hvis vi i fig.~2.7 fjerner kanten $(w,x)$ i $G$, opstår en graf uden kredse.
En graf uden kredse hedder \emph{kredsfri} eller \emph{acyklisk}.
\index{acyklisk|textbf}
\index{graf!acyklisk|textbf}
En rettet, acyklisk graf betegnes kort \emph{rag}.
\index{rag|siehe{graf, rettet, acyklisk}}
\index{graf!rettet, acyklisk|textbf}
Hver stærke sammenhængskomponent i en rag består af præcis en knude.
En urettet graf kaldes 
\emph{sammenhængende},
\index{graf!sammenhængende|textbf},
hvis der for hvert par $u$, $v$ af knuder findes en (urettet) vej fra $u$ til $v$.
Sammenhængskomponenterne er de maksimale, sammenhængdende delgrafer.
Inden for en sammenhængskomponent er hvert par af knuder forbundet af en vej, mens der ikke fører nogen vej fra nogen sammenhængskomponent til en anden.
I fig.~2.7 har graphen $G$ sammenhængskomponenterne $\{u,v,w\}$, $\{s,t\}$ og $\{x\}$.
Knudemængden $\{u,w\}$ inducerer en sammenhængende delgraf, men denne er ikke maksimal og udgør derfor ingen sammenhængskomponent.

\begin{exerc}
Beskriv mindst ti grundlæggende forskellige anvendelser, som kan modelleres med grafer.
(Bil- og cykelveje gælder \emph{ikke} som grundlæggende forskellige!)
Mindst fem anvendelse bør ikke forekommme i bogen.
\end{exerc}

\begin{exerc}
En graf er \emph{planær},
  \index{graf!planær|textbf}
  hvis den kan tegnes på et stykke papir på sådan måde, at ingen kanter krydser hinanden.
Forklar, hvorfor vejnet ikke nødvendigvis er planære.
  \index{graf!vejnet}
Vis at $K_5$ og $K_{3,3}$ fra fig.~2.7 ikke er planære.
\end{exerc}

\subsection{En første grafalgoritme}

\index{algoritmekonstruktion!grådig!fravær af kredse}

Det er tid til en eksempelalgoritme.
Vi skal betragte en algoritme, som afgør, om en rettet graf $G$ er acyklisk eller ej.
\index{kredsfrihed|textbf}
Udgangspunktet er den enkle observation, at en knude $v$ med udgrad $0$ ikke kan ligge på en kreds.
Hvis vi altså stryger $v$ (og dets indgående kanter), opnår vi en graf $G'$, som er acyklisk, hvis og kun hvis $G$ er acyklisk.
Denne transformation gentages nu, indtil en af to siutationer opstår:
Enten ender vi i den tomme graf, dvs. grafen uden knuder, som jo sikkert er acyklisk.
Eller vi ender i en graf $G^*$, hvor hver knude har udgrad mindst $1$.
I det sidste tilfælde er det let at finde en kreds:
Begynd med en vilkårlig knude.
Fra hver knude, som vi netop har nået, fortsætter vi ad en vilkårlig kant til den næste knude, indtil vi møder en knude $v'$, som vi har mødt før.
Den herved konstruerede vej har formen $\langle v, \ldots, v',\ldots, v'\rangle$, hvis suffiks $\langle v',\ldots,v'\rangle$ danner en kreds.
For eksempel har grafen $G$ i fig.~2.7 ingen knude med udgrad $0$.
For at finde en kreds, kunne vi begynde i knude $z$ og følge turen $\langle z,w,x,u,v,w\rangle$, vil vi støder på knude $w$ for anden gang.
Herved har vi fundet kredsen $\langle w,x,u,v,w\rangle$.
Anderledes forløber det, når vi betragter grafen $G$ uden kanten $(w,x)$.
Da findes ingen kreds, og algoritmen stryger samtlige knuder i rækkefølgen $w$, $v$, $u$ , $t$, $s$, $z$, $y$, $x$.
I kapitel~\ref{ch:grepresent:} skal vi se, hvordan man kan repræsentere grafen på en måde, så den foroven skitserede algoritme kører i lineær tid, dvs. i tid $O(|V|+|E|)$.
(Se hertil opgave~\ref{ch:grepresent:ex:dagtest}.)
Algoritmen kan let udvides til at være certificerende:
\index{algoritmekonstruktion!certifikat}
Når algoritmen finder en kreds, er grafen sikkert cyklisk, så det er let at afgøre, om den fundne knudefølge udgør en kreds i $G$.
Når algoritmen fjerner samtlige knuder i $G$, nummererer vi knuderne i den rækkefølge, de blev strøget.
Når vi fjerner en knude $v$ med udgrad $0$, må alle kanter i $G$, som har $v$ som startknude, gå til knuder, som blev fjernet tidligere, og som derfor har et lavere nummer end $v$. 
Nummereringne beviser altså, at $G$ er acyklisk:
Langs hver kant er knudenummeret faldende, hvilket igen er nemt at certificere.

\begin{exerc}
For vilkårligt $n$, find en rag med $\frac{1}{2}n(n-1)$ kanter.
\index{graf!rettet!acyklisk} 
Vis, at der ikke findes en rag med flere end $\frac{1}{2}n(n-1)$ kanter.
\end{exerc}

\subsection{Træer}

En urettet graf er et \emph{træ},
\index{træ|textbf}
hvis der mellem hvert par af knuder findes netop én vej; fx som i fig.~2.8.
En urettet graf er en \emph{skov},
\index{skov|textbf}
hvis der mellem hvert par af knuder findes højst én vej.
Det ses let, at hver sammenhængskomponent i en skov er et træ.

\begin{figure}
\tikzset{
	vertex/.style = {circle, fill, inner sep = 1.5pt},
	weight/.style = {font=\small, midway, auto, inner sep = 1pt, circle},
	>=stealth'}
\begin{tabular}{ccccc}
\begin{tikzpicture}[scale=0.7, baseline=(s)]
\node at (2.5,1.5) [red] {\emph{urettet}};
\node (u) at (.33,-2) [vertex, label = below :$u$] {};
\node (v) at (1.66,-2.33) [vertex, label = below :$v$] {};
\node (s) at (0,0) [vertex, label = above :$s$] {};
\node (t) at (2,0) [vertex, label = above :$t$] {};
\node (r) at (1,-1) [vertex, label = below right:$r$] {};
\draw (u)--(v);
\draw (u)--(r);
\draw (r)--(s);
\draw (r)--(t);
%
\begin{scope}[xshift = 4cm]
\node at (0,1) [red] {\emph{rodfæstet}};
\node (r) at (0,0)   [vertex, label = above:$r$] {};
\node (ss) at (-1,-1) [vertex, label = below :$s$] {};
\node (t) at (0,-1)  [vertex, label = below:$t$] {};
\node (u) at (1,-1)  [vertex, label = above:$u$] {};
\node (v) at (1,-2)  [vertex, label = below:$v$] {};
\draw (u)--(v);
\draw (r)--(u);
\draw (r)--(ss);
\draw (r)--(t);
\end{scope}
\end{tikzpicture}
&
\begin{tikzpicture}[scale=0.7, baseline=(r)]
\node at (1.5,1.5) [red] {\emph{rettet}};
\node at (0,1) [red] {\emph{udtræ}};
\node (r) at (0,0)   [vertex, label = above:$r$] {};
\node (s) at (-1,-1) [vertex, label = below :$s$] {};
\node (t) at (0,-1)  [vertex, label = below:$t$] {};
\node (u) at (1,-1)  [vertex, label = above:$u$] {};
\node (v) at (1,-2)  [vertex, label = below:$v$] {};
\draw [->] (u)--(v);
\draw [->] (r)--(u);
\draw [->] (r)--(s);
\draw [->] (r)--(t);
\begin{scope}[xshift = 3cm]
\node at (0,1) [red] {\emph{indtræ}};
\node (r) at (0,0)   [vertex, label = above:$r$] {};
\node (s) at (-1,-1) [vertex, label = below :$s$] {};
\node (t) at (0,-1)  [vertex, label = below:$t$] {};
\node (u) at (1,-1)  [vertex, label = above:$u$] {};
\node (v) at (1,-2)  [vertex, label = below:$v$] {};
\draw [<-] (u)--(v);
\draw [<-] (r)--(u);
\draw [<-] (r)--(s);
\draw [<-] (r)--(t);
\end{scope}
\end{tikzpicture}
&
\begin{tikzpicture}[scale=0.7, , baseline=(add), every node/.style={draw, circle, inner sep = 1pt}]
\node at (0,1.5) [draw = none, rectangle, red] {\emph{ordnet}};
\node at (0,1) [draw = none, rectangle, red] {\emph{udtrykstræ}};
\node (add) at (0,0)    {$+$};
\node (a) at (-.66,-1)  {$a$};
\node (div) at (.66,-1)  {$/$};
\node (2) at (.16,-2)   {$2$};
\node (b) at (1.16,-2)   {$b$};
\draw [->] (add)--(a);
\draw [->] (add)--(div);
\draw [->] (div)--(2);
\draw [->] (div)--(b);
\end{tikzpicture}
\end{tabular}
\caption{Forskellige slags træer.
Fra venstre til højre ses et urettet træ, et urettet, rodfæstet træ, et træ rettet væk fra og et rettet mod roden og et aritmetisk udtryk.
}
\end{figure}

\begin{lem}
  For en urettet graf $G$ er følgende ækvivalent:
  \begin{enumerate}[(1)]
    \item $G$ er et træ
    \item $G$ er sammenhængende og har præcis $n-1$ kanter.
    \item $G$ er sammenhængende og acyklisk
  \end{enumerate}

\end{lem}
\begin{proof}
I et træ findes mellem hvert par af knuder én entydig bestemt vej.
Derfor er træet sammenhængende og indeholder ingen kreds.
Omvendt gælder:
Hvis $G$ indeholder to knuder, som er forbundet med forskellige veje, så indeholder $G$ en kreds.
(Betragt hertil $u$ og $v$ og to forskellige veje fra $u$ til $v$ sådan at summen af længderne af disse veje er minimal over alle valg af knudepar og veje.
Ved minimalitet indses, at vejene må forlade $u$ ad to forskellige kanter, og kan først mødes igen i $v$.
Derfor danner vejene en enkel kreds.)
Hermed har vi vist ækvivalens af (1) og (3).
Vi viser nu, at (2) og (3) også er ækvivalente.
Hertil betrager vi en sammenhængde graf $G=(V,E)$ med $m=|E|$.
Vi gennemfører nu følgende tankeeksperiment:
Vi begynder med den tomme graf
  %(TODO: empty graph means several things meant something else)
med knudemængde $V$ og ingen kanter og tilføjer en kant fra $E$ efter den anden.
Tilføjelsen af hver kant reducerer antallet af sammenhængskomponenter med højst $1$.
Idet vi begyndte med $n=|V|$ komponenter, og ender med en eneste komponent, må antallet af kanter være mindst $n-1$.
Antag nu, at tilføjelsen af kanten $e=\{u,v\}$ ikke formindsker antallet af komponenter med $1$.
Det må betyde, at $u$ og $v$ allerede er forbundet med en vej, og tilføjelsen af $e$ vil derfor danne en kreds.
Hvis $G$ er acyklisk, kan dette altså ikke ske, og vi har $m=n-1$.
Derfor følger (2) af (3).
Antag nu, at $G$ er sammenhængende og har præcis $n-1$ kanter.
Igen tilføjer vi disse kanter til en oprindeligt tom graf.
Når kanten $e=\{u,v\}$ lukker en kreds, er $u$ og $v$ allerede forbundet, og tilføjelsen af $e$ reducerer ikke antallet af komponenter.
Vi skulle altså ende med mere end én komponent, hvilket er en modstrid.
Derfor følger (3) af (2).
\end{proof}

Lemma 2.8 gælder ikke for rettede grafer.
\index{graf!rettet!acyklisk} 
Især kan en rag have meget mere end $n-1$ kanter.
En rettet graf kaldes \emph{udtræ}
\index{udtræ}
\index{træ!ud-}
med rod $r$,
\index{rod}
\index{træ!rod|textbf}
hvis der fra $r$ findes netop én vej til hver knude.
Den kaldes \emph{indtræ},
\index{indtræ}
\index{træ!ind-}
hvis der findes netop én vej fra hver knude til $r$.
Figur~2.8 viser nogle eksempler.
\emph{Dybden}
\index{knude!dybde i træ|textbf}
af en knude $v$ i et træ med rod er længden af vejen mellem $v$ og roden.
\emph{Højden}
\index{træ!højde|textbf}
(også kaldt \emph{dybden}) af et træ med rod er den maksimale dybde av nogen knude.

Man kan rodfæste et urettet træ ved at erklære en vilkårlig knude til rod.
Dataloger har den spøjse vane, at tegne træer så roden er foroven  og alle kanter forløber oppefra og ned.
For rodfæstede træer er det almindeligt, at betegne forholdet mellem knuder med ord, som er taget fra familjerelationer.
Kanter forløber mellem en 
\emph{forælder} og dets 
\emph{børn}.
Knuder med samme forælder hedder 
\emph{søskende}.
En knude uden børn kaldes et 
\emph{blad}.
En knude, om ikke er et blad, kaldes en 
\emph{indre} knude.
Når $u$ ligger på vejen fra roden til en knude $v$, og $u$ er forskellig fra $v$, kaldes $u$ en \emph{ane}
til $v$, tilsvarende kaldes $v$ en 
\emph{efterkommer}
til $u$.
En knude $u$ og alle dens efterkommere danner et 
\emph{undertræ}
med rod $u$.
Vi betragter et eksempel:
I de rettede træer i fig.~2.8 er $r$ roden; $s$, $t$ og $v$ er blade; $s$, $t$ og $u$ er søskende, fordi de er børn til samme forælder $r$; $u$ er en indre knude; $r$ og $u$ er ane til $v$; $s$, $t$, $u$ og $v$ er efterkommmerne til $r$; $v$ og $u$ danner et deltræ med rod $u$.

\subsection{Ordnede træer}

Træer egner sig til at repræsentere hierarkier.
Betragt fx det aritmetiske udtryk $a+2/b$.
Vi ved godt, at dette skal betyde, at $a$ og $2/b$ skal lægges sammen.
Men det er slet ikke så let at udlæse fra tegnfølgen $\langle a,+,2, b\rangle$.
Fx skal man iagttage, at division udføres før addition.
Oversætteren isolerer denne syntaktiske viden i en \emph{parser}, som ud fra formelteksten skaber en struktureret, træbaseret repræsentation.
Vores eksempeludtryk ville føre til træet til højre i fig.~2.8.
Disse træer er rettede og -- i modsætning til grafteoretiske træer -- desuden \emph{ordnede}, dvs. at rækkefølgen af hver knudes børn ligger fast.
I vores eksempel er $a$-knuden det første barn af roden, $/$-kunden er det andet barn.

\begin{figure}
  \newcommand{\eval}{\operatorname{\emph{eval}}}
  \begin{tabbing}
    ~~~~\=~~~~\=\kill
    $\Function \eval(r):\RR$\\
    \> $\iif r \text{ er et blad } \tthen \return \text{ det i $r$ gemte tal}$\\
    \> $\eelse$\\
    \>\> $v_1:= \eval(\text{første barn til $r$})$\\
    \>\> $v_2:= \eval(\text{andet barn til $r$})$\\
    \>\> $\return v_1 \text{ \emph{operator$(r)$} } v_2$
  \end{tabbing}
  \caption{Rekursiv evaluering af et utrykstræ med rod $r$.}
\end{figure}

Udtrykstræer kan enkelt evalueres med en rekursiv algoritme.
Fig.~2.9 angiver en algoritme for evalueringen af udtrykstræer, hvis blade indeholder tal, og hvis indre knuder indeholder operationer som $+$, $-$, $\cdot$ eller $/$.

I denne bog vil vi møde mange andre eksempler på ordnede træer.
I kapitel 6 og 7 bruges de til at illustrere grundlæggende datastrukturer, i kapitel 12 til den systematiske gennemgang af løsningsrum.
\section{P og NP}
\llabel{s:P and NP}

\index{P@$\classP$|textbf}%
\index{NP@$\NP$|textbf}%
\index{effektivitet|siehe{udførelsestid}}%

Hvornår kalder vi en  algoritme for »effektiv«?
Findes der problemer, for hvilke der ikke findes effektive algoritmer?
Selvfølgeligt er det lidt af en smagssag, hvor man præcis vil trække grænsen mellem »effektive« og »ueffektive« algoritmer.
Men det har vist sig, at følgende adskillelse er nyttig:
En algoritme $\mathcal{A}$ bruger \emph{polynomiel tid} eller er en  \emph{polynomialtidsalgoritme},
\index{udførelsestid!polynomiel|textbf}
\index{algoritme!polynomialtids-|textbf}
hvis der findes et polynomium $p(n)$, således at udførelsestiden  af $\mathcal{A}$ på instanser af størrelse $n$ tilhører $O(p(n))$. 
Medmindre andet er sagt, vil vi måle instansstørrelsen i antal bit.
Vi siger at et problem \emph{kan løses i polynomiel tid}, hvis der findes en polynomialtidsalgoritme for det.
Nu sætter vi lighedstegn mellem »kan løses effektivt« og »kan løses i polynomiel tid«. 
En af fordelene ved denne definition er, at implementationsdetaljer normalt ikke spiller nogen rolle.
For eksempel er det underordnet, om en snedig datastruktur er i stand til at reducere udførelsestiden af en $O(n^3)$-algoritme med en faktor $n$.
Alle kapitler i denne bog, med undtagelse af kapitel~\ref{ch:optimization:}, drejer sig om effektive algoritmer.
  
Der findes mange problemer, for hvilke der findes algoritmer, uden at man kender en \emph{effektiv} algoritmer for dem.
Vi vil nævne seks eksempler:
\begin{itemize}
  \item Hamiltonkredsproblemet:
    Givet en urettet graf.
    Afgør, om den indeholder en hamiltonkreds.
    \index{kreds!hamilton-}
  \item Opfyldningsproblemet for boolske formler:
    \index{tildelingsproblem|textbf} 
    Givet en boolsk formel på \emph{konjunktiv normalform}. 
    Afgør, om der findes en opfyldende tildeling. --
    Her er en boolsk formel på konjunktiv normalform en konjunktion 
    $C_1 \wedge \cdots \wedge C_k$ af \emph{klausuler}. 
    En klausul er en disjunktion $(\ell_1 \vee \cdots \vee \ell_h)$ af \emph{literaler},
    og en literal er en boolsk variabel $x_i$ eller en negeret boolsk variabel $\neg x_i$.  
    Et eksempel på en klausul er $(x_1 \vee \neg x_3 \vee \neg x_9)$. 
    En tildeling knytter en boolsk værdi til hver variabel, den er \emph{opfyldende}, hvis den gør formlen sand.
  \item Klikeproblemet:
    \index{klike|sieheunter{graf}}
    \index{graf!klike|textbf}
    \index{graf!fuldstændig}
    Givet en urettet graf og et naturligt tal $k$.
    Afgør, om grafen indeholder en fuldstændig delgraf (også kaldt en \emph{klike}) med $k$ knuder.
    -- En graf kaldes \emph{fuldstændig}, hvis hvert par af knuder er forbundet med en kant;
    et eksempel er »5-kliken« $K_5$ i figur.~\lref{fig:graphexample}.
  \item Rygsæksproblemet:
    \index{rygsæk|textbf} 
    Givet $n$ par $(w_i,p_i)$  af naturliget tal for $i\in \{1,\ldots, n\}$ og to naturlige tal $M$ og $P$. 
    Afgør, om der findes en delmængde $I \subseteq \{1,\ldots, n\}$ af indeks, som opfylder 
    $\sum_{i \in I} w_i \le M$ og $\sum_{i \in I} p_i \ge P$.
    -- Man skal tænke på dette som $n$ givne objekter, hvor objekt $i$ har volumen $w_i$ og profit $p_i$, og som man prøver at pakke i en rygsæk med volumen $M$ for at opnå en samlet profit på mindst $P$.
  \item Handelsrejsendeproblemet:
    \index{handelsrejsendeproblemet|textbf}
    \index{tsp|siehe{handelsrejsendeproblemet}}
    Givet en kantvægtet urettet graf og det naturligt tal $C$.
    Afgør, om grafen indeholder en hamiltonkreds af længde højst $C$.
    (Se nærmere i afsnit~\ref{ch:mst:ss:tsp}.)
  \item Graffarvningsproblemet:
    \index{graf!farvning|textbf} 
    Givet en urettet graf og et naturligt tal $k>0$.
    Afgør, om der findes en farving af knuderne i grafen med $k$ farver, således hvert par af naboknuder har forskellige farver.
\end{itemize}
Bare fordi vi ikke kender nogle effektive algoritmer for noget af disse seks problemer, kan vi selvfølgeligt ikke udelukke, at den slags algoritmer alligevel findes.
Man ved det helt enkelt ikke.
Især har vi ingen matematiske beviser for, at der for disse problemer ikke kan findes en effektiv algoritme.
Generelt er det meget vanskeligt at bevise, at der for et givet problem ikke kan findes en algoritme inden for en bestemt tidsgrænse.
(Enkle eksempler på den slags nedre grænser skal vi dog lære at kende i kapitel~\ref{ch:sort:s:lower}.)
De fleste algoritmikere tror dog ikke, at der findes effektive algoritmer for de seks problemer.

Teorien for beregningskompleksitet,
\index{beregningskompleksitet}
\index{kompleksitetsteori}
bland dataloger ofte bare kaldt »kompleksitetsteorien«, giver en interessant måde at argumentere for disse problemers sværhed.
Teorien sammenfatter algoritmiske problemer i store grupper, kaldt »kompleksitetsklasser«, således at problemer i samme klasse er ækvivalente med hensyn til deres kompleksitet.
Især findes der en stor gruppe af ækvivalente problemer, som kaldes
\emph{$\NP$-fuldstændige}.
\index{NP-fuldstændig@$\NP$-fuldstændig|textbf} 
Her er »$\NP$« en forkortelse for »nondeterministisk polynomiel tid«. 
Det er unødvendigt for forståelsen af resten af afsnittet, om læseren kender begrebet »nondeterministisk polynomiel tid« eller ej. 
De seks problemer foroven er allesammen $\NP$-fuldstændige, ligesom mange andre naturlige problemer.
I resten af asnittet vil vi definere klassen $\NP$ og klassen af $\NP$-fuldstændige problemer formelt.
For en grundig fremstilling af teorien henviser vi til lærebøger i beregningsteori og kompleksitetsteori~\cite{Ausiello-et-al:book,Garey-Johnson:book,Sipser,Wegener:ComplexityTheory}.
\index{Garey, M. R.}
\index{Johnson, D. S.}
\index{Ausiello, G.}
\index{Sipser, M.}
\index{Wegener, I.}

Som sædvandligt i kompleksitetsteorien går vi ud fra, at instansen til problemet er kodet som streng over et fast, endeligt alfabet $\Sigma$ med $|\Sigma|\ge2$.
(Man kan tænke sig ASCII- eller Unicode-alfabeterne, eller binær\-repræsentationen af disse alfabeter. 
I det sidste tilfælde er $\Sigma=\{0,1\}$.)
Mængen af strenge (eller »ord«) med bogstaver fra $\Sigma$ betegnes $\Sigma^*$.
For $x=a_1\cdots a_n \in\Sigma^*$ bruger vi antallet $|x|$ af tegn i $x$ som størrelsesmål, med konventionen $n=|x|$.\footnote{
Undtagelsen er grafer.
I denne bog, ligesom i resten af literaturen, bruges $n$ for at betegne antallet af \emph{knuder} i grafen, se afsnit~\ref{s:graphnot}.
Størrelsen af en graf med $n$ knuder og $m$ kanter er derimod $\Theta(n+m)$ eller $\Theta(n^2)$, afhængigt af den valgte repræsentation (se afsnit~\ref{ch:grepresent}).
Graferne i denne bog er næsten udelukkende »simple« (der gælder $E\subseteq V\times V$, dvs. vores grafer har ingen multikanter i samme retning), så vi har $m\leq n^2$. 
Derfor er  grafers størrelse polynomiel i $n$, og den uheldige sammenblanding af $n$ og $n+m$ gør ingen forskel med hensyn til polynomialtidsreduktioner.}
%TODO add footnote to original
Et \emph{beslutningsproblem}
\index{beslutningsproblem} 
er en delmængde $L \subseteq \Sigma^*$.
Med $\chi_L\colon \Sigma^*\to\{0,1\}$ (læs: »chi«, udtalt med k) betegner vi den  \emph{karakteristiske funktion}
\index{karakteristisk funktion} af $L$, definieret som 
\begin{equation*}
  \chi_L(x)= 
  \begin{cases}1\,, &\text{hvis } x \in L\,,\\
    0\,,& \text{hvis } x \not\in L\,.    
  \end{cases}
\end{equation*}
Et beslutningsproblem kan \emph{løses i polynomiel tid}, hvis dens karakteristiske funktion kan beregnes i polynomiel tid.
Med $\classP$ betegnes klassen af beslutningsproblemer, som kan løses i polynomiel tid.
Et beslutningsproblemer $L$ tilhører klassen $\NP$, hvis der findes et prædikat $Q(x,y)$,
(dvs. en mængde $Q\subseteq(\Sigma^*)^2$) og en polynomium $p$, således at der gælder følgende:  
\begin{enumerate}[(1)]
\item For hvert $x \in \Sigma^*$ gælder $x \in L$, hvis og kun hvis der eksisterer $y \in \Sigma^*$ med $\abs{y} \le p(\abs{x})$ og således at $Q(x,y)$ gælder;
\item den karakteristike funktion for $Q$ kan beregnes i polynomiel tid. 
\end{enumerate}
Hvis  $x\in L$ og $y$ opfylder $Q(x,y)=\mathit{sand}$, kalder vi $y$ for et \emph{vidne} eller \emph{certifikat} for $x$ ($y$ bevidner altså den omstændighed, at $x\in L$).
For vores eksempelproblemer kan man let vise, at de tilhører $\NP$.
For hamiltonkredsproblemet er vidnet en hamiltonkreds i instansgrafen.
Et vidne for en boolsk funktion er en tildeling af de variable, som gør formlen sand.
At en instans rygsæksproblemet kan løses, bevidnes af en delmænge af objekterne, hvis samlede volumen får plads i rygsækken og som samlet udgør mindst den ønskede profit.

\begin{exerc}
  Vis, at klikeproblemet \index{graf!klike}, handelsrejsendeproblemet \index{handelsrejsendeproblem} og graffarvningsproblemet tilhører $\NP$.
\end{exerc}

Man formoder, at $\classP$ er en ægte delmængde af $\NP$.
Selvom der findes gode argumenter for denne formodning, som vi straks skal se, er den ikke bevist.
Et bevis for formodningen ville medføre, at der ikke findes effektive algoritmer for $\NP$-fuldstændige problemer. 

Beslutningsproblemet $L$  kan \emph{reduceres i polynomiel tid}
 (eller bare \emph{reduceres})
\index{reduktion|textbf}
\index{reducibel|textbf} 
til beslutningsproblemet $L'$, 
hvis der findes en funtion $g$, som kan beregnes i polynomiel tid,
og for hvilken der gælder for alle $x \in \Sigma^*$, at $x \in L$ hvis og kun hvis $g(x) \in L'$.
Hvis $L$ kan reduceres til $L'$, og $L'$ tilhører $\classP$, indses nemt at $L$ også tilhører $\classP$.
(Bevis: Givet en algoritme for reduktionensfunktionen $g$ med polynomiel tidsgrænse $p(n)$ og en algoritme for $\chi_{L'}$ med polynomiel tidsgrænse $q(n)$.
Så har vi følgende algoritme for  $\chi_L$. 
På input $x$ beregner vi $g(x)$ i tid højst $ p(|x|)$ med den første algoritme, hvorefter vi bruger den anden algoritme for at afgøre om $g(x)\in L'$.
% TODO Turing machines suddenly appear in original
Afgørelsen tager tid $q(|g(x)|)$.
For at begrænse længden af strengen $g(x)$ er det nok an indse, at den første algoritme i tid $p(|x|)$ højst kan skabe en streng af længde $p(|x|)$; strengt taget er dette et udsagn om maskinmodellen, som behøver mindst en tidsenhed for at skabe et tegn i den nye streng og fx gemme den i en lagercelle for sig -- detaljer for forskellige repræsentationer af bl.a. tegnfølger findes i kapitel~\ref{s: sequence}.
Vi har altså, at $|g(x)| \le |x| + p(|x|)$, så den samlede udførelsestid er $p(|x|) + q(|x|+p(|x|))$,
hvilket er et polynomium i $|x|$.)
På lignende måde kan man indse, at reducibilitet er en transitiv relation.
Beslutningsproblemetet $L$  kaldes \emph{$\NP$-svært},
\index{NP-svxr@$\NP$-svær|textbf} 
hvis \emph{hvert} problem i $\NP$ kann reduceres til $L$ i polyniel tid.
Et problem kaldes \emph{$\NP$-fuldstændigt}, hvis det er  $\NP$-svært  og tilhører $\NP$.
Umiddelbart virker det meget vanskeligt at bevise, at der overhovedet findes $\NP$-fuldstændige problemer -- blandt andet skal man jo vise, at \emph{hvert} problem i $\NP$ kan reduceres til $L$. 
Men netop det gjordes i år 1971, da Cook og Levin viste (uafhænigt af hinanden), at opfyldningsproblemet for boolske formler er $\NP$-fuldstændigt~\cite{Cook71,Lev73}.
Siden har det været »let« at etablere $\NP$-fuldstændighed.
Antag, at vi vil vise, at $L$ er $\NP$-fuldstændigt.
Dertil skal vi vise to ting:
(1) $L \in \NP$, og (2) der findes et andet problem $L'$, hvis $\NP$-fuldstændighed allerede er etableret, og som kan reduceres til $L$.
Hvert nye $\NP$-fuldstændige problem gør det lettere, at vise andre problemers $\NP$-fuldstændighed. 
% TODO removed reference to Kann's survey
Vi ser nu nærmere på et eksempel på en reduktion.

\begin{lemma} 
  Opfyldningsproblemet for boolske formler kan reduceres i polynomiel tid til klikeproblemet.
\end{lemma}

\begin{proof} 
  Lad $F = C_1 \wedge \cdots \wedge C_k$ være en boolsk formel på konjunktiv normalform, med 
  $C_i = (\ell_{i1} \vee \ldots \vee \ell_{ih_i})$ og $\ell_{ij} = x_{ij}^{\beta_{ij}}$ for $i\in\{1,\ldots, n\}$ for $j\in\{1,\ldots,r\}$. 
  Her er $x_{ij}$ en boolsk variabel, og $\beta_{ij} \in \{0,1\}$.
  Toptegnet $0$ angiver en negeret variabel. 
  Betragt nu følgende graf $G$.
  Knuderne svarer til formlens literaler, dvs.  $V = \{\,r_{ij}\colon 1 \le i \le k \text{ og } 1 \le j \le h_i\}$.
  Et par af knuder $r_{ij}$ og $r_{i'j'}$ er naboer, hvis $i \not= i'$ og enten $x_{ij} \not= x_{i'j'}$ eller $\beta_{ij} = \beta_{i'j'}$.
  Med andre ord:
  Knuderne for to literaler er naboer, hvis og kun hvis literalerne hører til forskellige klausuler, og der findes en tildeling, som opfylder begge samtidigt.
  Vi påstår, at $F$ kan opfyldes, hvis og kun hvis $G$ har en klike af størrelse $k$.

  Antag først, at der findes en tildeling $\alpha$, som opfylder $F$.
  Denne tildeling opfylder mindst en literal per klausul, lad os sige literalen
  $\ell_{i,j(i)}$ i klausul $C_{i}$ for $i\in\{1,\ldots, k\}$.     
  Betragt nu den delgraf af $G$, som induceres af de $k$ mange knuder $r_{i,j(i)}$ for  $i\in\{1 ,\ldots, k\}$. 
  Vi påstår, at delgrafen er en klike.
  Antag modsætningsvist, at $r_{i,j(i)}$ og $r_{i',j(i')}$ ikke er naboer.          
  I så fald gælder $x_{i,j(i)} = x_{i',j(i')}$ og $\beta_{i,j(i)} \not= \beta_{i',j(i')}$.
  Men da er literalerne $\ell_{i,j(i)}$ og $\ell_{i',j(i')}$ hinandens komplement, og $\alpha$ kan ikke opfylde dem begge, i modstrid med konstruktionen.

  Antag omvendt, at $G$ har en klike $K$ bestående af $k$ knuder.
  Da kan vi konstruere en opfyldende tildeling $\alpha$ for $F$.
  Kliken $K$ indeholder en knude for hvert $i\in\{1,\ldots, k\}$, som vi kalder $r_{i,j(i)}$.
  Lad nu $\alpha$ være givet ved $\alpha(x_{i,j(i)}) = \beta_{i,j(i)}$, for de øvrige variable kan $\alpha$ vælges vilkårligt.
  Læg mærke til, at $\alpha$ er veldefineret, idet $x_{i,j(i)} = x_{i',j(i')}$ medfører $\beta_{i,j(i)} = \beta_{i',j(i')}$;
  ellers skulle knuderne $r_{i,j(i)}$ og $r_{i',j(i')}$ ikke være naboer. 
  Det er klart, at $\alpha$ opfylder $F$.
\end{proof}

\begin{exerc} 
  Vis, at hamiltonkredsproblemet kan reduceres til opfyldningsproblemet for boolske formler.
\end{exerc}

\begin{exerc} 
  Vis, at graffarvningsproblemet kan reduceres til opfyldningsproblemet for boolske formler.
\end{exerc}

De $\NP$-fuldstædige problemer »sidder i samme båd«.
Hvis nogen skulle være i stand til at løse \emph{et eneste} af dem i polynomiel tid, så ville $\NP=\classP$.
Idet rigtig mange mennesker uden success har prøvet at udvikle den slags algoritmer, bliver det mere og mere vanskeligt at forestille sig, at det nogensinde skulle lykkes.
% TODO skulle lykkedes?
De $\NP$-fuldstædige problemer dokumenterer altså sammenn deres egen vanskelighed.

Kan man bruge $\NP$-fuldstændighedsteorien også på optimeringsproblemer?
Optimeringsproblemer
\index{optimeringsproblem} 
(se kapitel~\ref{ch:optimization:}) 
kan let omvandles til beslutningsproblemer.
I stedet for at lede efter en optimal løsning, prøver vi bare at afgøre, om der \emph{findes} en gyldig løsning med målfunktionsværdien mindst $k$, hvor $k$ er del af input.
Den omvende konstruktion gælder ligeledes:
Hvis der findes en algoritme for at afgøre, om der findes en gyldig løsning med målfunktionsværdi mindst $k$, kan vi bruge kombinationen af eksponentiel og binær søgning
\index{søgning!eksponentiel}
\index{søgning!binær-} 
(se afsnit~\lref{s:binary search}) 
for at bestemme den optimale værdi af målfunktionen.

En algoritme for beslutningsproblemet svarer »ja« eller »nej«, afhængig af, om input tilhører det tilsvarende sprog eller ej.
Den giver dog ikke noget vidne.
Ofte kan man konstruere vidner ved at udføre beslutningsalgoritmen gentagne gange på lettere forandrede instanser.
Lad os fx antage, at grafen $G$ indeholder en klike af størrelse $k$, og at vi ønsker at finde sådan en klike.
Antag, at har en algorime, der afgør, om en givet graf indeholder en klike af en givet størrelse.
For $k=1$ kan en vilkårlig knude i $G$ bruges som svar på den eftersøgte klike. 
Eller vælger vi en vilkårlig knude $v$ i $G$ og spørger, og konstruerer grafen $G'$ som $G'=G\setminus v$ 
(notationen beskriver grafen $G$ uden knude $v$ og dens hosliggende kanter)
Vi spørger nu algoritmen, om $G'$ indeholder en klike af størrelse $k$.
I bekræftende fald, fortsætter vi søgningen efter en $k$-klike rekursivt i $G'$.
I modsat fald må $v$ indgå i samtlige $k$-kliker i $G$.
Da betragter vi mængden $V'$  af naboer til $v$ og fortsætter søgningen efter en $(k-1)$-klike $K'$ rekursivt i den af $V'$ inducerede delgraf. 
Mængden  $\set{v} \cup K'$ er en $k$-klike $G$. 
%TODO wrong in original, V' need not be a clique



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Implementationsaspekter}
\llabel{s:implementation}

Vores pseudokode 
\index{pseudocode} 
lader sig nemt oversætte til programmer i imperative programmeringssprog.
For \CC, Java og Python går vil lidt mere i detaljer. 
Programmeringssproget Eiffel\index{Eiffel}~\cite{Meyer97}
\index{Meyer, B.}
gør det muligt at arbejde med antagelser, invarianter, før- og efterbetingelser.

Vores specialværdier $\bot$, $-\infty$ og $\infty$ stilles til rådighed for kommatal
\index{kommatal}
\index{IEEE-kommatal|siehe{kommatal}} 
af programmeringssproget.
For andre datatyper skal man simulere disse værdier.
For eksempel kan man bruge det mindste og største heltal for  $-\infty$ hhv. $\infty$.
\index{uendeligt ($\infty$)}
Udefinerede pegere repræsenteres ofte af nulpegeren {\bf null}. 
Sommetider bruger vi specialværdierne $\bot$, $-\infty$ og $\infty$ kun af bekvemmelighedsgrunde;
en robust implementation skulle i forvejen undgå deres brug.
Vi skal se nogle eksempler på dette i de senere kapitler.

Randomiserede algoritmer har brug for en tilfældighedskilde.
\index{tilfældighedskilde}
Her har man valget mellem en maskinel tilfældighedsgenerator, som producerer ægte tilfældige tal, og en algoritmisk generator, der genererer pseudotilfældige tal.

\subsection{\protect\CC}\index{C++@\CC}
Man kan opfatte vores pseudokode som en kompakt notation for en delmængde af \CC.
Lagerforvaltningsoperationerne $\tAllocate$ og $\tDispose$ ligner operationerne \Id{new} og \Id{delete} i \CC.    
Når en række bliver skabt i \CC, kaldes den underliggende types standardkonstruktoren for hver indgang i rækken; det tager altså $\Omega(n)$ tid at skabe en række af længde $n$, hvorimod det kun tager konstant tid at skabe en række af $n$ elementer af heltalstypen \Id{int}. 
I bogens fremstilling foretages der ingen initialisering. 
Vi går derimod ud fra, at rækker uden udtrykkelig initialisering er fyldt med vilkårligt indhold, ofte kaldt »\emph{affald}«.
I \CC\ kan man opnå den samme opførsel ved at benytte de mere grundlæggende C-funktioner  
\Id{malloc} og \Id{free}.
Denne fremgangsmåde der dog ildeset og bør kun bruges i undtagelsestilfælde, når initialisering af rækken udgør en flaskehals i kørslen.
Hvis lagerforvaltningen for mange små objekter bliver kritisk for udførelsestiden, kan man tilpasse \CC\-standardbibliotekets klasse \Id{allocator} til respektive situation.

Vores parameterisering af klasser ved brug af $\Of$ er et specialtilfælde af \CC-sprogets mønstermekanisme.
De parametre, der ved objektdeklarationen tilføjes klassenavnet i parenteser svarer til \CC-konstruktorens parametre.

Antagelser implementeres som C-makroer i et inkluderet arkiv kaldt {\tt assert.h}.
Normalt udløser en brudt antagelse en køretidsfejl; fejlmeddelselsen indeholder antagelsen position i programteksten.
Ved at definere makroen \Id{NDEBUG} slår man den automatiske sikring af antagelser fra.

For mange af de datastrukturer og algoritmer, som diskuteres i denne bog, findes der fremragende implementationer i diverse standardbiblioteker.
Gode biblioteker er \emph{Standard Template Library} (STL)\index{STL}~\cite{Plau00},
BOOST\index{Boost}~\cite{boost} for \CC\
og LEDA\index{LEDA}~\cite{LEDAbook,LEDA-AS}.

\subsection{Java}
\index{Java}
I Java findes ingen udtrykkelig lagerforvaltning.
Derimod findes et separat lagerrensningsprogram, 
\index{lagerrensning|textbf}
\index{garbage collection|siehe{lagerrensning}}
som regelæssigt identificerer lagersegmenter, som ikke længere er i brug, og stiller dem til rådighed igen.
Dette forenkler programmeringen i Java betydeligt, men kan også have meget negative effekter på tidsforbruget.
Metoder, som afhjælper dette fænomen, ligger uden for bogens emner.
Java tillader parameteriserede klasser i form af generiske typer.
Antagelser implementeres som kommandoen \Id{assert}.

Pakken \Id{java}.\Id{util} og datastrukturbiblioteket JDSL~\cite{JDSL}
\index{JDSL}
indeholder fremragende implementationer af mange datastrukturer og algoritmer.

\subsection{Python}
\index{Python}

Pythons abstraktionsniveau er endnu højere en Javas; på nogle punkter ligger syntaksen meget tæt på vores pseudokode, på andre punkter er sproget næsten for langt fra von Neumann-arkitekturen til at en oversættelse bliver meningsfuld.
Som pædagogisk værktøj for at forstå koncepterne i denne bog, virker sproget ypperligt; vil man derimod skrive højtoptimerede programmer med fokus på effektivitet, gør man klogt i at vælge et andet sprog.

På samme måde som Java tillader Python heller ingen udtrykkelig lagerforvaltning, og oprydningsarbejdet sker i baggrundnen og uden for programmørens kontrol, med de samme fordele og ulemper.
Pythons typesystem er dynamisk, så parameterising af klasser med $\Of$ giver ikke meget mening.
Antagelser implementeres med \Id{assert}.

Mest bemærkelsesværdigt er det måske, at Python ikke stiller rækker til rådighed, men direkte bruger en liste-abstraktion, som vi først vil møde i kapitel~\ref{sec: sequence}.
Læseren aftale med sig selv, at fx udtrykket »$A$ = [0] * $N$« skaber en række af heltal af længde $N$, initialiseret til $0$, for at oversætte pseudokoden til fungerende python.
Men i virkeligheden er $A$ en pythonliste med langt mere magtfulde operationer.
Listeoperatiorne i moderne pythonfortolker er selvfølgeligt selv baseret på meget effektive implementationer af operationer på rækker, skrevet i C, men dette abstraktionsniveau er altså ikke eksponeret til programmøren.
Modulet \Id{arrays} stiller dog »rigtige« rækker af vsse grundtyper til rådighed.
Typen \Id{Set} i vores pseudokode er på de anden side del sproget, og udtrykkene \Id{None} og \Id{float('inf')} kan bruges som vores $\bot$ og $\infty$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Historiske anmærkninger og videre resultater}
\llabel{s:further}

Definitionen af registermaskinen til algoritmeanalyse blev foreslået af Shep\-herd\-son og Sturgis~\cite{Shepherdson-Sturgis}.
Modellen begrænser størrelsen af de tal, der kan gemmes en i enkelt lagercelle, til logaritmisk mange bit.
Uden denne indskrænkning opstår der uønskede konsekvenser, fordi modellen bliver alt for kraftig; fx kollapser kompleksitetsklasserne $\classP$ og PSpace~\cite{Hartmanis-Simon}.
En meget detaljeret abstrakt maskinmodel er beskrevet af Knuth~\cite{Knu99}.

Invarianter som koncept i programsemantik blev introduceret af Floyd~\cite{Floyd:meaning},
\index{Floyd, R. W.} 
og yderligere systematiseret af Hoare~\cite{Hoare:Axioms,Hoare:Data}.
\index{Hoare, C. A. R.} 
Bogen~\cite{GKP94}
\index{Knuth, D.}
\index{Graham, R. L.}
\index{Patashnik, O.}
indeholder en meget omfattende gemmengang af arbejdet med summer
\index{sum} og rekursionsligninger
\index{rekursionsligning}
og mange andre emner i »tællende« kombinatorik, som kommer til anvendelse i algoritmeanalysen.

Bøger om oversætterkonstruktion 
\index{oversætter}
(fx \cite{compiler,Wilhelm-Maurer})
\index{Wilhelm, R.}
\index{Mauer, D.}
forklarer detaljerne ved oversættelsen af højniveauprogrammeringssprog
\index{programmeringssprog} 
til maskinkode.

\index{udførelsestid|sieheauch{algoritmeanalyse}}%
\index{algoritmeanalyse|sieheauch{udførelsestid}}%
\index{kompleksitet|sieheauch{udførelsestid}}%
\index{input|siehe{instans}}
